{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-i8jBl9GRH"
      },
      "source": [
        "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "# Advanced RAG example\n",
        "\n",
        "Now that you have a good foundation in Redis data structures, search capabilities, and basic RAG with the redisvl client from [/getting_started/02_redisvl](../getting_started/02_redisvl.ipynb).\n",
        "\n",
        "We will extend the basic RAG example with a few special topics/techniques:\n",
        "- Dense content representation\n",
        "- Query rewriting / expansion\n",
        "- Semantic caching\n",
        "- Conversational memory persistence\n",
        "\n",
        "## Let's Begin!\n",
        "<a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/RAG/04_advanced_redisvl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improve accuracy with dense content representations\n",
        "In the basic example, we took raw chunks of text from our pdf documents and generated embeddings for them to be stored in the vector database. This is okay but one technique we can use to improve the quality of retrieval is to leverage an LLM from OpenAI during ETL. We will prompt the LLM to summarize and decompose the raw pdf text into more discrete propositional phrases. This will enhance the clarity of the text and improve semantic retrieval for RAG.\n",
        "\n",
        "The goal is to utilize a preprocessing technique similar to what's outlined here:\n",
        "https://github.com/langchain-ai/langchain/blob/master/templates/propositional-retrieval/propositional_retrieval/proposal_chain.py\n",
        "\n",
        "If you already have a redis-stack instance running locally from before feel free to jump ahead but if not execute the following commands to get the environment properly setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9HzsnQ1uiz"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "### Pull Github Materials\n",
        "Because you are likely running this notebook in **Google Colab**, we need to first\n",
        "pull the necessary dataset and materials directly from GitHub.\n",
        "\n",
        "**If you are running this notebook locally**, FYI you may not need to perform this\n",
        "step at all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AJJ2UW6M1ui0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 138 (delta 68), reused 91 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (138/138), 7.19 MiB | 7.61 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n"
          ]
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "!git clone https://github.com/redis-developer/redis-ai-resources.git temp_repo\n",
        "!mv temp_repo/python-recipes/RAG/resources .\n",
        "!rm -rf temp_repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z67mf6T91ui2"
      },
      "source": [
        "### Install Python Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DgxBQFXQ1ui2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: redis in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (5.0.4)\n",
            "Requirement already satisfied: redisvl in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.1.3)\n",
            "Requirement already satisfied: pandas in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (2.7.0)\n",
            "Requirement already satisfied: langchain in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.0.312)\n",
            "Requirement already satisfied: langchain-community in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.0.20)\n",
            "Requirement already satisfied: openai in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (1.30.2)\n",
            "Requirement already satisfied: tqdm in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (4.66.4)\n",
            "Requirement already satisfied: unstructured[pdf] in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.13.7)\n",
            "Requirement already satisfied: numpy in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from redisvl) (1.26.4)\n",
            "Requirement already satisfied: pyyaml in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from redisvl) (6.0.1)\n",
            "Requirement already satisfied: coloredlogs in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from redisvl) (15.0.1)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from redisvl) (2.7.1)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from redisvl) (8.3.0)\n",
            "Requirement already satisfied: tabulate<1,>=0.9.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from redisvl) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: chardet in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (5.2.1)\n",
            "Requirement already satisfied: nltk in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (3.8.1)\n",
            "Requirement already satisfied: requests in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (2.11.1)\n",
            "Requirement already satisfied: dataclasses-json in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (0.6.5)\n",
            "Requirement already satisfied: python-iso639 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (3.9.0)\n",
            "Requirement already satisfied: backoff in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (4.11.0)\n",
            "Requirement already satisfied: unstructured-client in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (0.22.0)\n",
            "Requirement already satisfied: wrapt in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: onnx in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: pdf2image in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: pdfminer.six in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (20231228)\n",
            "Requirement already satisfied: pikepdf in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (8.15.1)\n",
            "Requirement already satisfied: pillow-heif in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (0.16.0)\n",
            "Requirement already satisfied: pypdf in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (4.2.0)\n",
            "Requirement already satisfied: unstructured-inference==0.7.31 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (0.7.31)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (0.3.12)\n",
            "Requirement already satisfied: google-cloud-vision in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured[pdf]) (3.7.2)\n",
            "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-inference==0.7.31->unstructured[pdf]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-inference==0.7.31->unstructured[pdf]) (0.0.9)\n",
            "Requirement already satisfied: huggingface-hub in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-inference==0.7.31->unstructured[pdf]) (0.23.0)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-inference==0.7.31->unstructured[pdf]) (4.9.0.80)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-inference==0.7.31->unstructured[pdf]) (1.17.3)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-inference==0.7.31->unstructured[pdf]) (4.40.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: scipy in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: Pillow in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sentence-transformers) (10.3.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: anyio<4.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (0.0.87)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.21 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain-community) (0.1.23)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from dataclasses-json->unstructured[pdf]) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: certifi in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from huggingface-hub->unstructured-inference==0.7.31->unstructured[pdf]) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from huggingface-hub->unstructured-inference==0.7.31->unstructured[pdf]) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from huggingface-hub->unstructured-inference==0.7.31->unstructured[pdf]) (23.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=2->redisvl) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=2->redisvl) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from requests->unstructured[pdf]) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from requests->unstructured[pdf]) (2.2.1)\n",
            "Requirement already satisfied: sympy in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[pdf]) (2024.4.28)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[pdf]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[pdf]) (0.4.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from beautifulsoup4->unstructured[pdf]) (2.5)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from coloredlogs->redisvl) (10.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (2.19.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-cloud-vision->unstructured[pdf]) (2.29.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-cloud-vision->unstructured[pdf]) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-cloud-vision->unstructured[pdf]) (4.25.3)\n",
            "Requirement already satisfied: click in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk->unstructured[pdf]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk->unstructured[pdf]) (1.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pdfminer.six->unstructured[pdf]) (42.0.7)\n",
            "Requirement already satisfied: Deprecated in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pikepdf->unstructured[pdf]) (1.2.14)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-client->unstructured[pdf]) (7.0.1)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-client->unstructured[pdf]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from deepdiff>=6.0->unstructured-client->unstructured[pdf]) (4.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.63.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.62.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (4.9)\n",
            "Requirement already satisfied: flatbuffers in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[pdf]) (24.3.25)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: iopath in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (0.11.0)\n",
            "Requirement already satisfied: torchvision in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (0.18.0)\n",
            "Requirement already satisfied: effdet in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: pytesseract in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (0.3.10)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.6.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (0.9.16)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (2.0.7)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: portalocker in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (2.8.2)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (4.29.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (3.8.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]) (3.1.2)\n"
          ]
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "!pip install -q redis redisvl pandas \"unstructured[pdf]\" sentence-transformers langchain langchain-community openai tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Redis Stack\n",
        "\n",
        "Later in this tutorial, Redis will be used to store, index, and query vector\n",
        "embeddings created from PDF document chunks. **We need to make sure we have a Redis\n",
        "instance available.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### For Colab\n",
        "Use the shell script below to download, extract, and install [Redis Stack](https://redis.io/docs/getting-started/install-stack/) directly\n",
        "from the Redis package archive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb jammy main\n",
            "Starting redis-stack-server, database path /var/lib/redis-stack\n"
          ]
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "%%sh\n",
        "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "sudo apt-get update  > /dev/null 2>&1\n",
        "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "redis-stack-server --daemonize yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### For Alternative Environments\n",
        "There are many ways to get the necessary redis-stack instance running\n",
        "1. On cloud, deploy a [FREE instance of Redis in the cloud](https://redis.com/try-free/). Or, if you have your\n",
        "own version of Redis Enterprise running, that works too!\n",
        "2. Per OS, [see the docs](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/)\n",
        "3. With docker: `docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Redis Connection URL\n",
        "\n",
        "By default this notebook connects to the local instance of Redis Stack. **If you have your own Redis Enterprise instance** - replace REDIS_PASSWORD, REDIS_HOST and REDIS_PORT values with your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace values below with your own if using Redis Cloud instance\n",
        "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\") # ex: \"redis-18374.c253.us-central1-1.gce.cloud.redislabs.com\"\n",
        "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")      # ex: 18374\n",
        "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")  # ex: \"1TNxTEdYRDgIDKM2gDfasupCADXXXX\"\n",
        "\n",
        "# If SSL is enabled on the endpoint, use rediss:// as the URL prefix\n",
        "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now that our environment is setup we can again load our financial documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrtWWU4I1ui3"
      },
      "source": [
        "### Dataset Preparation (PDF Documents)\n",
        "\n",
        "To best demonstrate Redis as a vector database layer, we will load a single\n",
        "financial (10k filings) doc and preprocess it using some helpers from LangChain:\n",
        "\n",
        "- `UnstructuredFileLoader` is not the only document loader type that LangChain provides. Docs: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file\n",
        "- `RecursiveCharacterTextSplitter` is what we use to create smaller chunks of text from the doc. Docs: https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uijl2qFH1ui3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listing available documents ... ['resources/nke-10k-2023.pdf', 'resources/amzn-10k-2023.pdf', 'resources/jnj-10k-2023.pdf', 'resources/aapl-10k-2023.pdf', 'resources/nvd-10k-2023.pdf', 'resources/msft-10k-2023.pdf']\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "# Load list of pdfs from a folder\n",
        "data_path = \"resources/\"\n",
        "docs = [os.path.join(data_path, file) for file in os.listdir(data_path)]\n",
        "\n",
        "print(\"Listing available documents ...\", docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "anya8hVnT6K_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done preprocessing. Created 180 chunks of the original pdf resources/nke-10k-2023.pdf\n"
          ]
        }
      ],
      "source": [
        "# pick out the Nike doc for this exercise\n",
        "doc = [doc for doc in docs if \"nke\" in doc][0]\n",
        "\n",
        "# set up the file loader/extractor and text splitter to create chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=0)\n",
        "loader = UnstructuredFileLoader(doc, mode=\"single\", strategy=\"fast\")\n",
        "\n",
        "# extract, load, and make chunks\n",
        "chunks = loader.load_and_split(text_splitter)\n",
        "\n",
        "print(\"Done preprocessing. Created\", len(chunks), \"chunks of the original pdf\", doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In the previous example, we would have gone ahead and embed the chunks as extracted here.\n",
        "\n",
        "Now we will instead leverage an LLM to create dense content representations to improve our retrieval accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup OpenAI as LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "import openai\n",
        "\n",
        "CHAT_MODEL = \"gpt-3.5-turbo-0125\"\n",
        "\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import json\n",
        "\n",
        "\n",
        "def create_dense_props(chunk):\n",
        "    \"\"\"Create dense representation of raw text content.\"\"\"\n",
        "\n",
        "    # The system message here should be HEAVILY customized for your specific use case\n",
        "    SYSTEM_PROMPT = \"\"\"\n",
        "    You are a helpful PDF extractor tool. You will be presented with segments from\n",
        "    raw PDF documents composed of 10k SEC filings information about public companies.\n",
        "\n",
        "    Decompose and summarize the raw content into clear and simple propositions,\n",
        "    ensuring they are interpretable out of context. Consider the following rules:\n",
        "    1. Split compound sentences into simpler dense phrases that retain existing\n",
        "    meaning.\n",
        "    2. Simplify technical jargon or wording if possible while retaining existing\n",
        "    meaning.\n",
        "    2. For any named entity that is accompanied by additional descriptive information,\n",
        "    separate this information into its own distinct proposition.\n",
        "    3. Decontextualize the proposition by adding necessary modifier to nouns or\n",
        "    entire sentences and replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\")\n",
        "    with the full name of the entities they refer to.\n",
        "    4. Present the results as a list of strings, formatted in JSON, under the key \"propositions\".\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.OpenAI().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": f\"Decompose this raw content using the rules above:\\n{chunk.page_content} \"}\n",
        "        ]\n",
        "    )\n",
        "    res = response.choices[0].message.content\n",
        "\n",
        "    try:\n",
        "        return json.loads(res)[\"propositions\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to parse propositions\", str(e), flush=True)\n",
        "        # Retry\n",
        "        return create_dense_props(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create text propositions using OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load from disk to save time or regenerate as needed.\n",
        "try:\n",
        "    with open(\"resources/propositions.json\", \"r\") as f:\n",
        "        propositions = json.load(f)\n",
        "except:\n",
        "    # create props\n",
        "    propositions = [create_dense_props(chunk) for chunk in tqdm.tqdm(chunks)]\n",
        "\n",
        "    # Save to disk for faster reload..\n",
        "    with open(\"resources/propositions.json\", \"w\") as f:\n",
        "        json.dump(propositions, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's evaluate the proposition vs the raw chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"As of November 30, 2022, the aggregate market value of NIKE, Inc.'s Common Stock held by non-affiliates was $144,299,267,044. NIKE, Inc. filed a Form 10-K with the UNITED STATES SECURITIES AND EXCHANGE COMMISSION. The Form 10-K is an ANNUAL REPORT FOR THE FISCAL YEAR ENDED MAY 31, 2023. NIKE, Inc.'s exact name as specified in its charter is NIKE, Inc. NIKE, Inc. is incorporated in Oregon with an IRS Employer Identification No. of 93-0584541. The principal executive offices of NIKE, Inc. are located at One Bowerman Drive, Beaverton, Oregon 97005-6453. NIKE, Inc.'s telephone number, including area code, is (503) 671-6453. NIKE, Inc. has Class B Common Stock registered on the New York Stock Exchange under the trading symbol NKE. NIKE, Inc. does not have any securities registered pursuant to SECTION 12(G) OF THE ACT.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "propositions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"As of November 30, 2022, the aggregate market values of the Registrant's Common Stock held by non-affiliates were:Class A$7,831,564,572 Class B136,467,702,472 $144,299,267,044\\n\\nTable of ContentsUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FOR THE FISCAL YEAR ENDED MAY 31, 2023OR☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FOR THE TRANSITION PERIOD FROM TO .Commission File No. 1-10635\\n\\nNIKE, Inc.(Exact name of Registrant as specified in its charter)Oregon93-0584541(State or other jurisdiction of incorporation)(IRS Employer Identification No.)One Bowerman Drive, Beaverton, Oregon 97005-6453(Address of principal executive offices and zip code)(503) 671-6453(Registrant's telephone number, including area code)SECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:Class B Common StockNKENew York Stock Exchange(Title of each class)(Trading symbol)(Name of each exchange on which registered)SECURITIES REGISTERED PURSUANT TO SECTION 12(G) OF THE ACT:NONE\", metadata={'source': 'resources/nke-10k-2023.pdf'})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings from propositions data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from redisvl.utils.vectorize import HFTextVectorizer\n",
        "\n",
        "hf = HFTextVectorizer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "prop_embeddings = hf.embed_many([\n",
        "    proposition for proposition in propositions\n",
        "])\n",
        "\n",
        "# Check to make sure we've created enough embeddings, 1 per document chunk\n",
        "len(prop_embeddings) == len(propositions) == len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5baI0xDQ1ui-"
      },
      "source": [
        "### Define a schema and create an index\n",
        "\n",
        "Below we connect to Redis and create an index that contains a text field, tag field, and vector field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zB1EW_9n1ui-"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'Redis' from 'redis' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Redis\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredisvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexSchema\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredisvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SearchIndex\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Redis' from 'redis' (unknown location)"
          ]
        }
      ],
      "source": [
        "from redis import Redis\n",
        "from redisvl.schema import IndexSchema\n",
        "from redisvl.index import SearchIndex\n",
        "\n",
        "\n",
        "index_name = \"redisvl\"\n",
        "\n",
        "\n",
        "schema = IndexSchema.from_dict({\n",
        "  \"index\": {\n",
        "    \"name\": index_name,\n",
        "    \"prefix\": \"chunk\"\n",
        "  },\n",
        "  \"fields\": [\n",
        "    {\n",
        "        \"name\": \"chunk_id\",\n",
        "        \"type\": \"tag\",\n",
        "        \"attrs\": {\n",
        "            \"sortable\": True\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"proposition\",\n",
        "        \"type\": \"text\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"text_embedding\",\n",
        "        \"type\": \"vector\",\n",
        "        \"attrs\": {\n",
        "            \"dims\": hf.dims,\n",
        "            \"distance_metric\": \"cosine\",\n",
        "            \"algorithm\": \"hnsw\",\n",
        "            \"datatype\": \"float32\"\n",
        "        }\n",
        "    }\n",
        "  ]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# connect to redis\n",
        "client = Redis.from_url(REDIS_URL)\n",
        "\n",
        "# create an index from schema and the client\n",
        "index = SearchIndex(schema, client)\n",
        "index.create(overwrite=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C70C-UWj1ujA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Index Information:\n",
            "╭──────────────┬────────────────┬────────────┬─────────────────┬────────────╮\n",
            "│ Index Name   │ Storage Type   │ Prefixes   │ Index Options   │   Indexing │\n",
            "├──────────────┼────────────────┼────────────┼─────────────────┼────────────┤\n",
            "│ redisvl      │ HASH           │ ['chunk']  │ []              │          0 │\n",
            "╰──────────────┴────────────────┴────────────┴─────────────────┴────────────╯\n",
            "Index Fields:\n",
            "╭────────────────┬────────────────┬────────┬────────────────┬────────────────┬────────────────┬────────────────┬────────────────┬────────────────┬─────────────────┬────────────────┬────────────────┬────────────────┬─────────────────┬────────────────╮\n",
            "│ Name           │ Attribute      │ Type   │ Field Option   │ Option Value   │ Field Option   │ Option Value   │ Field Option   │   Option Value │ Field Option    │ Option Value   │ Field Option   │   Option Value │ Field Option    │   Option Value │\n",
            "├────────────────┼────────────────┼────────┼────────────────┼────────────────┼────────────────┼────────────────┼────────────────┼────────────────┼─────────────────┼────────────────┼────────────────┼────────────────┼─────────────────┼────────────────┤\n",
            "│ chunk_id       │ chunk_id       │ TAG    │ SEPARATOR      │ ,              │                │                │                │                │                 │                │                │                │                 │                │\n",
            "│ proposition    │ proposition    │ TEXT   │ WEIGHT         │ 1              │                │                │                │                │                 │                │                │                │                 │                │\n",
            "│ text_embedding │ text_embedding │ VECTOR │ algorithm      │ HNSW           │ data_type      │ FLOAT32        │ dim            │            384 │ distance_metric │ COSINE         │ M              │             16 │ ef_construction │            200 │\n",
            "╰────────────────┴────────────────┴────────┴────────────────┴────────────────┴────────────────┴────────────────┴────────────────┴────────────────┴─────────────────┴────────────────┴────────────────┴────────────────┴─────────────────┴────────────────╯\n"
          ]
        }
      ],
      "source": [
        "# get info about the index\n",
        "# NBVAL_SKIP\n",
        "!rvl index info -i redisvl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrj-jeGmBRTL"
      },
      "source": [
        "### Process and load dataset\n",
        "Below we use the RedisVL index to simply load the list of document chunks to Redis db."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsg09Keg1ujA"
      },
      "outputs": [],
      "source": [
        "# load expects an iterable of dictionaries\n",
        "from redisvl.redis.utils import array_to_buffer\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        'chunk_id': f'{i}',\n",
        "        'proposition': proposition,\n",
        "        # For HASH -- must convert embeddings to bytes\n",
        "        'text_embedding': array_to_buffer(prop_embeddings[i])\n",
        "    } for i, proposition in enumerate(propositions)\n",
        "]\n",
        "\n",
        "# RedisVL handles batching automatically\n",
        "keys = index.load(data, id_field=\"chunk_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup RedisVL AsyncSearchIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from redis.asyncio import Redis\n",
        "from redisvl.index import AsyncSearchIndex\n",
        "\n",
        "client = Redis.from_url(REDIS_URL)\n",
        "index = AsyncSearchIndex(index.schema, client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test the updated RAG workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from redisvl.query import VectorQuery\n",
        "from redisvl.index import AsyncSearchIndex\n",
        "\n",
        "\n",
        "def promptify(query: str, context: str) -> str:\n",
        "    return f'''Use the provided context below derived from public financial\n",
        "    documents to answer the user's question. If you can't answer the user's\n",
        "    question, based on the context; do not guess. If there is no context at all,\n",
        "    respond with \"I don't know\".\n",
        "\n",
        "    User question:\n",
        "\n",
        "    {query}\n",
        "\n",
        "    Helpful context:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Answer:\n",
        "    '''\n",
        "\n",
        "# Update the retrieval helper to use propositions\n",
        "async def retrieve_context(index: AsyncSearchIndex, query_vector) -> str:\n",
        "    \"\"\"Fetch the relevant context from Redis using vector search\"\"\"\n",
        "    print(\"Using dense content representation\", flush=True)\n",
        "    results = await index.query(\n",
        "        VectorQuery(\n",
        "            vector=query_vector,\n",
        "            vector_field_name=\"text_embedding\",\n",
        "            return_fields=[\"proposition\"],\n",
        "            num_results=3\n",
        "        )\n",
        "    )\n",
        "    content = \"\\n\".join([result[\"proposition\"] for result in results])\n",
        "    return content\n",
        "\n",
        "# Update the answer_question method\n",
        "async def answer_question(index: AsyncSearchIndex, query: str):\n",
        "    \"\"\"Answer the user's question\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
        "    to public financial 10k documents in order to answer users questions about company\n",
        "    performance, ethics, characteristics, and core information.\n",
        "    \"\"\"\n",
        "\n",
        "    query_vector = hf.embed(query)\n",
        "    # Fetch context from Redis using vector search\n",
        "    context = await retrieve_context(index, query_vector)\n",
        "    # Generate contextualized prompt and feed to OpenAI\n",
        "    response = await openai.AsyncClient().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": promptify(query, context)}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        seed=42\n",
        "    )\n",
        "    # Response provided by LLM\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a list of questions\n",
        "questions = [\n",
        "    \"What is the trend in the company's revenue and profit over the past few years?\",\n",
        "    \"What are the company's primary revenue sources?\",\n",
        "    \"How much debt does the company have, and what are its capital expenditure plans?\",\n",
        "    \"What does the company say about its environmental, social, and governance (ESG) practices?\",\n",
        "    \"What is the company's strategy for growth?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dense content representation\n",
            "Using dense content representation\n",
            "Using dense content representation\n",
            "Using dense content representation\n",
            "Using dense content representation\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the trend in the company's revenue and...</td>\n",
              "      <td>Based on the provided context, the trend in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are the company's primary revenue sources?</td>\n",
              "      <td>The company's primary revenue sources are as f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How much debt does the company have, and what ...</td>\n",
              "      <td>The company has a total long-term debt of $8,9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What does the company say about its environmen...</td>\n",
              "      <td>Based on the provided context, the company ack...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the company's strategy for growth?</td>\n",
              "      <td>The company's strategy for growth includes emp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the trend in the company's revenue and...   \n",
              "1    What are the company's primary revenue sources?   \n",
              "2  How much debt does the company have, and what ...   \n",
              "3  What does the company say about its environmen...   \n",
              "4         What is the company's strategy for growth?   \n",
              "\n",
              "                                              answer  \n",
              "0  Based on the provided context, the trend in th...  \n",
              "1  The company's primary revenue sources are as f...  \n",
              "2  The company has a total long-term debt of $8,9...  \n",
              "3  Based on the provided context, the company ack...  \n",
              "4  The company's strategy for growth includes emp...  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "\n",
        "results = await asyncio.gather(*[\n",
        "    answer_question(index, question) for question in questions\n",
        "])\n",
        "\n",
        "pd.DataFrame(columns=[\"question\", \"answer\"], data=list(zip(questions, results)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnkK0NwIIM9q"
      },
      "source": [
        "### Improve accuracy with query rewriting / expansion\n",
        "\n",
        "We can also use the power on an LLM to rewrite or expand an input question.\n",
        "\n",
        "Example: https://github.com/langchain-ai/langchain/blob/master/templates/rewrite-retrieve-read/rewrite_retrieve_read/chain.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnWhfeiGYVrI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dense content representation\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Based on the information provided, we can see that NIKE, Inc. is a large company with multiple subsidiaries operating in various jurisdictions such as the United States, Netherlands, China, Mexico, Japan, and Korea. The company's revenues are significant, with detailed breakdowns of revenue streams from footwear, apparel, equipment, and other sources. Additionally, the company has a diverse customer base, including wholesale customers and direct-to-consumer sales channels. NIKE, Inc. also has a strong presence in international markets, with operations in North America, Western Europe, and Asia.\\n\\nOverall, based on the extensive information provided about the company's operations, revenue streams, and global presence, we can conclude that NIKE, Inc. is a large and well-established company in the sports apparel and footwear industry.\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "# An example question that is a bit simplistic...\n",
        "await answer_question(index, \"How big is the company?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg55HqLFIRXJ"
      },
      "outputs": [],
      "source": [
        "# NBVAL_SKIP\n",
        "async def rewrite_query(query: str, prompt: str = None):\n",
        "    \"\"\"Rewrite the user's original query\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = prompt if prompt else \"\"\"Given the user's input question below, find a better or\n",
        "    more complete way to phrase this question in order to improve semantic search\n",
        "    engine retrieval quality over a set of SEC 10K PDF docs. Return the rephrased\n",
        "    question as a string in a JSON response under the key \"query\".\"\"\"\n",
        "\n",
        "    response = await openai.AsyncClient().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": f\"Original input question from user: {query}\"}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        seed=42\n",
        "    )\n",
        "    # Response provided by LLM\n",
        "    rewritten_query = json.loads(response.choices[0].message.content)[\"query\"]\n",
        "    return rewritten_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ce8fC8KR50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What is the size of the company in terms of revenue, number of employees, and market share?'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "# Example Sinple Query Rewritten\n",
        "await rewrite_query(\"How big is the company?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ubNQrJOYL42"
      },
      "outputs": [],
      "source": [
        "async def answer_question(index: AsyncSearchIndex, query: str, **kwargs):\n",
        "    \"\"\"Answer the user's question\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
        "    to public financial 10k documents in order to answer users questions about company\n",
        "    performance, ethics, characteristics, and core information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Rewrite the query using an LLM\n",
        "    rewritten_query = await rewrite_query(query, **kwargs)\n",
        "    print(\"User query updated to:\\n\", rewritten_query, flush=True)\n",
        "\n",
        "    query_vector = hf.embed(rewritten_query)\n",
        "    # Fetch context from Redis using vector search\n",
        "    context = await retrieve_context(index, query_vector)\n",
        "    print(\"Context retrieved\", flush=True)\n",
        "\n",
        "    # Generate contextualized prompt and feed to OpenAI\n",
        "    response = await openai.AsyncClient().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": promptify(rewritten_query, context)}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        seed=42\n",
        "    )\n",
        "    # Response provided by LLM\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIO_jW6KYsMU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User query updated to:\n",
            " What is the size of the company in terms of revenue, number of employees, and market share?\n",
            "Using dense content representation\n",
            "Context retrieved\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Based on the provided context, the company in question is Nike, Inc. Here is the breakdown of the company's size based on the information provided:\\n\\n1. Revenue:\\n   - Total revenues for Nike, Inc. amount to $51.2 billion.\\n   - Revenues in North America total $21.6 billion.\\n   - Revenues in Europe, Middle East & Africa total $13.4 billion.\\n   - Revenues in Greater China total $7.2 billion.\\n   - Revenues in Asia Pacific & Latin America total $6.4 billion.\\n   - Revenues from footwear total $35.3 billion.\\n   - Revenues from apparel total $13.8 billion.\\n   - Revenues from equipment total $2.4 billion.\\n   - Revenues from other sources total $27 million.\\n\\n2. Number of Employees:\\n   The number of employees is not explicitly provided in the context. This information may be available in the company's annual report or other filings.\\n\\n3. Market Share:\\n   Market share information is not directly provided in the context. Market share data is typically not disclosed in financial documents but may be estimated by industry analysts based on sales data and industry reports.\\n\\nIn summary, based on the revenue figures provided, Nike, Inc. is a large company with significant revenue across different regions and product categories.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "# Now try again with query re-writing enabled\n",
        "await answer_question(index, \"How big is the company?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p97uL4g9T6LQ"
      },
      "source": [
        "### Improve performance and cut costs with LLM caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7geEAsYST6LQ"
      },
      "outputs": [],
      "source": [
        "from redisvl.extensions.llmcache import SemanticCache\n",
        "\n",
        "llmcache = SemanticCache(\n",
        "    name=\"llmcache\",\n",
        "    vectorizer=hf,\n",
        "    redis_url=REDIS_URL,\n",
        "    ttl=120,\n",
        "    distance_threshold=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ALcQXAqT6LQ"
      },
      "outputs": [],
      "source": [
        "from functools import wraps\n",
        "\n",
        "# Create an LLM caching decorator\n",
        "def cache(func):\n",
        "    @wraps(func)\n",
        "    async def wrapper(index, query_text, *args, **kwargs):\n",
        "        query_vector = llmcache._vectorizer.embed(query_text)\n",
        "\n",
        "        # Check the cache with the vector\n",
        "        if result := llmcache.check(vector=query_vector):\n",
        "            return result[0]['response']\n",
        "\n",
        "        response = await func(index, query_text, query_vector=query_vector)\n",
        "        llmcache.store(query_text, response, query_vector)\n",
        "        return response\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@cache\n",
        "async def answer_question(index: AsyncSearchIndex, query: str, **kwargs):\n",
        "    \"\"\"Answer the user's question\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
        "    to public financial 10k documents in order to answer users questions about company\n",
        "    performance, ethics, characteristics, and core information.\n",
        "    \"\"\"\n",
        "\n",
        "    context = await retrieve_context(index, kwargs[\"query_vector\"])\n",
        "    response = await openai.AsyncClient().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": promptify(query, context)}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        seed=42\n",
        "    )\n",
        "    # Response provided by GPT-3.5\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXK_BXuhT6LQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dense content representation\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Nike's total revenue for the fiscal year 2023 was $51.2 billion, which represents a 10% increase compared to the previous fiscal year. The revenue growth was mainly driven by higher revenues in North America, Europe, Middle East & Africa, Asia Pacific & Latin America, and Greater China.\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "query = \"What was Nike's revenue last year compared to this year??\"\n",
        "\n",
        "await answer_question(index, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mZpSpf9T6LQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Nike's total revenue for the fiscal year 2023 was $51.2 billion, which represents a 10% increase compared to the previous fiscal year. The revenue growth was mainly driven by higher revenues in North America, Europe, Middle East & Africa, Asia Pacific & Latin America, and Greater China.\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "query = \"What was Nike's total revenue in the last year compared to now??\"\n",
        "\n",
        "await answer_question(index, query)\n",
        "\n",
        "# notice no HTTP request to OpenAI since this question is \"close enough\" to the last one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaiF_ws7itsi"
      },
      "source": [
        "### Improve personalization with including chat session history\n",
        "\n",
        "In order to preserve state in the conversation, it's imperitive to offload conversation history to a database that can handle high transaction throughput for writes/reads to limit system latency.\n",
        "\n",
        "We can store message history for a particular user session in a Redis List data type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMOF7fJQdhgN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "class ChatBot:\n",
        "    def __init__(self, index: AsyncSearchIndex, user: str):\n",
        "        self.index = index\n",
        "        self.user = user\n",
        "\n",
        "    async def get_messages(self) -> list:\n",
        "        \"\"\"Get all messages associated with a session\"\"\"\n",
        "        return [\n",
        "            json.loads(msg) for msg in await self.index.client.lrange(f\"messages:{self.user}\", 0, -1)\n",
        "        ]\n",
        "\n",
        "    async def add_messages(self, messages: list):\n",
        "        \"\"\"Add chat messages to a Redis List\"\"\"\n",
        "        return await self.index.client.rpush(\n",
        "            f\"messages:{self.user}\", *[json.dumps(msg) for msg in messages]\n",
        "        )\n",
        "\n",
        "    async def clear_history(self):\n",
        "        \"\"\"Clear session chat\"\"\"\n",
        "        await index.client.delete(f\"messages:{self.user}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def promptify(query: str, context: str) -> str:\n",
        "        return f'''Use the provided context below derived from public financial\n",
        "        documents to answer the user's question. If you can't answer the user's\n",
        "        question, based on the context; do not guess. If there is no context at all,\n",
        "        respond with \"I don't know\".\n",
        "\n",
        "        User question:\n",
        "\n",
        "        {query}\n",
        "\n",
        "        Helpful context:\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Answer:\n",
        "        '''\n",
        "\n",
        "    async def retrieve_context(self, query_vector) -> str:\n",
        "        \"\"\"Fetch the relevant context from Redis using vector search\"\"\"\n",
        "        results = await self.index.query(\n",
        "            VectorQuery(\n",
        "                vector=query_vector,\n",
        "                vector_field_name=\"text_embedding\",\n",
        "                return_fields=[\"propositions\"],\n",
        "                num_results=3\n",
        "            )\n",
        "        )\n",
        "        content = \"\\n\".join([result[\"propositions\"] for result in results])\n",
        "        return content\n",
        "\n",
        "    async def answer_question(self, query: str):\n",
        "        \"\"\"Answer the user's question with historical context and caching baked-in\"\"\"\n",
        "\n",
        "        SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
        "        to public financial 10k documents in order to answer users questions about company\n",
        "        performance, ethics, characteristics, and core information.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create query vector\n",
        "        query_vector = llmcache._vectorizer.embed(query)\n",
        "\n",
        "        # TODO - implement semantic gaurdrails?\n",
        "\n",
        "        # Check the cache with the vector\n",
        "        if result := llmcache.check(vector=query_vector):\n",
        "            answer = result[0]['response']\n",
        "        else:\n",
        "            # TODO - implement query rewriting?\n",
        "            context = await self.retrieve_context(query_vector)\n",
        "            session = await self.get_messages()\n",
        "            # TODO - implement session summarization?\n",
        "            messages = (\n",
        "                [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] +\n",
        "                session +\n",
        "                [{\"role\": \"user\", \"content\": self.promptify(query, context)}]\n",
        "            )\n",
        "            # Response provided by GPT-3.5\n",
        "            response = await openai.AsyncClient().chat.completions.create(\n",
        "                model=CHAT_MODEL,\n",
        "                messages=messages,\n",
        "                temperature=0.1,\n",
        "                seed=42\n",
        "            )\n",
        "            answer = response.choices[0].message.content\n",
        "            llmcache.store(query, answer, query_vector)\n",
        "\n",
        "        # Add message history\n",
        "        await self.add_messages([\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "            {\"role\": \"assistant\", \"content\": answer}\n",
        "        ])\n",
        "\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the entire RAG workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z3RUvyxdhiz"
      },
      "outputs": [],
      "source": [
        "# Setup Session\n",
        "chat = ChatBot(index, \"tyler\")\n",
        "await chat.clear_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'propositions'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m stopterms:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat\u001b[38;5;241m.\u001b[39manswer_question(user_query)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[43], line 74\u001b[0m, in \u001b[0;36mChatBot.answer_question\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     71\u001b[0m     answer \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# TODO - implement query rewriting?\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_context(query_vector)\n\u001b[1;32m     75\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_messages()\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# TODO - implement session summarization?\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[43], line 53\u001b[0m, in \u001b[0;36mChatBot.retrieve_context\u001b[0;34m(self, query_vector)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch the relevant context from Redis using vector search\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m     46\u001b[0m     VectorQuery(\n\u001b[1;32m     47\u001b[0m         vector\u001b[38;5;241m=\u001b[39mquery_vector,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 53\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m[\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpropositions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
            "Cell \u001b[0;32mIn[43], line 53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch the relevant context from Redis using vector search\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m     46\u001b[0m     VectorQuery(\n\u001b[1;32m     47\u001b[0m         vector\u001b[38;5;241m=\u001b[39mquery_vector,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 53\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpropositions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
            "\u001b[0;31mKeyError\u001b[0m: 'propositions'"
          ]
        }
      ],
      "source": [
        "# Run a simple chat\n",
        "stopterms = [\"exit\", \"quit\", \"end\", \"cancel\"]\n",
        "\n",
        "# Simple Chat\n",
        "# NBVAL_SKIP\n",
        "while True:\n",
        "    user_query = input()\n",
        "    if user_query.lower() in stopterms:\n",
        "        break\n",
        "    answer = await chat.answer_question(user_query)\n",
        "    print(answer, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoPQMAShZ5Uy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'Hello. How many employees does Nike have?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'NIKE had about 83,700 employees globally as of May 31, 2023, including retail and part-time workers, as well as independent contractors and temporary personnel.'},\n",
              " {'role': 'user',\n",
              "  'content': 'Ok. What about the different products Nike sells. What kinds of products does it sell?'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"NIKE sells a wide range of products including athletic footwear, apparel, accessories, and equipment under the NIKE Brand, Jordan Brand, and Converse. The NIKE Brand offers performance athletic products for Men's, Women's, and Kids' categories, including sport-inspired lifestyle items. The Jordan Brand focuses on athletic and casual products with a basketball focus, while Converse sells casual products under various trademarks. Additionally, NIKE sells licensed apparel with team logos, performance equipment, and accessories such as bags, socks, and eyewear.\"},\n",
              " {'role': 'user', 'content': 'Which products are the most profitable?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Based on the provided context, it appears that footwear is the most profitable product category for NIKE. Footwear revenues for NIKE were $14,897, which is significantly higher than revenues from apparel ($5,947), equipment ($764), and other sources ($0). Additionally, the context mentions that footwear revenues increased by 22% on a currency-neutral basis, with unit sales increasing by 17% and higher ASP per pair contributing to revenue growth. This indicates that footwear is a key driver of profitability for NIKE.'}]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "await chat.get_messages()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l4uEgKzljes"
      },
      "source": [
        "## Your Next Steps\n",
        "\n",
        "While a good start, there is still more to do. **For example**:\n",
        "- we could utilize message history to generate an updated and contextualized query to use for retrieval and answer generation (with an LLM). Otherwise, there can be a disconnect between what a user is asking (in context) and what they are asking in isolation.\n",
        "- we could utilize an LLM to summarize conversation history to use as context instead of passing the whole slew of messages to the Chat endpoint.\n",
        "- we could utilize semantic properties of the message history (or summaries) in order to fetch only relevant conversation bits (vector search).\n",
        "- we could utilize a technique like HyDE ( a form of query rewriting ) to improve the retrieval quality from raw user input to source documents OR try to break down user questions into sub questions and fetch / join context based on the different searces.\n",
        "- we could incorporate semantic routing to take a broken down question and route to different data sources, indices, or query types (etc).\n",
        "- we could add semantic guardrails on the front end or back end of the conversation I/O to ensure we are within bounds of approved topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wscs4Mvo1ujD"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "Clean up the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On6yNuQn1ujD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "await index.client.flushall()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

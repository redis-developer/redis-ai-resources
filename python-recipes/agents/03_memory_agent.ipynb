{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Memory Agent Example\n",
    "The following example shows how to build an agent that uses multiple forms of memory:\n",
    "  1. Short-term memory (messages in the current conversation)\n",
    "  2. Long-term memory\n",
    "     1. Semantic: General knowledge\n",
    "     2. Episodic: User specific experiences\n",
    "     3. Procedural: How to do things\n",
    "\n",
    "## Let's Begin!\n",
    "<a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/agents/03_memory_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Cannot install langgraph-checkpoint and langgraph-checkpoint-redis==0.0.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain-openai langgraph-checkpoint langgraph-checkpoint-redis \"langchain-community>=0.2.11\" tavily-python langchain-redis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPEN_AI_API key\n",
    "\n",
    "You must add an OpenAI API key with billing information enabled is required for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run redis\n",
    "TODO\n",
    "\n",
    "NOTE: The existing magic shell commands don't work when run locally for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Alternative Environments\n",
    "There are many ways to get the necessary redis-stack instance running\n",
    "1. On cloud, deploy a [FREE instance of Redis in the cloud](https://redis.com/try-free/). Or, if you have your\n",
    "own version of Redis Enterprise running, that works too!\n",
    "2. Per OS, [see the docs](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/)\n",
    "3. With docker: `docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest`\n",
    "\n",
    "## Test connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from redis import Redis\n",
    "\n",
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "\n",
    "client = Redis.from_url(REDIS_URL)\n",
    "client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_redis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_agent_executor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_react_agent\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RedisSaver\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_redis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RedisCache\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mredis\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mredisvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SearchIndex\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_redis'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from langgraph.graph.message import MessagesState, RemoveMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from queue import Queue\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.prebuilt.chat_agent_executor import create_react_agent\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "from langchain_redis import RedisCache\n",
    "\n",
    "import redis\n",
    "from redisvl.index import SearchIndex\n",
    "from redisvl.query import VectorRangeQuery\n",
    "from redisvl.query.filter import Tag\n",
    "from redisvl.schema.schema import IndexSchema\n",
    "from redisvl.query import FilterQuery\n",
    "\n",
    "\n",
    "from util import get_logger\n",
    "from dotenv import load_dotenv\n",
    "import threading\n",
    "import json\n",
    "import time\n",
    "import ulid\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "# Memory types\n",
    "class MemoryType(str, Enum):\n",
    "    EPISODIC = \"episodic\"  # User specific experiences\n",
    "    PROCEDURAL = \"procedural\"  # How to do things\n",
    "    SEMANTIC = \"semantic\"  # General knowledge\n",
    "\n",
    "\n",
    "# Redis Configuration\n",
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
    "REDIS_PORT = int(os.getenv(\"REDIS_PORT\", 6379))\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")\n",
    "VECTOR_DIM = 1536  # ada-002 has 1536 dimensions\n",
    "\n",
    "MESSAGE_SUMMARIZATION_THRESHOLD = 12\n",
    "\n",
    "# Models and Tools\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "summarizer = ChatOpenAI(model=\"gpt-4o\", temperature=0.3)\n",
    "web_search_tool = TavilySearchResults(max_results=2)\n",
    "openai_embed = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "\n",
    "class RuntimeState(MessagesState):\n",
    "    \"\"\"Agent state (just messages for now)\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str\n",
    "    type: MemoryType\n",
    "    metadata: str\n",
    "\n",
    "\n",
    "class Memories(BaseModel):\n",
    "    memories: List[Memory]\n",
    "\n",
    "\n",
    "class StoredMemory(Memory):\n",
    "    id: str  # The redis key\n",
    "    memory_id: ulid.ULID = Field(default_factory=lambda: ulid.ULID())\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    user_id: str\n",
    "\n",
    "\n",
    "class StoredMemories(BaseModel):\n",
    "    memories: List[StoredMemory]\n",
    "\n",
    "\n",
    "# Initialize Redis connection\n",
    "redis_client = redis.Redis(\n",
    "    host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, decode_responses=True\n",
    ")\n",
    "\n",
    "# Define schema for memory index\n",
    "memory_schema = IndexSchema(\n",
    "    **{\n",
    "        \"index\": {\n",
    "            \"name\": \"agent_memories\",\n",
    "            \"prefix\": \"memory:\",\n",
    "            \"key_separator\": \":\",\n",
    "            \"storage_type\": \"json\",\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"content\", \"type\": \"text\"},\n",
    "            {\"name\": \"memory_type\", \"type\": \"tag\"},\n",
    "            {\"name\": \"metadata\", \"type\": \"text\"},\n",
    "            {\"name\": \"created_at\", \"type\": \"text\"},\n",
    "            {\"name\": \"user_id\", \"type\": \"tag\"},\n",
    "            {\"name\": \"memory_id\", \"type\": \"tag\"},\n",
    "            {\n",
    "                \"name\": \"embedding\",\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"algorithm\": \"flat\",\n",
    "                    \"dims\": VECTOR_DIM,\n",
    "                    \"distance_metric\": \"cosine\",\n",
    "                    \"datatype\": \"float32\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create search index\n",
    "try:\n",
    "    memory_index = SearchIndex(\n",
    "        schema=memory_schema, redis_client=redis_client, overwrite=True\n",
    "    )\n",
    "    memory_index.create()\n",
    "    logger.info(\"Memory index ready\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating index: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "cache = RedisCache(\n",
    "    redis_client=redis_client,\n",
    ")\n",
    "set_llm_cache(cache)\n",
    "\n",
    "# Create a checkpoint saver for LangGraph short-term memory\n",
    "redis_saver = RedisSaver(redis_client=redis_client)\n",
    "redis_saver.setup()\n",
    "\n",
    "# Create agents with specific roles\n",
    "travel_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[web_search_tool],\n",
    "    checkpointer=redis_saver,\n",
    "    prompt=SystemMessage(\n",
    "        content=\"\"\"\n",
    "        You are a travel assistant helping users plan their trips. You remember user preferences\n",
    "        and provide personalized recommendations based on past interactions.\n",
    "        \n",
    "        You have access to the following types of memory:\n",
    "        1. Short-term memory: The current conversation thread\n",
    "        2. Long-term memory: \n",
    "           - Episodic: User preferences and past trip experiences\n",
    "           - Procedural: How to book flights, hotels, etc.\n",
    "           - Semantic: General knowledge about travel destinations\n",
    "           \n",
    "        Always be helpful, personal, and context-aware in your responses.\n",
    "        \"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# TODO: Could be a function instead\n",
    "memory_manager = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[],\n",
    "    checkpointer=redis_saver,\n",
    "    response_format=Memories,\n",
    "    prompt=SystemMessage(\n",
    "        content=\"\"\"\n",
    "        You are a memory management assistant that helps extract important information from \n",
    "        conversations. Your job is to identify what information should be stored in long-term \n",
    "        memory from the conversation history.\n",
    "        \n",
    "        For each piece of information, determine:\n",
    "        1. Whether it should be stored as episodic, procedural, or semantic memory\n",
    "        2. What metadata should be attached to it\n",
    "        \n",
    "        Reply with a JSON-formatted list of memories to store. Example:\n",
    "        ```\n",
    "        [\n",
    "          {\n",
    "            \"content\": \"User prefers window seats on flights\",\n",
    "            \"type\": \"episodic\",\n",
    "            \"metadata\": {\"category\": \"flight_preference\", \"importance\": \"high\"}\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"Steps to book a flight with layover preferences\",\n",
    "            \"type\": \"procedural\",\n",
    "            \"metadata\": {\"category\": \"booking_process\"}\n",
    "          }\n",
    "        ]\n",
    "        ```\n",
    "        \n",
    "        Only extract information that would be useful in future conversations.\n",
    "        \"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def similar_memory_exists(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    user_id: str,\n",
    "    similarity_threshold: float = 0.2,\n",
    ") -> bool:\n",
    "    \"\"\"Check if a similar memory already exists in the database\"\"\"\n",
    "    # Create embedding for the new content\n",
    "    query_embedding = openai_embed.embed_query(content)\n",
    "\n",
    "    of_type_for_user = (Tag(\"user_id\") == user_id) & (Tag(\"memory_type\") == memory_type)\n",
    "\n",
    "    # Search for similar memories\n",
    "    vector_query = VectorRangeQuery(\n",
    "        vector=query_embedding,\n",
    "        num_results=1,\n",
    "        vector_field_name=\"embedding\",\n",
    "        filter_expression=of_type_for_user,\n",
    "        distance_threshold=similarity_threshold,\n",
    "        return_fields=[\"content\"],\n",
    "    )\n",
    "    results = memory_index.query(vector_query)\n",
    "    logger.debug(f\"Similar memory search results: {results}\")\n",
    "\n",
    "    if results:\n",
    "        logger.debug(f\"Similar memory found: {results[0]['id']}\")\n",
    "        logger.info(f\"Similar memory found, skipping storage: {content}\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def store_memory(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    user_id: str,\n",
    "    metadata: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Store a memory in Redis, avoiding duplicates\"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = \"{}\"\n",
    "\n",
    "    logger.info(f\"Preparing to store memory: {content}\")\n",
    "\n",
    "    if similar_memory_exists(content, memory_type, user_id):\n",
    "        logger.info(\"Similar memory found, skipping storage\")\n",
    "        return\n",
    "\n",
    "    embedding = openai_embed.embed_query(content)\n",
    "\n",
    "    memory_data = {\n",
    "        \"user_id\": user_id,\n",
    "        \"content\": content,\n",
    "        \"memory_type\": memory_type.value,\n",
    "        \"metadata\": metadata,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"embedding\": embedding,\n",
    "        \"memory_id\": str(ulid.ULID()),\n",
    "    }\n",
    "\n",
    "    # Store in Redis\n",
    "    try:\n",
    "        memory_index.load([memory_data])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error storing memory: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Stored {memory_type} memory: {content}\")\n",
    "\n",
    "\n",
    "def retrieve_memories(\n",
    "    query: str, memory_type: Optional[MemoryType] = None, limit: int = 5\n",
    ") -> StoredMemories:\n",
    "    \"\"\"Retrieve relevant memories from Redis\"\"\"\n",
    "    # Create vector query\n",
    "    logger.debug(f\"Retrieving memories for query: {query}\")\n",
    "    vector_query = VectorRangeQuery(\n",
    "        vector=openai_embed.embed_query(query),\n",
    "        return_fields=[\"content\", \"memory_type\", \"metadata\", \"created_at\"],\n",
    "        num_results=limit,\n",
    "        vector_field_name=\"embedding\",\n",
    "        distance_threshold=0.2,\n",
    "    )\n",
    "\n",
    "    # Add filter for memory type if specified\n",
    "    if memory_type:\n",
    "        vector_query.set_filter(f\"@memory_type:{{{memory_type}}}\")\n",
    "\n",
    "    # Execute search\n",
    "    results = memory_index.query(vector_query)\n",
    "\n",
    "    # Parse results\n",
    "    memories = []\n",
    "    for doc in results:\n",
    "        try:\n",
    "            memory = StoredMemory(\n",
    "                id=doc[\"id\"],\n",
    "                memory_id=doc[\"memory_id\"],\n",
    "                user_id=doc[\"user_id\"],\n",
    "                content=doc[\"content\"],\n",
    "                type=MemoryType(doc[\"memory_type\"]),\n",
    "                created_at=doc[\"created_at\"],\n",
    "                metadata=doc[\"metadata\"],\n",
    "            )\n",
    "            memories.append(memory)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing memory: {e}\")\n",
    "            continue\n",
    "    return StoredMemories(memories=memories)\n",
    "\n",
    "\n",
    "def extract_memories(\n",
    "    last_processed_message_id: Optional[str],\n",
    "    state: RuntimeState,\n",
    "    config: RunnableConfig,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Extract and store memories in long-term memory\"\"\"\n",
    "    if len(state[\"messages\"]) < 3:  # Need at least a user message and agent response\n",
    "        return last_processed_message_id\n",
    "\n",
    "    user_id = config.get(\"configurable\", {}).get(\"user_id\", None)\n",
    "    if not user_id:\n",
    "        logger.warning(\"No user ID found in config when extracting memories\")\n",
    "        return last_processed_message_id\n",
    "\n",
    "    # Get the messages\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Find the newest message ID (or None if no IDs)\n",
    "    newest_message_id = None\n",
    "    for msg in reversed(messages):\n",
    "        if hasattr(msg, \"id\") and msg.id:\n",
    "            newest_message_id = msg.id\n",
    "            break\n",
    "\n",
    "    # If we've already processed up to this message ID, skip\n",
    "    if (\n",
    "        last_processed_message_id\n",
    "        and newest_message_id\n",
    "        and last_processed_message_id == newest_message_id\n",
    "    ):\n",
    "        logger.debug(f\"Already processed messages up to ID {newest_message_id}\")\n",
    "        return last_processed_message_id\n",
    "\n",
    "    # Get the last few messages for context\n",
    "    recent_messages = messages[-5:] if len(messages) > 5 else messages\n",
    "\n",
    "    # Format messages for the memory agent\n",
    "    message_history = \"\\n\".join(\n",
    "        [\n",
    "            f\"{'User' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content}\"\n",
    "            for msg in recent_messages\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Ask memory agent to extract memories\n",
    "    prompt = f\"\"\"\n",
    "    Please analyze this recent conversation and extract important information that \n",
    "    should be stored in long-term memory:\n",
    "    \n",
    "    {message_history}\n",
    "    \n",
    "    What information should be stored in long-term memory?\n",
    "    \"\"\"\n",
    "\n",
    "    result = memory_manager.invoke(\n",
    "        {\"messages\": [HumanMessage(content=prompt)]}, config=config\n",
    "    )\n",
    "    memories_to_store: Memories = result[\"structured_response\"]\n",
    "\n",
    "    # Store each extracted memory\n",
    "    for memory_data in memories_to_store.memories:\n",
    "        store_memory(\n",
    "            content=memory_data.content,\n",
    "            memory_type=memory_data.type,\n",
    "            user_id=user_id,\n",
    "            metadata=memory_data.metadata,\n",
    "        )\n",
    "\n",
    "    # Return data with the newest processed message ID\n",
    "    return newest_message_id\n",
    "\n",
    "\n",
    "def retrieve_relevant_memories(\n",
    "    state: RuntimeState, config: RunnableConfig\n",
    ") -> RuntimeState:\n",
    "    \"\"\"Retrieve relevant memories based on the current conversation\"\"\"\n",
    "    if not state[\"messages\"]:\n",
    "        logger.debug(\"No messages in state\")\n",
    "        return state\n",
    "\n",
    "    logger.debug(f\"inbound retrieve_relevant_memories: {len(state['messages'])}\")\n",
    "\n",
    "    latest_message = state[\"messages\"][-1]\n",
    "    if not isinstance(latest_message, HumanMessage):\n",
    "        logger.debug(\"Latest message is not a HumanMessage: \", latest_message)\n",
    "        return state\n",
    "\n",
    "    query = str(latest_message.content)\n",
    "    episodic = retrieve_memories(query=query, memory_type=MemoryType.EPISODIC, limit=3)\n",
    "    procedural = retrieve_memories(\n",
    "        query=query, memory_type=MemoryType.PROCEDURAL, limit=2\n",
    "    )\n",
    "    semantic = retrieve_memories(query=query, memory_type=MemoryType.SEMANTIC, limit=2)\n",
    "    relevant_memories = episodic.memories + procedural.memories + semantic.memories\n",
    "\n",
    "    logger.debug(f\"All relevant memories: {relevant_memories}\")\n",
    "\n",
    "    if relevant_memories:\n",
    "        memory_context = \"\\n\\n### Relevant memories from previous conversations:\\n\"\n",
    "\n",
    "        # Group by memory type\n",
    "        memory_types = {\n",
    "            MemoryType.EPISODIC: \"User Preferences & History\",\n",
    "            MemoryType.PROCEDURAL: \"Booking Procedures\",\n",
    "            MemoryType.SEMANTIC: \"Destination Knowledge\",\n",
    "        }\n",
    "\n",
    "        for mem_type, type_label in memory_types.items():\n",
    "            memories_of_type = [m for m in relevant_memories if m.type == mem_type]\n",
    "            if memories_of_type:\n",
    "                memory_context += f\"\\n**{type_label}**:\\n\"\n",
    "                for mem in memories_of_type:\n",
    "                    memory_context += f\"- {mem.content}\\n\"\n",
    "\n",
    "        augmented_message = HumanMessage(content=f\"{query}\\n{memory_context}\")\n",
    "        state[\"messages\"][-1] = augmented_message\n",
    "\n",
    "        logger.debug(f\"Augmented message: {augmented_message.content}\")\n",
    "\n",
    "    logger.debug(f\"outbound retrieve_relevant_memories: {len(state['messages'])}\")\n",
    "\n",
    "    return state.copy()\n",
    "\n",
    "\n",
    "def summarize_conversation(\n",
    "    state: RuntimeState, config: RunnableConfig\n",
    ") -> Optional[RuntimeState]:\n",
    "    \"\"\"\n",
    "    Summarize a list of messages into a concise summary\n",
    "    to reduce context length while preserving important information.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    current_message_count = len(messages)\n",
    "    if current_message_count <= MESSAGE_SUMMARIZATION_THRESHOLD:\n",
    "        logger.debug(f\"Not summarizing conversation: {current_message_count}\")\n",
    "        return None\n",
    "\n",
    "    # Summarize all but the latest message\n",
    "    messages_to_summarize = state[\"messages\"][:-1]\n",
    "\n",
    "    # Create a system prompt for the summarizer\n",
    "    system_prompt = \"\"\"\n",
    "    You are a conversation summarizer. Your task is to create a concise summary \n",
    "    of the previous conversation between a user and a travel assistant.\n",
    "    \n",
    "    The summary should:\n",
    "    1. Highlight key topics, preferences, and decisions\n",
    "    2. Include any specific trip details (destinations, dates, preferences)\n",
    "    3. Note any outstanding questions or topics that need follow-up\n",
    "    4. Be concise but informative\n",
    "    \n",
    "    Format your summary as a brief narrative paragraph.\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke the summarizer\n",
    "    summary_messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(\n",
    "            content=f\"Please summarize this conversation:\\n\\n{messages_to_summarize}\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    summary_response = summarizer.invoke(summary_messages, config=config)\n",
    "\n",
    "    logger.info(f\"Summarized {len(messages)} messages into a conversation summary\")\n",
    "\n",
    "    summary_message = SystemMessage(\n",
    "        content=f\"\"\"\n",
    "        Summary of the conversation so far:\n",
    "        \n",
    "        {summary_response.content}\n",
    "            \n",
    "            Please continue the conversation based on this summary and the recent messages.\n",
    "        \"\"\"\n",
    "    )\n",
    "    remove_messages = [\n",
    "        RemoveMessage(id=msg.id) for msg in messages_to_summarize if msg.id is not None\n",
    "    ]\n",
    "\n",
    "    state[\"messages\"] = [  # type: ignore\n",
    "        *remove_messages,\n",
    "        summary_message,\n",
    "        state[\"messages\"][-1],\n",
    "    ]\n",
    "\n",
    "    return state.copy()\n",
    "\n",
    "\n",
    "def respond_to_user(state: RuntimeState, config: RunnableConfig) -> RuntimeState:\n",
    "    \"\"\"Generate a response to the user based on the conversation and memories\"\"\"\n",
    "    if not state[\"messages\"]:\n",
    "        return state\n",
    "\n",
    "    # Invoke the travel agent with the context messages\n",
    "    result = travel_agent.invoke({\"messages\": state[\"messages\"]}, config=config)\n",
    "    result_messages = result.get(\"messages\", [])\n",
    "\n",
    "    if result_messages and any(isinstance(m, AIMessage) for m in result_messages):\n",
    "        # Find the last AI message in the result\n",
    "        ai_messages = [m for m in result_messages if isinstance(m, AIMessage)]\n",
    "        if ai_messages:\n",
    "            agent_response = ai_messages[-1]  # Get the last AI message\n",
    "            state[\"messages\"].append(agent_response)\n",
    "        else:\n",
    "            logger.error(\"No AIMessage found in result messages\")\n",
    "            agent_response = AIMessage(\n",
    "                content=\"I'm sorry, I couldn't understand your request.\"\n",
    "            )\n",
    "    else:\n",
    "        logger.error(\"No valid assistant response found in result\")\n",
    "        agent_response = AIMessage(\n",
    "            content=\"I'm sorry, I couldn't process your request properly.\"\n",
    "        )\n",
    "\n",
    "    state[\"messages\"].append(agent_response)\n",
    "\n",
    "    logger.debug(f\"respond_to_user: Returning {len(state['messages'])} messages\")\n",
    "\n",
    "    return state.copy()\n",
    "\n",
    "\n",
    "def memory_worker(memory_queue: Queue, user_id: str):\n",
    "    \"\"\"Worker function that processes memory extraction requests from a queue\"\"\"\n",
    "    key = f\"memory_worker:{user_id}:last_processed_message_id\"\n",
    "\n",
    "    last_processed_message_id = redis_client.get(key)\n",
    "    logger.debug(f\"Last processed message ID: {last_processed_message_id}\")\n",
    "    last_processed_message_id = (\n",
    "        str(last_processed_message_id) if last_processed_message_id else None\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get the next state and config from the queue (blocks until an item is available)\n",
    "            state, config = memory_queue.get()\n",
    "\n",
    "            # Process the memory extraction\n",
    "            last_processed_message_id = extract_memories(\n",
    "                last_processed_message_id, state, config\n",
    "            )\n",
    "            logger.debug(\n",
    "                f\"Memory worker extracted memories. Last processed message ID: {last_processed_message_id}\"\n",
    "            )\n",
    "\n",
    "            if last_processed_message_id:\n",
    "                logger.debug(\n",
    "                    f\"Setting last processed message ID: {last_processed_message_id}\"\n",
    "                )\n",
    "                redis_client.set(key, last_processed_message_id)\n",
    "\n",
    "            # Mark the task as done\n",
    "            memory_queue.task_done()\n",
    "            logger.debug(\"Memory extraction completed for queue item\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in memory worker thread: {e}\")\n",
    "\n",
    "\n",
    "def consolidate_memories(user_id: str):\n",
    "    \"\"\"\n",
    "    Periodically scan the memory database and merge similar memories.\n",
    "    This should be run as a scheduled task.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting memory consolidation for user {user_id}\")\n",
    "\n",
    "    # First, get all memories for the user\n",
    "    try:\n",
    "        # For each memory type, consolidate separately\n",
    "        for memory_type in MemoryType:\n",
    "            # Get all memories of this type for the user\n",
    "            of_type_for_user = (Tag(\"user_id\") == user_id) & (\n",
    "                Tag(\"memory_type\") == memory_type\n",
    "            )\n",
    "            filter_query = FilterQuery(filter_expression=of_type_for_user)\n",
    "            all_memories = memory_index.query(filter_query)\n",
    "            if not all_memories:\n",
    "                continue\n",
    "\n",
    "            # Group similar memories\n",
    "            processed_ids = set()\n",
    "            for memory in all_memories:\n",
    "                if memory[\"id\"] in processed_ids:\n",
    "                    continue\n",
    "\n",
    "                memory_embedding = memory[\"embedding\"]\n",
    "                vector_query = VectorRangeQuery(\n",
    "                    vector=memory_embedding,\n",
    "                    num_results=10,\n",
    "                    vector_field_name=\"embedding\",\n",
    "                    filter_expression=of_type_for_user\n",
    "                    & (Tag(\"memory_id\") != memory[\"memory_id\"]),\n",
    "                    distance_threshold=0.2,\n",
    "                    return_fields=[\n",
    "                        \"content\",\n",
    "                        \"memory_type\",\n",
    "                        \"metadata\",\n",
    "                        \"user_id\",\n",
    "                        \"memory_id\",\n",
    "                    ],\n",
    "                )\n",
    "                similar_memories = memory_index.query(vector_query)\n",
    "\n",
    "                # If we found similar memories, consolidate them\n",
    "                if similar_memories:\n",
    "                    combined_content = memory[\"content\"]\n",
    "                    combined_metadata = memory[\"metadata\"]\n",
    "\n",
    "                    if combined_metadata:\n",
    "                        try:\n",
    "                            combined_metadata = json.loads(combined_metadata)\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Error parsing metadata: {e}\")\n",
    "                            combined_metadata = {}\n",
    "\n",
    "                    for similar in similar_memories:\n",
    "                        # Merge the content of similar memories\n",
    "                        combined_content += f\" {similar['content']}\"\n",
    "\n",
    "                        if similar[\"metadata\"]:\n",
    "                            try:\n",
    "                                similar_metadata = json.loads(similar[\"metadata\"])\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"Error parsing metadata: {e}\")\n",
    "                            similar_metadata = {}\n",
    "\n",
    "                        combined_metadata = {**combined_metadata, **similar_metadata}\n",
    "\n",
    "                    # Create a consolidated memory\n",
    "                    new_metadata = {\n",
    "                        \"consolidated\": True,\n",
    "                        \"source_count\": len(similar_memories) + 1,\n",
    "                        **combined_metadata,\n",
    "                    }\n",
    "                    consolidated_memory = {\n",
    "                        \"content\": summarize_memories(combined_content, memory_type),\n",
    "                        \"memory_type\": memory_type.value,\n",
    "                        \"metadata\": json.dumps(new_metadata),\n",
    "                        \"user_id\": user_id,\n",
    "                    }\n",
    "\n",
    "                    # Delete the old memories\n",
    "                    delete_memory(memory[\"id\"])\n",
    "                    for similar in similar_memories:\n",
    "                        delete_memory(similar[\"id\"])\n",
    "\n",
    "                    # Store the new consolidated memory\n",
    "                    store_memory(\n",
    "                        content=consolidated_memory[\"content\"],\n",
    "                        memory_type=memory_type,\n",
    "                        user_id=user_id,\n",
    "                        metadata=consolidated_memory[\"metadata\"],\n",
    "                    )\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"Consolidated {len(similar_memories) + 1} memories into one\"\n",
    "                    )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        # logger.error(f\"Error during memory consolidation: {e}\")\n",
    "\n",
    "\n",
    "def delete_memory(memory_id: str):\n",
    "    \"\"\"Delete a memory from Redis\"\"\"\n",
    "    try:\n",
    "        result = memory_index.drop_keys([memory_id])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Deleting memory {memory_id} failed: {e}\")\n",
    "    if result == 0:\n",
    "        logger.debug(f\"Deleting memory {memory_id} failed: memory not found\")\n",
    "    else:\n",
    "        logger.info(f\"Deleted memory {memory_id}\")\n",
    "\n",
    "\n",
    "def summarize_memories(combined_content: str, memory_type: MemoryType) -> str:\n",
    "    \"\"\"Use the LLM to create a concise summary of similar memories\"\"\"\n",
    "    try:\n",
    "        system_prompt = f\"\"\"\n",
    "        You are a memory consolidation assistant. Your task is to create a single, \n",
    "        concise memory from these similar memory fragments. This is a {memory_type.value} memory.\n",
    "        \n",
    "        Combine the information without repetition while preserving all important details.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(\n",
    "                content=f\"Please consolidate these similar memories into one:\\n\\n{combined_content}\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        response = summarizer.invoke(messages)\n",
    "        return str(response.content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error summarizing memories: {e}\")\n",
    "        # Fall back to just using the combined content\n",
    "        return combined_content\n",
    "\n",
    "\n",
    "workflow = StateGraph(RuntimeState)\n",
    "\n",
    "workflow.add_node(\"retrieve_memories\", retrieve_relevant_memories)\n",
    "workflow.add_node(\"respond\", respond_to_user)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve_memories\")\n",
    "workflow.add_edge(\"retrieve_memories\", \"respond\")\n",
    "workflow.add_edge(\"respond\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile(checkpointer=redis_saver)\n",
    "\n",
    "\n",
    "def memory_consolidation_worker(user_id: str):\n",
    "    \"\"\"Worker that periodically consolidates memories\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            consolidate_memories(user_id)\n",
    "            # Run every 10 minutes\n",
    "            time.sleep(10 * 60)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in memory consolidation worker: {e}\")\n",
    "            # If there's an error, wait an hour and try again\n",
    "            time.sleep(60 * 60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main interaction loop for the travel agent\"\"\"\n",
    "    print(\"Welcome to the Travel Assistant! (Type 'exit' to quit)\")\n",
    "    user_id = \"demo_user\"\n",
    "\n",
    "    config = RunnableConfig(\n",
    "        configurable={\"thread_id\": \"book_flight\", \"user_id\": user_id}\n",
    "    )\n",
    "    state = RuntimeState(messages=[])\n",
    "\n",
    "    # Create a queue for memory processing\n",
    "    memory_queue = Queue()\n",
    "\n",
    "    # Start a worker thread that will process memory extraction tasks\n",
    "    memory_thread = threading.Thread(\n",
    "        target=memory_worker, args=(memory_queue, user_id), daemon=True\n",
    "    )\n",
    "    memory_thread.start()\n",
    "\n",
    "    # Start the memory consolidation thread\n",
    "    consolidation_thread = threading.Thread(\n",
    "        target=memory_consolidation_worker, args=(user_id,), daemon=True\n",
    "    )\n",
    "    consolidation_thread.start()\n",
    "\n",
    "    # Pre-seed some knowledge\n",
    "    store_memory(\n",
    "        content=\"\"\"\n",
    "        Popular tourist destinations include Paris, Tokyo, New York, and Rome\n",
    "        \"\"\",\n",
    "        memory_type=MemoryType.SEMANTIC,\n",
    "        user_id=user_id,\n",
    "        metadata='{\"category\": \"destinations\"}',\n",
    "    )\n",
    "\n",
    "    store_memory(\n",
    "        content=\"\"\"\n",
    "        When booking flights, always check for layover duration - at least 1\n",
    "        hour for domestic and 2 hours for international flights is recommended\n",
    "        \"\"\",\n",
    "        memory_type=MemoryType.PROCEDURAL,\n",
    "        user_id=user_id,\n",
    "        metadata='{\"category\": \"booking_tips\"}',\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Thank you for using the Travel Assistant. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "\n",
    "        try:\n",
    "            # Process the user input through the graph\n",
    "            final_state = None\n",
    "            print(f\"Incoming state: {len(state['messages'])}\")\n",
    "            for result in graph.stream(state, config=config, stream_mode=\"values\"):\n",
    "                final_state = RuntimeState(**result)\n",
    "\n",
    "            # Use the final state to get and print the assistant's response once\n",
    "            if final_state and final_state[\"messages\"]:\n",
    "                logger.debug(f\"Len of messages: {len(final_state['messages'])}\")\n",
    "                logger.debug(f\"Final messages: {len(final_state['messages'])}\")\n",
    "\n",
    "                assistant_message = final_state[\"messages\"][-1]\n",
    "                if isinstance(assistant_message, AIMessage):\n",
    "                    print(f\"\\nA: {assistant_message.content}\")\n",
    "\n",
    "                summarized_state = summarize_conversation(final_state, config)\n",
    "                if summarized_state:\n",
    "                    logger.debug(\n",
    "                        f\"Using new summarized state: {len(summarized_state['messages'])}\"\n",
    "                    )\n",
    "                    final_state = summarized_state\n",
    "                    graph.update_state(\n",
    "                        config, values=final_state, as_node=\"retrieve_memories\"\n",
    "                    )\n",
    "\n",
    "                state = final_state\n",
    "\n",
    "            # Add the current state to the memory processing queue\n",
    "            memory_queue.put((state.copy(), config))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error processing request: {e}\")\n",
    "            print(\n",
    "                \"\\nAssistant: I'm sorry, I encountered an error processing your request.\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Agent Memory Using Redis and LangGraph\n",
    "This notebook demonstrates how to manage short-term and long-term agent memory using Redis and LangGraph. We'll explore:\n",
    "\n",
    "1. Short-term memory management using LangGraph's checkpointer\n",
    "2. Long-term memory storage and retrieval using RedisVL\n",
    "3. Manually storing and retrieving long-term memory vs. exposing tool access (AKA function-calling)\n",
    "4. Managing conversation history size with summarization\n",
    "5. Memory consolidation and decay\n",
    "\n",
    "## Let's Begin!\n",
    "<a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/agents/03_memory_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5075.53s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain-openai langgraph-checkpoint langgraph-checkpoint-redis \"langchain-community>=0.2.11\" tavily-python langchain-redis pydantic ulid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPEN_AI_API key\n",
    "\n",
    "You must add an OpenAI API key with billing information enabled is required for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run redis\n",
    "\n",
    "### For colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "%%sh\n",
    "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
    "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
    "sudo apt-get update  > /dev/null 2>&1\n",
    "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
    "redis-stack-server --daemonize ye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Alternative Environments\n",
    "There are many ways to get the necessary redis-stack instance running\n",
    "1. On cloud, deploy a [FREE instance of Redis in the cloud](https://redis.com/try-free/). Or, if you have your\n",
    "own version of Redis Enterprise running, that works too!\n",
    "2. Per OS, [see the docs](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/)\n",
    "3. With docker: `docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest`\n",
    "\n",
    "## Test connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from redis import Redis\n",
    "\n",
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "\n",
    "redis_client = Redis.from_url(REDIS_URL)\n",
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-Term vs. Long-Term Memory\n",
    "\n",
    "The agent uses **short-term memory** and **long-term memory**. The implementations\n",
    "of short-term and long-term memory differ, as does how agent uses them. Let's\n",
    "dig into the details. We'll return to code soon!\n",
    "\n",
    "### Short-Term Memory\n",
    "\n",
    "For short-term memory, the agent keeps track of conversation history with Redis.\n",
    "Because this is a LangGraph agent, we use the `RedisSaver` class to achieve\n",
    "this. `RedisSaver` is what LangGraph refers to as a _checkpointer_. You can read\n",
    "more about checkpointers in the [LangGraph\n",
    "documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/).\n",
    "\n",
    "If Redis persistence is on, then Redis will persist short-term memory to\n",
    "disk. This means if you quit the agent and return with the same thread ID and\n",
    "user ID, you'll resume the same conversation.\n",
    "\n",
    "Conversation histories can grow long and pollute an LLM's context window. To manage\n",
    "this, after every \"turn\" of a conversation, the agent summarizes messages when the\n",
    "conversation grows past a configurable threshold. Checkpointers do not do this by\n",
    "default, so we've created a node in the graph for summarization.\n",
    "\n",
    "**NOTE**: We'll see example code for the summarization node later in this notebook.\n",
    "\n",
    "### Long-Term Memory\n",
    "\n",
    "Aside from conversation history, the agent stores long-term memories in a search\n",
    "index in Redis, using [RedisVL](https://docs.redisvl.com/en/latest/).\n",
    "\n",
    "The agent tracks two types of long-term memories:\n",
    "\n",
    "- **Episodic**: User-specific experiences and preferences\n",
    "- **Semantic**: General knowledge about travel destinations and requirements\n",
    "\n",
    "**NOTE** If you're familiar with the [CoALA\n",
    "paper](https://arxiv.org/abs/2309.02427), the terms \"episodic\" and \"semantic\"\n",
    "here map to the same concepts in the paper. CoALA discusses a third type of\n",
    "memory, _procedural_. In our example, we consider logic encoded in Python in the\n",
    "agent codebase to be its procedural memory.\n",
    "\n",
    "### Representing Long-Term Memory in Python\n",
    "We use a couple of Pydantic models to represent long-term memories, both before\n",
    "and after they're stored in Redis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import ulid\n",
    "\n",
    "\n",
    "class MemoryType(str, Enum):\n",
    "    \"\"\"\n",
    "    The type of a long-term memory.\n",
    "\n",
    "    EPISODIC: User specific experiences and preferences\n",
    "\n",
    "    SEMANTIC: General knowledge on top of the user's preferences and LLM's\n",
    "    training data.\n",
    "    \"\"\"\n",
    "\n",
    "    EPISODIC = \"episodic\"\n",
    "    SEMANTIC = \"semantic\"\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    \"\"\"Represents a single long-term memory.\"\"\"\n",
    "\n",
    "    content: str\n",
    "    memory_type: MemoryType\n",
    "    metadata: str\n",
    "    \n",
    "    \n",
    "class Memories(BaseModel):\n",
    "    \"\"\"\n",
    "    A list of memories extracted from a conversation by an LLM.\n",
    "\n",
    "    NOTE: OpenAI's structured output requires us to wrap the list in an object.\n",
    "    \"\"\"\n",
    "\n",
    "    memories: List[Memory]\n",
    "\n",
    "\n",
    "class StoredMemory(Memory):\n",
    "    \"\"\"A stored long-term memory\"\"\"\n",
    "\n",
    "    id: str  # The redis key\n",
    "    memory_id: ulid.ULID = Field(default_factory=lambda: ulid.ULID())\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    user_id: Optional[str] = None\n",
    "    thread_id: Optional[str] = None\n",
    "    memory_type: Optional[MemoryType] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll return to these models soon, to see them in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-Term Memory Storage and Retrieval\n",
    "\n",
    "The `RedisSaver` class handles the basics of short-term memory storage for us,\n",
    "so we don't need to do anything here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Term Memory Storage and Retrieval\n",
    "\n",
    "We use RedisVL to store and retrieve long-term memories with vector embeddings.\n",
    "This allows for semantic search of past experiences and knowledge.\n",
    "\n",
    "Let's set up a new search index to store and query memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.index import SearchIndex\n",
    "from redisvl.schema.schema import IndexSchema\n",
    "\n",
    "# Define schema for long-term memory index\n",
    "memory_schema = IndexSchema(\n",
    "    **{\n",
    "        \"index\": {\n",
    "            \"name\": \"agent_memories\",\n",
    "            \"prefix\": \"memory:\",\n",
    "            \"key_separator\": \":\",\n",
    "            \"storage_type\": \"json\",\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"content\", \"type\": \"text\"},\n",
    "            {\"name\": \"memory_type\", \"type\": \"tag\"},\n",
    "            {\"name\": \"metadata\", \"type\": \"text\"},\n",
    "            {\"name\": \"created_at\", \"type\": \"text\"},\n",
    "            {\"name\": \"user_id\", \"type\": \"tag\"},\n",
    "            {\"name\": \"memory_id\", \"type\": \"tag\"},\n",
    "            {\n",
    "                \"name\": \"embedding\",\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"algorithm\": \"flat\",\n",
    "                    \"dims\": 1536,  # OpenAI embedding dimension\n",
    "                    \"distance_metric\": \"cosine\",\n",
    "                    \"datatype\": \"float32\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create search index\n",
    "try:\n",
    "    long_term_memory_index = SearchIndex(\n",
    "        schema=memory_schema, redis_client=redis_client, overwrite=True\n",
    "    )\n",
    "    long_term_memory_index.create()\n",
    "    print(\"Long-term memory index ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage and Retrieval Functions\n",
    "\n",
    "Now that we have a search index in Redis, we can write functions to store and\n",
    "retrieve memories. We can use RedisVL to write these.\n",
    "\n",
    "First, we'll write a utility function to check if a memory similar to a given\n",
    "memory already exists in the index. Later, we can use this to avoid storing\n",
    "duplicate memories.\n",
    "\n",
    "#### Checking for Similar Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from redisvl.query import VectorRangeQuery\n",
    "from redisvl.query.filter import Tag\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# If we have any memories that aren't associated with a user, we'll use this ID.\n",
    "SYSTEM_USER_ID = \"system\"\n",
    "\n",
    "openai_embed = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "\n",
    "def similar_memory_exists(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    distance_threshold: float = 0.1,\n",
    ") -> bool:\n",
    "    \"\"\"Check if a similar long-term memory already exists in Redis.\"\"\"\n",
    "    query_embedding = openai_embed.embed_query(content)\n",
    "    filters = (Tag(\"user_id\") == user_id) & (Tag(\"memory_type\") == memory_type)\n",
    "    if thread_id:\n",
    "        filters = filters & (Tag(\"thread_id\") == thread_id)\n",
    "\n",
    "    # Search for similar memories\n",
    "    vector_query = VectorRangeQuery(\n",
    "        vector=query_embedding,\n",
    "        num_results=1,\n",
    "        vector_field_name=\"embedding\",\n",
    "        filter_expression=filters,\n",
    "        distance_threshold=distance_threshold,\n",
    "        return_fields=[\"id\"],\n",
    "    )\n",
    "    results = long_term_memory_index.query(vector_query)\n",
    "    logger.debug(f\"Similar memory search results: {results}\")\n",
    "\n",
    "    if results:\n",
    "        logger.debug(\n",
    "            f\"{len(results)} similar {'memory' if results.count == 1 else 'memories'} found. First: \"\n",
    "            f\"{results[0]['id']}. Skipping storage.\"\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing and Retrieving Long-Term Memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `similar_memory_exists()` function when we store memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import ulid\n",
    "\n",
    "\n",
    "def store_memory(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    metadata: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Store a long-term memory in Redis, avoiding duplicates.\"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = \"{}\"\n",
    "\n",
    "    logger.info(f\"Preparing to store memory: {content}\")\n",
    "\n",
    "    if similar_memory_exists(content, memory_type, user_id, thread_id):\n",
    "        logger.info(\"Similar memory found, skipping storage\")\n",
    "        return\n",
    "\n",
    "    embedding = openai_embed.embed_query(content)\n",
    "\n",
    "    memory_data = {\n",
    "        \"user_id\": user_id or SYSTEM_USER_ID,\n",
    "        \"content\": content,\n",
    "        \"memory_type\": memory_type.value,\n",
    "        \"metadata\": metadata,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"embedding\": embedding,\n",
    "        \"memory_id\": str(ulid.ULID()),\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        long_term_memory_index.load([memory_data])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error storing memory: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Stored {memory_type} memory: {content}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we're storing memories, we can retrieve them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_memories(\n",
    "    query: str,\n",
    "    memory_type: Union[Optional[MemoryType], List[MemoryType]] = None,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    distance_threshold: float = 0.1,\n",
    "    limit: int = 5,\n",
    ") -> List[StoredMemory]:\n",
    "    \"\"\"Retrieve relevant memories from Redis\"\"\"\n",
    "    # Create vector query\n",
    "    logger.debug(f\"Retrieving memories for query: {query}\")\n",
    "    vector_query = VectorRangeQuery(\n",
    "        vector=openai_embed.embed_query(query),\n",
    "        return_fields=[\n",
    "            \"content\",\n",
    "            \"memory_type\",\n",
    "            \"metadata\",\n",
    "            \"created_at\",\n",
    "            \"memory_id\",\n",
    "            \"thread_id\",\n",
    "            \"user_id\",\n",
    "        ],\n",
    "        num_results=limit,\n",
    "        vector_field_name=\"embedding\",\n",
    "        dialect=2,\n",
    "        distance_threshold=distance_threshold,\n",
    "    )\n",
    "\n",
    "    base_filters = [f\"@user_id:{{{user_id or SYSTEM_USER_ID}}}\"]\n",
    "\n",
    "    if memory_type:\n",
    "        if isinstance(memory_type, list):\n",
    "            base_filters.append(f\"@memory_type:{{{'|'.join(memory_type)}}}\")\n",
    "        else:\n",
    "            base_filters.append(f\"@memory_type:{{{memory_type.value}}}\")\n",
    "\n",
    "    if thread_id:\n",
    "        base_filters.append(f\"@thread_id:{{{thread_id}}}\")\n",
    "\n",
    "    vector_query.set_filter(\" \".join(base_filters))\n",
    "\n",
    "    # Execute search\n",
    "    results = long_term_memory_index.query(vector_query)\n",
    "\n",
    "    # Parse results\n",
    "    memories = []\n",
    "    for doc in results:\n",
    "        try:\n",
    "            memory = StoredMemory(\n",
    "                id=doc[\"id\"],\n",
    "                memory_id=doc[\"memory_id\"],\n",
    "                user_id=doc[\"user_id\"],\n",
    "                thread_id=doc.get(\"thread_id\", None),\n",
    "                memory_type=MemoryType(doc[\"memory_type\"]),\n",
    "                content=doc[\"content\"],\n",
    "                created_at=doc[\"created_at\"],\n",
    "                metadata=doc[\"metadata\"],\n",
    "            )\n",
    "            memories.append(memory)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing memory: {e}\")\n",
    "            continue\n",
    "    return memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Long-Term Memory Manually vs. Calling Tools\n",
    "\n",
    "While making LLM queries, agents can store and retrieve relevant long-term\n",
    "memories in one of two ways (and more, but these are the two we'll discuss):\n",
    "\n",
    "1. Expose memory retrieval and storage as \"tools\" that the LLM can decide to call contextually.\n",
    "2. Manually augment prompts with relevant memories, and manually extract and store relevant memories.\n",
    "\n",
    "These approaches both have tradeoffs.\n",
    "\n",
    "**Tool-calling** leaves the decision to store a memory or find relevant memories\n",
    "up to the LLM. This can add latency to requests. It will generally result in\n",
    "fewer calls to Redis but will also sometimes miss out on retrieving potentially\n",
    "relevant context and/or extracting relevant memories from a conversation.\n",
    "\n",
    "**Manual memory management** will result in more calls to Redis but will produce\n",
    "fewer round-trip LLM requests, reducing latency. Manually extracting memories\n",
    "will generally extract more memories than tool calls, which will store more data\n",
    "in Redis and should result in more context added to LLM requests. More context\n",
    "means more contextual awareness but also higher token spend.\n",
    "\n",
    "You can test both approaches with this agent by changing the `memory_strategy`\n",
    "variable.\n",
    "\n",
    "## Managing Memory Manually\n",
    "With the manual memory management strategy, we're going to extract memories after\n",
    "every interaction between the user and the agent. We're then going to retrieve\n",
    "those memories during future interactions before we send the query.\n",
    "\n",
    "### Extracting Memories\n",
    "We'll call this `extract_memories` function manually after each interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph.message import MessagesState\n",
    "\n",
    "\n",
    "class RuntimeState(MessagesState):\n",
    "    \"\"\"Agent state (just messages for now)\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "memory_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.3).with_structured_output(\n",
    "    Memories\n",
    ")\n",
    "\n",
    "\n",
    "def extract_memories(\n",
    "    last_processed_message_id: Optional[str],\n",
    "    state: RuntimeState,\n",
    "    config: RunnableConfig,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Extract and store memories in long-term memory\"\"\"\n",
    "    logger.debug(f\"Last message ID is: {last_processed_message_id}\")\n",
    "\n",
    "    if len(state[\"messages\"]) < 3:  # Need at least a user message and agent response\n",
    "        logger.debug(\"Not enough messages to extract memories\")\n",
    "        return last_processed_message_id\n",
    "\n",
    "    user_id = config.get(\"configurable\", {}).get(\"user_id\", None)\n",
    "    if not user_id:\n",
    "        logger.warning(\"No user ID found in config when extracting memories\")\n",
    "        return last_processed_message_id\n",
    "\n",
    "    # Get the messages\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Find the newest message ID (or None if no IDs)\n",
    "    newest_message_id = None\n",
    "    for msg in reversed(messages):\n",
    "        if hasattr(msg, \"id\") and msg.id:\n",
    "            newest_message_id = msg.id\n",
    "            break\n",
    "\n",
    "    logger.debug(f\"Newest message ID is: {newest_message_id}\")\n",
    "\n",
    "    # If we've already processed up to this message ID, skip\n",
    "    if (\n",
    "        last_processed_message_id\n",
    "        and newest_message_id\n",
    "        and last_processed_message_id == newest_message_id\n",
    "    ):\n",
    "        logger.debug(f\"Already processed messages up to ID {newest_message_id}\")\n",
    "        return last_processed_message_id\n",
    "\n",
    "    # Find the index of the message with last_processed_message_id\n",
    "    start_index = 0\n",
    "    if last_processed_message_id:\n",
    "        for i, msg in enumerate(messages):\n",
    "            if hasattr(msg, \"id\") and msg.id == last_processed_message_id:\n",
    "                start_index = i + 1  # Start processing from the next message\n",
    "                break\n",
    "\n",
    "    # Check if there are messages to process\n",
    "    if start_index >= len(messages):\n",
    "        logger.debug(\"No new messages to process since last processed message\")\n",
    "        return newest_message_id\n",
    "\n",
    "    # Get only the messages after the last processed message\n",
    "    messages_to_process = messages[start_index:]\n",
    "\n",
    "    # If there are not enough messages to process, include some context\n",
    "    if len(messages_to_process) < 3 and start_index > 0:\n",
    "        # Include up to 3 messages before the start_index for context\n",
    "        context_start = max(0, start_index - 3)\n",
    "        messages_to_process = messages[context_start:]\n",
    "\n",
    "    # Format messages for the memory agent\n",
    "    message_history = \"\\n\".join(\n",
    "        [\n",
    "            f\"{'User' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content}\"\n",
    "            for msg in messages_to_process\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a long-memory manager. Your job is to analyze this message history\n",
    "    and extract information that might be useful in future conversations.\n",
    "    \n",
    "    Extract two types of memories:\n",
    "    1. EPISODIC: Personal experiences and preferences specific to this user\n",
    "       Example: \"User prefers window seats\" or \"User had a bad experience in Paris\"\n",
    "    \n",
    "    2. SEMANTIC: General facts and knowledge about travel that could be useful\n",
    "       Example: \"The best time to visit Japan is during cherry blossom season in April\"\n",
    "    \n",
    "    For each memory, provide:\n",
    "    - Type: The memory type (EPISODIC/SEMANTIC)\n",
    "    - Content: The actual information to store\n",
    "    - Metadata: Relevant tags and context (as JSON)\n",
    "    \n",
    "    IMPORTANT RULES:\n",
    "    1. Only extract information that would be genuinely useful for future interactions.\n",
    "    2. Do not extract procedural knowledge - that is handled by the system's built-in tools and prompts.\n",
    "    3. You are a large language model, not a human - do not extract facts that you already know.\n",
    "    \n",
    "    Message history:\n",
    "    {message_history}\n",
    "    \n",
    "    Extracted memories:\n",
    "    \"\"\"\n",
    "\n",
    "    memories_to_store: Memories = memory_llm.invoke([HumanMessage(content=prompt)])  # type: ignore\n",
    "\n",
    "    # Store each extracted memory\n",
    "    for memory_data in memories_to_store.memories:\n",
    "        store_memory(\n",
    "            content=memory_data.content,\n",
    "            memory_type=memory_data.memory_type,\n",
    "            user_id=user_id,\n",
    "            metadata=memory_data.metadata,\n",
    "        )\n",
    "\n",
    "    # Return data with the newest processed message ID\n",
    "    return newest_message_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On future interactions, we'll query for relevant memories and add them to\n",
    "the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_memories(\n",
    "    state: RuntimeState, config: RunnableConfig\n",
    ") -> RuntimeState:\n",
    "    \"\"\"Retrieve relevant memories based on the current conversation.\"\"\"\n",
    "    if not state[\"messages\"]:\n",
    "        logger.debug(\"No messages in state\")\n",
    "        return state\n",
    "\n",
    "    latest_message = state[\"messages\"][-1]\n",
    "    if not isinstance(latest_message, HumanMessage):\n",
    "        logger.debug(\"Latest message is not a HumanMessage: \", latest_message)\n",
    "        return state\n",
    "\n",
    "    user_id = config.get(\"configurable\", {}).get(\"user_id\", SYSTEM_USER_ID)\n",
    "\n",
    "    query = str(latest_message.content)\n",
    "    relevant_memories = retrieve_memories(\n",
    "        query=query,\n",
    "        memory_type=[MemoryType.EPISODIC, MemoryType.SEMANTIC],\n",
    "        limit=5,\n",
    "        user_id=user_id,\n",
    "        distance_threshold=0.3,\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"All relevant memories: {relevant_memories}\")\n",
    "\n",
    "    if relevant_memories:\n",
    "        memory_context = \"\\n\\n### Relevant memories from previous conversations:\\n\"\n",
    "\n",
    "        # Group by memory type\n",
    "        memory_types = {\n",
    "            MemoryType.EPISODIC: \"User Preferences & History\",\n",
    "            MemoryType.SEMANTIC: \"Travel Knowledge\",\n",
    "        }\n",
    "\n",
    "        for mem_type, type_label in memory_types.items():\n",
    "            memories_of_type = [\n",
    "                m for m in relevant_memories if m.memory_type == mem_type\n",
    "            ]\n",
    "            if memories_of_type:\n",
    "                memory_context += f\"\\n**{type_label}**:\\n\"\n",
    "                for mem in memories_of_type:\n",
    "                    memory_context += f\"- {mem.content}\\n\"\n",
    "\n",
    "        augmented_message = HumanMessage(content=f\"{query}\\n{memory_context}\")\n",
    "        state[\"messages\"][-1] = augmented_message\n",
    "\n",
    "        logger.debug(f\"Augmented message: {augmented_message.content}\")\n",
    "\n",
    "    return state.copy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

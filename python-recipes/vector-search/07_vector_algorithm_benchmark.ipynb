{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "# Vector Algorithm Benchmark: FLAT vs HNSW vs SVS-VAMANA\n",
    "\n",
    "## Let's Begin!\n",
    "<a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/vector-search/07_vector_algorithm_benchmark_real_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This notebook benchmarks FLAT, HNSW, and SVS-VAMANA vector search algorithms using **real data from Hugging Face** across different embedding dimensions.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Memory usage comparison** across algorithms and dimensions\n",
    "- **Index creation performance** with real text data\n",
    "- **Query performance** and latency analysis\n",
    "- **Search quality** with recall metrics on real embeddings\n",
    "- **Algorithm selection guidance** based on your requirements\n",
    "\n",
    "## Benchmark Configuration\n",
    "\n",
    "- **Dataset**: SQuAD (Stanford Question Answering Dataset) from Hugging Face\n",
    "- **Algorithms**: FLAT, HNSW, SVS-VAMANA\n",
    "- **Dimensions**: 384, 768, 1536 (native sentence-transformer embeddings)\n",
    "- **Dataset Size**: 1,000 documents per dimension\n",
    "- **Query Set**: 50 real questions per configuration\n",
    "- **Focus**: Real-world performance with actual text embeddings\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Redis Stack 8.2.0+ with RediSearch 2.8.10+\n",
    "- At least 4GB RAM for comfortable benchmarking\n",
    "- Internet connection for downloading SQuAD dataset\n",
    "- ~30-45 minutes runtime for complete benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Installation & Setup\n",
    "\n",
    "**🐳 Docker Setup (Required):**\n",
    "\n",
    "Before running this notebook, make sure Redis Stack is running:\n",
    "\n",
    "```bash\n",
    "# Start Redis Stack with Docker\n",
    "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package.split('[')[0])  # Handle package[extras] format\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install required packages\n",
    "install_if_missing(\"redisvl\")\n",
    "install_if_missing(\"matplotlib\")\n",
    "install_if_missing(\"seaborn\")\n",
    "install_if_missing(\"pandas\")\n",
    "install_if_missing(\"datasets\")\n",
    "install_if_missing(\"sentence-transformers\")\n",
    "\n",
    "print(\"✅ All dependencies are ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "# Redis and RedisVL imports\n",
    "import redis\n",
    "from redisvl.index import SearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.redis.utils import array_to_buffer, buffer_to_array\n",
    "from redisvl.utils import CompressionAdvisor\n",
    "from redisvl.redis.connection import supports_svs\n",
    "\n",
    "# Configuration\n",
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📚 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark configuration\n",
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    dimensions: List[int]\n",
    "    algorithms: List[str]\n",
    "    docs_per_dimension: int\n",
    "    query_count: int\n",
    "    \n",
    "# Initialize benchmark configuration\n",
    "config = BenchmarkConfig(\n",
    "    dimensions=[384, 768, 1536],\n",
    "    algorithms=['flat', 'hnsw', 'svs-vamana'],\n",
    "    docs_per_dimension=1000,\n",
    "    query_count=50\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"🔧 Benchmark Configuration:\",\n",
    "    f\"Dimensions: {config.dimensions}\",\n",
    "    f\"Algorithms: {config.algorithms}\",\n",
    "    f\"Documents per dimension: {config.docs_per_dimension:,}\",\n",
    "    f\"Test queries: {config.query_count}\",\n",
    "    f\"Total documents: {len(config.dimensions) * config.docs_per_dimension:,}\",\n",
    "    f\"Dataset: SQuAD from Hugging Face\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Redis and SVS Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Redis connection and capabilities\n",
    "try:\n",
    "    client = redis.Redis.from_url(REDIS_URL)\n",
    "    client.ping()\n",
    "    \n",
    "    redis_info = client.info()\n",
    "    redis_version = redis_info['redis_version']\n",
    "    \n",
    "    svs_supported = supports_svs(client)\n",
    "    \n",
    "    print(\n",
    "        \"✅ Redis connection successful\",\n",
    "        f\"📊 Redis version: {redis_version}\",\n",
    "        f\"🔧 SVS-VAMANA supported: {'✅ Yes' if svs_supported else '❌ No'}\",\n",
    "        sep=\"\\n\"\n",
    "    )\n",
    "    \n",
    "    if not svs_supported:\n",
    "        print(\"⚠️  SVS-VAMANA not supported. Benchmark will skip SVS tests.\")\n",
    "        config.algorithms = ['flat', 'hnsw']  # Remove SVS from tests\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Redis connection failed: {e}\")\n",
    "    print(\"Please ensure Redis Stack is running on localhost:6379\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Real Dataset from Hugging Face\n",
    "\n",
    "Load the SQuAD dataset and generate real embeddings using sentence-transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_dataset(num_docs: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load SQuAD dataset from Hugging Face\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        print(\"📥 Loading SQuAD dataset from Hugging Face...\")\n",
    "        \n",
    "        # Load SQuAD dataset\n",
    "        dataset = load_dataset(\"squad\", split=\"train\")\n",
    "        \n",
    "        # Take a subset for our benchmark\n",
    "        dataset = dataset.select(range(min(num_docs, len(dataset))))\n",
    "        \n",
    "        # Convert to our format\n",
    "        documents = []\n",
    "        for i, item in enumerate(dataset):\n",
    "            # Combine question and context for richer text\n",
    "            text = f\"{item['question']} {item['context']}\"\n",
    "            \n",
    "            documents.append({\n",
    "                'doc_id': f'squad_{i:06d}',\n",
    "                'title': item['title'],\n",
    "                'question': item['question'],\n",
    "                'context': item['context'][:500],  # Truncate long contexts\n",
    "                'text': text,\n",
    "                'category': 'qa',  # All are Q&A documents\n",
    "                'score': 1.0\n",
    "            })\n",
    "        \n",
    "        print(f\"✅ Loaded {len(documents)} documents from SQuAD\")\n",
    "        return documents\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️  datasets library not available, falling back to local data\")\n",
    "        return load_local_fallback_data(num_docs)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to load SQuAD dataset: {e}\")\n",
    "        print(\"Falling back to local data...\")\n",
    "        return load_local_fallback_data(num_docs)\n",
    "\n",
    "def load_local_fallback_data(num_docs: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fallback to local movie dataset if SQuAD is not available\"\"\"\n",
    "    try:\n",
    "        import json\n",
    "        with open('resources/movies.json', 'r') as f:\n",
    "            movies = json.load(f)\n",
    "        \n",
    "        # Expand the small movie dataset by duplicating with variations\n",
    "        documents = []\n",
    "        for i in range(num_docs):\n",
    "            movie = movies[i % len(movies)]\n",
    "            documents.append({\n",
    "                'doc_id': f'movie_{i:06d}',\n",
    "                'title': f\"{movie['title']} (Variant {i // len(movies) + 1})\",\n",
    "                'question': f\"What is {movie['title']} about?\",\n",
    "                'context': movie['description'],\n",
    "                'text': f\"What is {movie['title']} about? {movie['description']}\",\n",
    "                'category': movie['genre'],\n",
    "                'score': movie['rating']\n",
    "            })\n",
    "        \n",
    "        print(f\"✅ Using local movie dataset: {len(documents)} documents\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load local data: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_for_texts(texts: List[str], dimensions: int) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for texts using sentence-transformers\"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "        # Choose model based on target dimensions\n",
    "        if dimensions == 384:\n",
    "            model_name = 'all-MiniLM-L6-v2'\n",
    "        elif dimensions == 768:\n",
    "            model_name = 'all-mpnet-base-v2'\n",
    "        elif dimensions == 1536:\n",
    "            # For 1536D, use gtr-t5-xl which produces native 1536D embeddings\n",
    "            model_name = 'sentence-transformers/gtr-t5-xl'\n",
    "        else:\n",
    "            model_name = 'all-MiniLM-L6-v2'  # Default\n",
    "        \n",
    "        print(f\"🤖 Generating {dimensions}D embeddings using {model_name}...\")\n",
    "        \n",
    "        model = SentenceTransformer(model_name)\n",
    "        embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "        \n",
    "        # Handle dimension adjustment\n",
    "        current_dims = embeddings.shape[1]\n",
    "        if current_dims < dimensions:\n",
    "            # Pad with small random values (better than zeros)\n",
    "            padding_size = dimensions - current_dims\n",
    "            padding = np.random.normal(0, 0.01, (embeddings.shape[0], padding_size))\n",
    "            embeddings = np.concatenate([embeddings, padding], axis=1)\n",
    "        elif current_dims > dimensions:\n",
    "            # Truncate\n",
    "            embeddings = embeddings[:, :dimensions]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings = embeddings / norms\n",
    "        \n",
    "        print(f\"✅ Generated embeddings: {embeddings.shape}\")\n",
    "        return embeddings.astype(np.float32)\n",
    "        \n",
    "    except ImportError:\n",
    "        print(f\"⚠️  sentence-transformers not available, using synthetic embeddings\")\n",
    "        return generate_synthetic_embeddings(len(texts), dimensions)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error generating embeddings: {e}\")\n",
    "        print(\"Falling back to synthetic embeddings...\")\n",
    "        return generate_synthetic_embeddings(len(texts), dimensions)\n",
    "\n",
    "def generate_synthetic_embeddings(num_docs: int, dimensions: int) -> np.ndarray:\n",
    "    \"\"\"Generate synthetic embeddings as fallback\"\"\"\n",
    "    print(f\"🔄 Generating {num_docs} synthetic {dimensions}D embeddings...\")\n",
    "    \n",
    "    # Create base random vectors\n",
    "    embeddings = np.random.normal(0, 1, (num_docs, dimensions)).astype(np.float32)\n",
    "    \n",
    "    # Add some clustering structure\n",
    "    cluster_size = num_docs // 3\n",
    "    embeddings[:cluster_size, :min(50, dimensions)] += 0.5\n",
    "    embeddings[cluster_size:2*cluster_size, min(50, dimensions):min(100, dimensions)] += 0.5\n",
    "    \n",
    "    # Normalize vectors\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    embeddings = embeddings / norms\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Load real dataset and generate embeddings\n",
    "print(\"🔄 Loading real dataset and generating embeddings...\")\n",
    "\n",
    "# Load the base dataset once\n",
    "raw_documents = load_squad_dataset(config.docs_per_dimension)\n",
    "texts = [doc['text'] for doc in raw_documents]\n",
    "\n",
    "# Generate separate query texts (use questions from SQuAD)\n",
    "query_texts = [doc['question'] for doc in raw_documents[:config.query_count]]\n",
    "\n",
    "benchmark_data = {}\n",
    "query_data = {}\n",
    "\n",
    "for dim in config.dimensions:\n",
    "    print(f\"\\n📊 Processing {dim}D embeddings...\")\n",
    "    \n",
    "    # Generate embeddings for documents\n",
    "    embeddings = generate_embeddings_for_texts(texts, dim)\n",
    "    \n",
    "    # Generate embeddings for queries\n",
    "    query_embeddings = generate_embeddings_for_texts(query_texts, dim)\n",
    "    \n",
    "    # Combine documents with embeddings\n",
    "    documents = []\n",
    "    for i, (doc, embedding) in enumerate(zip(raw_documents, embeddings)):\n",
    "        documents.append({\n",
    "            **doc,\n",
    "            'embedding': array_to_buffer(embedding, dtype='float32')\n",
    "        })\n",
    "    \n",
    "    benchmark_data[dim] = documents\n",
    "    query_data[dim] = query_embeddings\n",
    "\n",
    "print(\n",
    "    f\"\\n✅ Generated benchmark data:\",\n",
    "    f\"Total documents: {sum(len(docs) for docs in benchmark_data.values()):,}\",\n",
    "    f\"Total queries: {sum(len(queries) for queries in query_data.values()):,}\",\n",
    "    f\"Dataset source: {'SQuAD (Hugging Face)' if 'squad_' in raw_documents[0]['doc_id'] else 'Local movies'}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Index Creation Benchmark\n",
    "\n",
    "Measure index creation time and memory usage for each algorithm and dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_schema(algorithm: str, dimensions: int, prefix: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create index schema for the specified algorithm\"\"\"\n",
    "    \n",
    "    base_schema = {\n",
    "        \"index\": {\n",
    "            \"name\": f\"benchmark_{algorithm}_{dimensions}d\",\n",
    "            \"prefix\": prefix,\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"doc_id\", \"type\": \"tag\"},\n",
    "            {\"name\": \"title\", \"type\": \"text\"},\n",
    "            {\"name\": \"category\", \"type\": \"tag\"},\n",
    "            {\"name\": \"score\", \"type\": \"numeric\"},\n",
    "            {\n",
    "                \"name\": \"embedding\",\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"dims\": dimensions,\n",
    "                    \"distance_metric\": \"cosine\",\n",
    "                    \"datatype\": \"float32\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Algorithm-specific configurations\n",
    "    vector_field = base_schema[\"fields\"][-1][\"attrs\"]\n",
    "    \n",
    "    if algorithm == 'flat':\n",
    "        vector_field[\"algorithm\"] = \"flat\"\n",
    "        \n",
    "    elif algorithm == 'hnsw':\n",
    "        vector_field.update({\n",
    "            \"algorithm\": \"hnsw\",\n",
    "            \"m\": 16,\n",
    "            \"ef_construction\": 200,\n",
    "            \"ef_runtime\": 10\n",
    "        })\n",
    "        \n",
    "    elif algorithm == 'svs-vamana':\n",
    "        # Get compression recommendation\n",
    "        compression_config = CompressionAdvisor.recommend(dims=dimensions, priority=\"memory\")\n",
    "        \n",
    "        vector_field.update({\n",
    "            \"algorithm\": \"svs-vamana\",\n",
    "            \"datatype\": compression_config.get('datatype', 'float32')\n",
    "        })\n",
    "        \n",
    "        # Handle dimensionality reduction for high dimensions\n",
    "        if 'reduce' in compression_config:\n",
    "            vector_field[\"dims\"] = compression_config['reduce']\n",
    "    \n",
    "    return base_schema\n",
    "\n",
    "def benchmark_index_creation(algorithm: str, dimensions: int, documents: List[Dict]) -> Tuple[SearchIndex, float, float]:\n",
    "    \"\"\"Benchmark index creation and return index, build time, and memory usage\"\"\"\n",
    "    \n",
    "    prefix = f\"bench:{algorithm}:{dimensions}d:\"\n",
    "    \n",
    "    # Clean up any existing index\n",
    "    try:\n",
    "        client.execute_command('FT.DROPINDEX', f'benchmark_{algorithm}_{dimensions}d')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create schema and index\n",
    "    schema = create_index_schema(algorithm, dimensions, prefix)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create index\n",
    "    index = SearchIndex.from_dict(schema, redis_url=REDIS_URL)\n",
    "    index.create(overwrite=True)\n",
    "    \n",
    "    # Load data in batches\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        index.load(batch)\n",
    "    \n",
    "    # Wait for indexing to complete\n",
    "    if algorithm == 'hnsw':\n",
    "        time.sleep(3)  # HNSW needs more time for graph construction\n",
    "    else:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    build_time = time.time() - start_time\n",
    "    \n",
    "    # Get index info for memory usage\n",
    "    try:\n",
    "        index_info = index.info()\n",
    "        index_size_mb = float(index_info.get('vector_index_sz_mb', 0))\n",
    "    except:\n",
    "        index_size_mb = 0.0\n",
    "    \n",
    "    return index, build_time, index_size_mb\n",
    "\n",
    "# Run index creation benchmarks\n",
    "print(\"🏗️ Running index creation benchmarks...\")\n",
    "\n",
    "creation_results = {}\n",
    "indices = {}\n",
    "\n",
    "for dim in config.dimensions:\n",
    "    print(f\"\\n📊 Benchmarking {dim}D embeddings:\")\n",
    "    \n",
    "    for algorithm in config.algorithms:\n",
    "        print(f\"  Creating {algorithm.upper()} index...\")\n",
    "        \n",
    "        try:\n",
    "            index, build_time, index_size_mb = benchmark_index_creation(\n",
    "                algorithm, dim, benchmark_data[dim]\n",
    "            )\n",
    "            \n",
    "            creation_results[f\"{algorithm}_{dim}\"] = {\n",
    "                'algorithm': algorithm,\n",
    "                'dimensions': dim,\n",
    "                'build_time_sec': build_time,\n",
    "                'index_size_mb': index_size_mb,\n",
    "                'num_docs': len(benchmark_data[dim])\n",
    "            }\n",
    "            \n",
    "            indices[f\"{algorithm}_{dim}\"] = index\n",
    "            \n",
    "            print(\n",
    "                f\"    ✅ {algorithm.upper()}: {build_time:.2f}s, {index_size_mb:.2f}MB\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ {algorithm.upper()} failed: {e}\")\n",
    "            creation_results[f\"{algorithm}_{dim}\"] = None\n",
    "\n",
    "print(\"\\n✅ Index creation benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Query Performance Benchmark\n",
    "\n",
    "Measure query latency and search quality for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(retrieved_ids: List[str], ground_truth_ids: List[str], k: int) -> float:\n",
    "    \"\"\"Calculate recall@k between retrieved and ground truth results\"\"\"\n",
    "    if not ground_truth_ids or not retrieved_ids:\n",
    "        return 0.0\n",
    "    \n",
    "    retrieved_set = set(retrieved_ids[:k])\n",
    "    ground_truth_set = set(ground_truth_ids[:k])\n",
    "    \n",
    "    if len(ground_truth_set) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = len(retrieved_set.intersection(ground_truth_set))\n",
    "    return intersection / len(ground_truth_set)\n",
    "\n",
    "def benchmark_query_performance(index: SearchIndex, query_vectors: np.ndarray, \n",
    "                               algorithm: str, dimensions: int) -> Dict[str, float]:\n",
    "    \"\"\"Benchmark query performance and quality\"\"\"\n",
    "    \n",
    "    latencies = []\n",
    "    all_results = []\n",
    "    \n",
    "    # Get ground truth from FLAT index (if available)\n",
    "    ground_truth_results = []\n",
    "    flat_index_key = f\"flat_{dimensions}\"\n",
    "    \n",
    "    if flat_index_key in indices and algorithm != 'flat':\n",
    "        flat_index = indices[flat_index_key]\n",
    "        for query_vec in query_vectors:\n",
    "            query = VectorQuery(\n",
    "                vector=query_vec,\n",
    "                vector_field_name=\"embedding\",\n",
    "                return_fields=[\"doc_id\"],\n",
    "                dtype=\"float32\",\n",
    "                num_results=10\n",
    "            )\n",
    "            results = flat_index.query(query)\n",
    "            ground_truth_results.append([doc[\"doc_id\"] for doc in results])\n",
    "    \n",
    "    # Benchmark the target algorithm\n",
    "    for i, query_vec in enumerate(query_vectors):\n",
    "        # Adjust query vector for SVS if needed\n",
    "        if algorithm == 'svs-vamana':\n",
    "            compression_config = CompressionAdvisor.recommend(dims=dimensions, priority=\"memory\")\n",
    "            \n",
    "            if 'reduce' in compression_config:\n",
    "                target_dims = compression_config['reduce']\n",
    "                if target_dims < dimensions:\n",
    "                    query_vec = query_vec[:target_dims]\n",
    "            \n",
    "            if compression_config.get('datatype') == 'float16':\n",
    "                query_vec = query_vec.astype(np.float16)\n",
    "                dtype = 'float16'\n",
    "            else:\n",
    "                dtype = 'float32'\n",
    "        else:\n",
    "            dtype = 'float32'\n",
    "        \n",
    "        # Execute query with timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        query = VectorQuery(\n",
    "            vector=query_vec,\n",
    "            vector_field_name=\"embedding\",\n",
    "            return_fields=[\"doc_id\", \"title\", \"category\"],\n",
    "            dtype=dtype,\n",
    "            num_results=10\n",
    "        )\n",
    "        \n",
    "        results = index.query(query)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        latencies.append(latency * 1000)  # Convert to milliseconds\n",
    "        all_results.append([doc[\"doc_id\"] for doc in results])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_latency = np.mean(latencies)\n",
    "    \n",
    "    # Calculate recall if we have ground truth\n",
    "    if ground_truth_results and algorithm != 'flat':\n",
    "        recall_5_scores = []\n",
    "        recall_10_scores = []\n",
    "        \n",
    "        for retrieved, ground_truth in zip(all_results, ground_truth_results):\n",
    "            recall_5_scores.append(calculate_recall(retrieved, ground_truth, 5))\n",
    "            recall_10_scores.append(calculate_recall(retrieved, ground_truth, 10))\n",
    "        \n",
    "        recall_at_5 = np.mean(recall_5_scores)\n",
    "        recall_at_10 = np.mean(recall_10_scores)\n",
    "    else:\n",
    "        # FLAT is our ground truth, so perfect recall\n",
    "        recall_at_5 = 1.0 if algorithm == 'flat' else 0.0\n",
    "        recall_at_10 = 1.0 if algorithm == 'flat' else 0.0\n",
    "    \n",
    "    return {\n",
    "        'avg_query_time_ms': avg_latency,\n",
    "        'recall_at_5': recall_at_5,\n",
    "        'recall_at_10': recall_at_10,\n",
    "        'num_queries': len(query_vectors)\n",
    "    }\n",
    "\n",
    "# Run query performance benchmarks\n",
    "print(\"🔍 Running query performance benchmarks...\")\n",
    "\n",
    "query_results = {}\n",
    "\n",
    "for dim in config.dimensions:\n",
    "    print(f\"\\n📊 Benchmarking {dim}D queries:\")\n",
    "    \n",
    "    for algorithm in config.algorithms:\n",
    "        index_key = f\"{algorithm}_{dim}\"\n",
    "        \n",
    "        if index_key in indices:\n",
    "            print(f\"  Testing {algorithm.upper()} queries...\")\n",
    "            \n",
    "            try:\n",
    "                performance = benchmark_query_performance(\n",
    "                    indices[index_key], \n",
    "                    query_data[dim], \n",
    "                    algorithm, \n",
    "                    dim\n",
    "                )\n",
    "                \n",
    "                query_results[index_key] = performance\n",
    "                \n",
    "                print(\n",
    "                    f\"    ✅ {algorithm.upper()}: {performance['avg_query_time_ms']:.2f}ms avg, \"\n",
    "                    f\"R@5: {performance['recall_at_5']:.3f}, R@10: {performance['recall_at_10']:.3f}\"\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ {algorithm.upper()} query failed: {e}\")\n",
    "                query_results[index_key] = None\n",
    "        else:\n",
    "            print(f\"  ⏭️  Skipping {algorithm.upper()} (index creation failed)\")\n",
    "\n",
    "print(\"\\n✅ Query performance benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Results Analysis and Visualization\n",
    "\n",
    "Analyze and visualize the benchmark results with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results into comprehensive dataset\n",
    "def create_results_dataframe() -> pd.DataFrame:\n",
    "    \"\"\"Combine all benchmark results into a pandas DataFrame\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for dim in config.dimensions:\n",
    "        for algorithm in config.algorithms:\n",
    "            key = f\"{algorithm}_{dim}\"\n",
    "            \n",
    "            if key in creation_results and creation_results[key] is not None:\n",
    "                creation_data = creation_results[key]\n",
    "                query_data_item = query_results.get(key, {})\n",
    "                \n",
    "                result = {\n",
    "                    'algorithm': algorithm,\n",
    "                    'dimensions': dim,\n",
    "                    'num_docs': creation_data['num_docs'],\n",
    "                    'build_time_sec': creation_data['build_time_sec'],\n",
    "                    'index_size_mb': creation_data['index_size_mb'],\n",
    "                    'avg_query_time_ms': query_data_item.get('avg_query_time_ms', 0),\n",
    "                    'recall_at_5': query_data_item.get('recall_at_5', 0),\n",
    "                    'recall_at_10': query_data_item.get('recall_at_10', 0)\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = create_results_dataframe()\n",
    "\n",
    "print(\"📊 Real Data Benchmark Results Summary:\")\n",
    "print(df_results.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Display key insights\n",
    "if not df_results.empty:\n",
    "    print(f\"\\n🎯 Key Insights from Real Data:\")\n",
    "    \n",
    "    # Memory efficiency\n",
    "    best_memory = df_results.loc[df_results['index_size_mb'].idxmin()]\n",
    "    print(f\"🏆 Most memory efficient: {best_memory['algorithm'].upper()} at {best_memory['dimensions']}D ({best_memory['index_size_mb']:.2f}MB)\")\n",
    "    \n",
    "    # Query speed\n",
    "    best_speed = df_results.loc[df_results['avg_query_time_ms'].idxmin()]\n",
    "    print(f\"⚡ Fastest queries: {best_speed['algorithm'].upper()} at {best_speed['dimensions']}D ({best_speed['avg_query_time_ms']:.2f}ms)\")\n",
    "    \n",
    "    # Search quality\n",
    "    best_quality = df_results.loc[df_results['recall_at_10'].idxmax()]\n",
    "    print(f\"🎯 Best search quality: {best_quality['algorithm'].upper()} at {best_quality['dimensions']}D (R@10: {best_quality['recall_at_10']:.3f})\")\n",
    "    \n",
    "    # Dataset info\n",
    "    dataset_source = 'SQuAD (Hugging Face)' if 'squad_' in raw_documents[0]['doc_id'] else 'Local movies'\n",
    "    print(f\"\\n📚 Dataset: {dataset_source}\")\n",
    "    print(f\"📊 Total documents tested: {df_results['num_docs'].iloc[0]:,}\")\n",
    "    print(f\"🔍 Total queries per dimension: {config.query_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for real data results\n",
    "def create_real_data_visualizations(df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations for real data benchmark results\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"⚠️  No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting area\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Real Data Vector Algorithm Benchmark Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Memory Usage Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    pivot_memory = df.pivot(index='dimensions', columns='algorithm', values='index_size_mb')\n",
    "    pivot_memory.plot(kind='bar', ax=ax1, width=0.8)\n",
    "    ax1.set_title('Index Size by Algorithm (Real Data)')\n",
    "    ax1.set_xlabel('Dimensions')\n",
    "    ax1.set_ylabel('Index Size (MB)')\n",
    "    ax1.legend(title='Algorithm')\n",
    "    ax1.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # 2. Query Performance\n",
    "    ax2 = axes[0, 1]\n",
    "    pivot_query = df.pivot(index='dimensions', columns='algorithm', values='avg_query_time_ms')\n",
    "    pivot_query.plot(kind='bar', ax=ax2, width=0.8)\n",
    "    ax2.set_title('Average Query Time (Real Embeddings)')\n",
    "    ax2.set_xlabel('Dimensions')\n",
    "    ax2.set_ylabel('Query Time (ms)')\n",
    "    ax2.legend(title='Algorithm')\n",
    "    ax2.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # 3. Search Quality\n",
    "    ax3 = axes[1, 0]\n",
    "    pivot_recall = df.pivot(index='dimensions', columns='algorithm', values='recall_at_10')\n",
    "    pivot_recall.plot(kind='bar', ax=ax3, width=0.8)\n",
    "    ax3.set_title('Search Quality (Recall@10)')\n",
    "    ax3.set_xlabel('Dimensions')\n",
    "    ax3.set_ylabel('Recall@10')\n",
    "    ax3.legend(title='Algorithm')\n",
    "    ax3.tick_params(axis='x', rotation=0)\n",
    "    ax3.set_ylim(0, 1.1)\n",
    "    \n",
    "    # 4. Memory Efficiency\n",
    "    ax4 = axes[1, 1]\n",
    "    df['docs_per_mb'] = df['num_docs'] / df['index_size_mb']\n",
    "    pivot_efficiency = df.pivot(index='dimensions', columns='algorithm', values='docs_per_mb')\n",
    "    pivot_efficiency.plot(kind='bar', ax=ax4, width=0.8)\n",
    "    ax4.set_title('Memory Efficiency (Real Data)')\n",
    "    ax4.set_xlabel('Dimensions')\n",
    "    ax4.set_ylabel('Documents per MB')\n",
    "    ax4.legend(title='Algorithm')\n",
    "    ax4.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "create_real_data_visualizations(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Real Data Insights and Recommendations\n",
    "\n",
    "Generate insights based on real data performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate real data specific recommendations\n",
    "if not df_results.empty:\n",
    "    dataset_source = 'SQuAD (Hugging Face)' if 'squad_' in raw_documents[0]['doc_id'] else 'Local movies'\n",
    "    \n",
    "    print(\n",
    "        f\"🎯 Real Data Benchmark Insights\",\n",
    "        f\"Dataset: {dataset_source}\",\n",
    "        f\"Documents: {df_results['num_docs'].iloc[0]:,} per dimension\",\n",
    "        f\"Embedding Models: sentence-transformers\",\n",
    "        \"=\" * 50,\n",
    "        sep=\"\\n\"\n",
    "    )\n",
    "    \n",
    "    for dim in config.dimensions:\n",
    "        dim_data = df_results[df_results['dimensions'] == dim]\n",
    "        \n",
    "        if not dim_data.empty:\n",
    "            print(f\"\\n📊 {dim}D Embeddings Analysis:\")\n",
    "            \n",
    "            for _, row in dim_data.iterrows():\n",
    "                algo = row['algorithm'].upper()\n",
    "                print(\n",
    "                    f\"  {algo}:\",\n",
    "                    f\"    Index: {row['index_size_mb']:.2f}MB\",\n",
    "                    f\"    Query: {row['avg_query_time_ms']:.2f}ms\",\n",
    "                    f\"    Recall@10: {row['recall_at_10']:.3f}\",\n",
    "                    f\"    Efficiency: {row['docs_per_mb']:.1f} docs/MB\",\n",
    "                    sep=\"\\n\"\n",
    "                )\n",
    "    \n",
    "    print(\n",
    "        f\"\\n💡 Key Takeaways with Real Data:\",\n",
    "        \"• Real embeddings show different performance characteristics than synthetic\",\n",
    "        \"• Sentence-transformer models provide realistic vector distributions\",\n",
    "        \"• SQuAD Q&A pairs offer diverse semantic content for testing\",\n",
    "        \"• Results are more representative of production workloads\",\n",
    "        \"• Consider testing with your specific embedding models and data\",\n",
    "        sep=\"\\n\"\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️  No results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cleanup\n",
    "\n",
    "Clean up benchmark indices to free memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up all benchmark indices\n",
    "print(\"🧹 Cleaning up benchmark indices...\")\n",
    "\n",
    "cleanup_count = 0\n",
    "for index_key, index in indices.items():\n",
    "    try:\n",
    "        index.delete(drop=True)\n",
    "        cleanup_count += 1\n",
    "        print(f\"  ✅ Deleted {index_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  Failed to delete {index_key}: {e}\")\n",
    "\n",
    "dataset_source = 'SQuAD (Hugging Face)' if 'squad_' in raw_documents[0]['doc_id'] else 'Local movies'\n",
    "\n",
    "print(\n",
    "    f\"\\n🎉 Real Data Benchmark Complete!\",\n",
    "    f\"Dataset: {dataset_source}\",\n",
    "    f\"Cleaned up {cleanup_count} indices\",\n",
    "    f\"\\nNext steps:\",\n",
    "    \"1. Review the real data performance characteristics above\",\n",
    "    \"2. Compare with synthetic data results if available\",\n",
    "    \"3. Test with your specific embedding models and datasets\",\n",
    "    \"4. Scale up with larger datasets for production insights\",\n",
    "    \"5. Consider the impact of real text diversity on algorithm performance\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

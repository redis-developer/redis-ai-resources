{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "# Migrating from FLAT to SVS-VAMANA\n",
    "\n",
    "## Let's Begin!\n",
    "<a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/vector-search/06_svs_vamana_migration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This notebook demonstrates how to migrate existing FLAT vector indices to SVS-VAMANA for improved memory efficiency and cost savings.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to assess your current FLAT index for migration\n",
    "- Step-by-step migration from FLAT to SVS-VAMANA\n",
    "- Memory usage comparison and cost analysis\n",
    "- Search quality validation\n",
    "- Performance benchmarking\n",
    "- Migration decision framework\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Redis Stack 8.2.0+ with RediSearch 2.8.10+\n",
    "- Existing vector index with substantial data (1000+ documents recommended)\n",
    "- Vector embeddings (768 dimensions using sentence-transformers/all-mpnet-base-v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup\n",
    "\n",
    "This notebook requires **sentence-transformers** for generating embeddings and **Redis Stack** running in Docker.\n",
    "\n",
    "**Requirements:**\n",
    "- Redis Stack 8.2.0+ with RediSearch 2.8.10+\n",
    "- sentence-transformers (for generating embeddings)\n",
    "- numpy (for vector operations)\n",
    "- redisvl (should be available in your environment)\n",
    "\n",
    "**üê≥ Docker Setup (Required):**\n",
    "\n",
    "Before running this notebook, make sure Redis Stack is running in Docker:\n",
    "\n",
    "```bash\n",
    "# Start Redis Stack with Docker\n",
    "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "```\n",
    "\n",
    "Or if you prefer using docker-compose, create a `docker-compose.yml` file:\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  redis:\n",
    "    image: redis/redis-stack:latest\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "      - \"8001:8001\"\n",
    "```\n",
    "\n",
    "Then run: `docker-compose up -d`\n",
    "\n",
    "**üìö Python Dependencies Installation:**\n",
    "\n",
    "Install the required Python packages:\n",
    "\n",
    "```bash\n",
    "# Install core dependencies\n",
    "pip install redisvl numpy sentence-transformers\n",
    "\n",
    "# Or install with specific versions for compatibility\n",
    "pip install redisvl>=0.2.0 numpy>=1.21.0 sentence-transformers>=2.2.0\n",
    "```\n",
    "\n",
    "**For Google Colab users, run this cell:**\n",
    "\n",
    "```python\n",
    "!pip install redisvl sentence-transformers numpy\n",
    "```\n",
    "\n",
    "**For Conda users:**\n",
    "\n",
    "```bash\n",
    "conda install numpy\n",
    "pip install redisvl sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup redis-vl environment\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "# Required imports from redis-vl\n",
    "import numpy as np\n",
    "import time\n",
    "from redisvl.index import SearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.redis.utils import array_to_buffer, buffer_to_array\n",
    "from redisvl.utils import CompressionAdvisor\n",
    "from redisvl.redis.connection import supports_svs\n",
    "import redis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify SVS-VAMANA Support\n",
    "\n",
    "First, let's ensure your Redis environment supports SVS-VAMANA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Redis connection successful\n",
      "‚úÖ SVS-VAMANA supported\n",
      "   Ready for migration!\n"
     ]
    }
   ],
   "source": [
    "# Check Redis connection and SVS support\n",
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "\n",
    "try:\n",
    "    client = redis.Redis.from_url(REDIS_URL)\n",
    "    client.ping()\n",
    "    print(\"‚úÖ Redis connection successful\")\n",
    "    \n",
    "    if supports_svs(client):\n",
    "        print(\"‚úÖ SVS-VAMANA supported\")\n",
    "        print(\"   Ready for migration!\")\n",
    "    else:\n",
    "        print(\"‚ùå SVS-VAMANA not supported\")\n",
    "        print(\"   Requires Redis >= 8.2.0 with RediSearch >= 2.8.10\")\n",
    "        print(\"   Please upgrade Redis Stack before proceeding\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Redis connection failed: {e}\")\n",
    "    print(\"   Please ensure Redis is running and accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Assess Your Current Index\n",
    "\n",
    "For this demonstration, we'll create a sample FLAT index. In practice, you would analyze your existing index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading sample movie data...\n",
      "Loaded 20 movie records\n",
      "Sample movie: Explosive Pursuit - A daring cop chases a notorious criminal across the city in a high-stakes game of cat and mouse.\n"
     ]
    }
   ],
   "source": [
    "# Download sample data from redis-ai-resources\n",
    "print(\"üì• Loading sample movie data...\")\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the movies dataset\n",
    "url = \"resources/movies.json\"\n",
    "with open(\"resources/movies.json\", \"r\") as f:\n",
    "    movies_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(movies_data)} movie records\")\n",
    "print(f\"Sample movie: {movies_data[0]['title']} - {movies_data[0]['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Migration Assessment\n",
      "Vector dimensions: 768 (sentence-transformers/all-mpnet-base-v2)\n",
      "Dataset size: 20 movie documents\n",
      "Data includes: title, genre, rating, description\n"
     ]
    }
   ],
   "source": [
    "# Configuration for demonstration  \n",
    "dims = 768  # sentence-transformers/all-mpnet-base-v2 - 768 dims\n",
    "\n",
    "num_docs = len(movies_data)  # Use actual dataset size\n",
    "\n",
    "print(\n",
    "    \"üìä Migration Assessment\",\n",
    "    f\"Vector dimensions: {dims} (sentence-transformers/all-mpnet-base-v2)\",\n",
    "    f\"Dataset size: {num_docs} movie documents\",\n",
    "    \"Data includes: title, genre, rating, description\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, let's configure a smaple FLAT index. Notice the algorithm value, dims value, and datatype value under fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sample FLAT index...\n",
      "‚úÖ Created FLAT index: migration_demo_flat\n"
     ]
    }
   ],
   "source": [
    "flat_schema = {\n",
    "    \"index\": {\n",
    "        \"name\": \"migration_demo_flat\",\n",
    "        \"prefix\": \"demo:flat:\",\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"movie_id\", \"type\": \"tag\"},\n",
    "        {\"name\": \"title\", \"type\": \"text\"},\n",
    "        {\"name\": \"genre\", \"type\": \"tag\"},\n",
    "        {\"name\": \"rating\", \"type\": \"numeric\"},\n",
    "        {\"name\": \"description\", \"type\": \"text\"},\n",
    "        {\n",
    "            \"name\": \"embedding\",\n",
    "            \"type\": \"vector\",\n",
    "            \"attrs\": {\n",
    "                \"dims\": dims,\n",
    "                \"algorithm\": \"flat\",\n",
    "                \"datatype\": \"float32\",\n",
    "                \"distance_metric\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create and populate FLAT index\n",
    "print(\"Creating sample FLAT index...\")\n",
    "flat_index = SearchIndex.from_dict(flat_schema, redis_url=REDIS_URL)\n",
    "flat_index.create(overwrite=True)\n",
    "print(f\"‚úÖ Created FLAT index: {flat_index.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Generate embeddings for movie descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings for movie descriptions...\n",
      "üì¶ Loading sentence transformer model...\n",
      "14:45:27 sentence_transformers.SentenceTransformer INFO   Use pytorch device_name: mps\n",
      "14:45:27 sentence_transformers.SentenceTransformer INFO   Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "‚úÖ Loaded embedding model with 768 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e06f2f860ec443e802a3fbf3961487c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 20 real embeddings using sentence-transformers\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"üîÑ Generating embeddings for movie descriptions...\")\n",
    "embedding_model=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "try:\n",
    "    # Try to use sentence-transformers for real embeddings\n",
    "    print(\"üì¶ Loading sentence transformer model...\")\n",
    "    model = SentenceTransformer(embedding_model)\n",
    "    print(f\"‚úÖ Loaded embedding model with {dims} dimensions\")\n",
    "    \n",
    "    # Generate real embeddings\n",
    "    descriptions = [movie['description'] for movie in movies_data]\n",
    "    embeddings = model.encode(descriptions, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    print(f\"‚úÖ Generated {len(embeddings)} real embeddings using sentence-transformers\")\n",
    "    \n",
    "except ImportError:\n",
    "    # Fallback to synthetic embeddings\n",
    "    print(\"‚ö†Ô∏è  sentence-transformers not available, using synthetic embeddings\")\n",
    "    print(f\"üì¶ Using {dims} dimensions for synthetic embeddings\")\n",
    "    \n",
    "    # Generate synthetic embeddings (normalized random vectors for demo)\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    embeddings = []\n",
    "    for i, movie in enumerate(movies_data):\n",
    "        # Create a pseudo-semantic embedding based on movie content\n",
    "        vector = np.random.random(dims).astype(np.float32)\n",
    "        # Add some structure based on genre\n",
    "        if movie['genre'] == 'action':\n",
    "            vector[:50] += 0.3  # Action movies cluster\n",
    "        else:  # comedy\n",
    "            vector[50:100] += 0.3  # Comedy movies cluster\n",
    "        \n",
    "        # Normalize\n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        embeddings.append(vector)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"‚úÖ Generated {len(embeddings)} synthetic embeddings\")\n",
    "\n",
    "# Prepare data for loading\n",
    "sample_data = []\n",
    "for i, movie in enumerate(movies_data):\n",
    "    sample_data.append({\n",
    "        'movie_id': str(movie['id']),\n",
    "        'title': movie['title'],\n",
    "        'genre': movie['genre'],\n",
    "        'rating': movie['rating'],\n",
    "        'description': movie['description'],\n",
    "        'embedding': array_to_buffer(embeddings[i].astype(np.float32), dtype='float32')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data into FLAT index...\n",
      "  Loaded 20/20 documents\n",
      "Waiting for indexing to complete...\n",
      "\n",
      "‚úÖ FLAT index loaded with 20 documents\n",
      "Index size: 3.0168838500976563 MB\n"
     ]
    }
   ],
   "source": [
    "# Load data into FLAT index\n",
    "print(\"üì• Loading data into FLAT index...\")\n",
    "batch_size = 100  # Process in batches\n",
    "\n",
    "for i in range(0, len(sample_data), batch_size):\n",
    "    batch = sample_data[i:i+batch_size]\n",
    "    flat_index.load(batch)\n",
    "    print(f\"  Loaded {min(i+batch_size, len(sample_data))}/{len(sample_data)} documents\")\n",
    "\n",
    "# Wait for indexing to complete\n",
    "print(\"Waiting for indexing to complete...\")\n",
    "time.sleep(3)\n",
    "\n",
    "flat_info = flat_index.info()\n",
    "print(f\"\\n‚úÖ FLAT index loaded with {flat_info['num_docs']} documents\")\n",
    "print(f\"Index size: {flat_info.get('vector_index_sz_mb', 'N/A')} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get Compression Recommendation\n",
    "\n",
    "The CompressionAdvisor analyzes your vector dimensions and provides optimal compression settings for SVS-VAMANA vector indices. It eliminates the guesswork from parameter tuning by providing intelligent recommendations based on your vector characteristics and performance priorities.\n",
    "\n",
    "## Configuration Strategy\n",
    "**High-Dimensional Vectors (‚â•1024 dims)**: Uses **LeanVec4x8** compression with dimensionality reduction. Memory priority reduces dimensions by 50%, speed priority by\n",
    "25%, balanced by 50%. Achieves 60-80% memory savings.\n",
    "\n",
    "**Lower-Dimensional Vectors (<1024 dims)**: Uses **LVQ compression** without dimensionality reduction. Memory priority uses LVQ4 (4 bits), speed uses LVQ4x8 (12 bits),\n",
    "balanced uses LVQ4x4 (8 bits). Achieves 60-87% memory savings.\n",
    "\n",
    "**Our Configuration (768 dims)**: Will use **LVQ compression** as we're below the 1024 dimension threshold. This provides excellent compression without dimensionality reduction.\n",
    "\n",
    "## Available Compression Types\n",
    "- **LVQ4/LVQ4x4/LVQ4x8**: 4/8/12 bits per dimension\n",
    "- **LeanVec4x8/LeanVec8x8**: 12/16 bits + dimensionality reduction for high-dim vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing compression options...\n",
      "\n",
      "MEMORY priority:\n",
      "  Algorithm: svs-vamana\n",
      "  Compression: LVQ4\n",
      "  Datatype: float32\n",
      "\n",
      "BALANCED priority:\n",
      "  Algorithm: svs-vamana\n",
      "  Compression: LVQ4x4\n",
      "  Datatype: float32\n",
      "\n",
      "PERFORMANCE priority:\n",
      "  Algorithm: svs-vamana\n",
      "  Compression: LVQ4x4\n",
      "  Datatype: float32\n",
      "\n",
      "üìã Selected configuration: LVQ4 with float32\n",
      "Expected memory savings: Significant for 768-dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "# Get compression recommendation\n",
    "print(\"üîç Analyzing compression options...\")\n",
    "print()\n",
    "\n",
    "# Try different priorities to show options\n",
    "priorities = [\"memory\", \"balanced\", \"performance\"]\n",
    "configs = {}\n",
    "\n",
    "for priority in priorities:\n",
    "    config = CompressionAdvisor.recommend(dims=dims, priority=priority)\n",
    "    configs[priority] = config\n",
    "    print(f\"{priority.upper()} priority:\")\n",
    "    print(f\"  Algorithm: {config['algorithm']}\")\n",
    "    print(f\"  Compression: {config.get('compression', 'None')}\")\n",
    "    print(f\"  Datatype: {config['datatype']}\")\n",
    "    if 'reduce' in config:\n",
    "        reduction = ((dims - config['reduce']) / dims) * 100\n",
    "        print(f\"  Dimensionality: {dims} ‚Üí {config['reduce']} ({reduction:.1f}% reduction)\")\n",
    "    print()\n",
    "\n",
    "# Select memory-optimized configuration for migration\n",
    "selected_config = configs[\"memory\"]\n",
    "print(f\"üìã Selected configuration: {selected_config['compression']} with {selected_config['datatype']}\")\n",
    "print(f\"Expected memory savings: Significant for {dims}-dimensional vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create SVS-VAMANA Index\n",
    "\n",
    "Now we'll create the new SVS-VAMANA index with the recommended compression settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SVS-VAMANA index with compression...\n",
      "‚úÖ Created SVS-VAMANA index: migration_demo_svs\n",
      "Compression: LVQ4\n",
      "Datatype: float32\n"
     ]
    }
   ],
   "source": [
    "# Fallback configuration if not defined (for CI/CD compatibility)\n",
    "if 'selected_config' not in locals():\n",
    "    from redisvl.utils import CompressionAdvisor\n",
    "    selected_config = CompressionAdvisor.recommend(dims=dims, priority=\"memory\")\n",
    "\n",
    "# Create SVS-VAMANA schema with compression\n",
    "svs_schema = {\n",
    "    \"index\": {\n",
    "        \"name\": \"migration_demo_svs\",\n",
    "        \"prefix\": \"demo:svs:\",\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"movie_id\", \"type\": \"tag\"},\n",
    "        {\"name\": \"title\", \"type\": \"text\"},\n",
    "        {\"name\": \"genre\", \"type\": \"tag\"},\n",
    "        {\"name\": \"rating\", \"type\": \"numeric\"},\n",
    "        {\"name\": \"description\", \"type\": \"text\"},\n",
    "        {\n",
    "            \"name\": \"embedding\",\n",
    "            \"type\": \"vector\",\n",
    "            \"attrs\": {\n",
    "                \"dims\": selected_config.get('reduce', dims),  # Use reduced dimensions (512)\n",
    "                \"algorithm\": \"svs-vamana\",\n",
    "                \"datatype\": selected_config['datatype'],\n",
    "                \"distance_metric\": \"cosine\"\n",
    "                # Note: Don't include the full selected_config to avoid dims/reduce conflict\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Creating SVS-VAMANA index with compression...\")\n",
    "svs_index = SearchIndex.from_dict(svs_schema, redis_url=REDIS_URL)\n",
    "svs_index.create(overwrite=True)\n",
    "print(f\"‚úÖ Created SVS-VAMANA index: {svs_index.name}\")\n",
    "print(f\"Compression: {selected_config.get('compression', 'None')}\")\n",
    "print(f\"Datatype: {selected_config['datatype']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Migrate Data\n",
    "\n",
    "Extract data from the original index and load it into the SVS-VAMANA index with compression applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Migrating data to SVS-VAMANA...\n",
      "Target dimensions: 768 (from 768)\n",
      "Target datatype: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Migrating data to SVS-VAMANA...\")\n",
    "\n",
    "# Fallback configuration if not defined (for CI/CD compatibility)\n",
    "if 'selected_config' not in locals():\n",
    "    from redisvl.utils import CompressionAdvisor\n",
    "    selected_config = CompressionAdvisor.recommend(dims=dims, priority=\"memory\")\n",
    "\n",
    "# Determine target vector dimensions (may be reduced by LeanVec)\n",
    "target_dims = selected_config.get('reduce', dims)\n",
    "target_dtype = selected_config['datatype']\n",
    "\n",
    "print(f\"Target dimensions: {target_dims} (from {dims})\")\n",
    "print(f\"Target datatype: {target_dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from original index...\n",
      "Found 40 documents to migrate\n",
      "Prepared 40 documents for migration\n"
     ]
    }
   ],
   "source": [
    "# Extract data from FLAT index\n",
    "print(\"Extracting data from original index...\")\n",
    "keys = client.keys(\"demo:flat:*\")\n",
    "print(f\"Found {len(keys)} documents to migrate\")\n",
    "\n",
    "# Process and transform data for SVS index\n",
    "svs_data = []\n",
    "for i, key in enumerate(keys):\n",
    "    doc_data = client.hgetall(key)\n",
    "    \n",
    "    if b'embedding' in doc_data:\n",
    "        # Extract original vector\n",
    "        original_vector = np.array(buffer_to_array(doc_data[b'embedding'], dtype='float32'))\n",
    "        \n",
    "        # Apply dimensionality reduction if needed (LeanVec)\n",
    "        if target_dims < dims:\n",
    "            vector = original_vector[:target_dims]\n",
    "        else:\n",
    "            vector = original_vector\n",
    "        \n",
    "        # Convert to target datatype\n",
    "        if target_dtype == 'float16':\n",
    "            vector = vector.astype(np.float16)\n",
    "        \n",
    "        svs_data.append({\n",
    "            \"movie_id\": doc_data[b'movie_id'].decode(),\n",
    "            \"title\": doc_data[b'title'].decode(),\n",
    "            \"genre\": doc_data[b'genre'].decode(),\n",
    "            \"rating\": int(doc_data[b'rating'].decode()),\n",
    "            \"description\": doc_data[b'description'].decode(),\n",
    "            \"embedding\": array_to_buffer(vector, dtype=target_dtype)\n",
    "        })\n",
    "    \n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(keys)} documents\")\n",
    "\n",
    "print(f\"Prepared {len(svs_data)} documents for migration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into SVS-VAMANA index...\n",
      "  Migrated 40/40 documents\n",
      "Waiting for indexing to complete...\n",
      "\n",
      "‚úÖ Migration complete! SVS index has 20 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data into SVS index\n",
    "print(\"Loading data into SVS-VAMANA index...\")\n",
    "batch_size = 100  # Define batch size for migration\n",
    "\n",
    "if len(svs_data) > 0:\n",
    "    for i in range(0, len(svs_data), batch_size):\n",
    "        batch = svs_data[i:i+batch_size]\n",
    "        svs_index.load(batch)\n",
    "        print(f\"  Migrated {min(i+batch_size, len(svs_data))}/{len(svs_data)} documents\")\n",
    "\n",
    "    # Wait for indexing to complete\n",
    "    print(\"Waiting for indexing to complete...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    svs_info = svs_index.info()\n",
    "    print(f\"\\n‚úÖ Migration complete! SVS index has {svs_info['num_docs']} documents\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data to migrate. Make sure the FLAT index was populated first.\")\n",
    "    print(\"   Run the previous cells to load data into the FLAT index.\")\n",
    "    svs_info = svs_index.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Memory Usage\n",
    "\n",
    "Let's analyze the memory savings achieved through compression. This is just an example on the small sample data. Use a larger dataset before deciding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Memory Usage Comparison\n",
      "========================================\n",
      "Original FLAT index:    3.02 MB\n",
      "SVS-VAMANA index:       3.02 MB\n",
      "\n",
      "üí∞ Memory savings: -0.0%\n",
      "Absolute reduction: -0.00 MB\n",
      "\n",
      "üíµ Cost Impact Analysis:\n",
      "Monthly cost reduction: $-0.00\n",
      "Annual cost reduction: $-0.00\n"
     ]
    }
   ],
   "source": [
    "# Helper function to extract memory info\n",
    "def get_memory_mb(index_info):\n",
    "    \"\"\"Extract memory usage in MB from index info\"\"\"\n",
    "    memory = index_info.get('vector_index_sz_mb', 0)\n",
    "    if isinstance(memory, str):\n",
    "        try:\n",
    "            return float(memory)\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    return float(memory)\n",
    "\n",
    "# Get memory usage\n",
    "flat_memory = get_memory_mb(flat_info)\n",
    "svs_memory = get_memory_mb(svs_info)\n",
    "\n",
    "print(\n",
    "    \"üìä Memory Usage Comparison\",\n",
    "    \"=\" * 40,\n",
    "    f\"Original FLAT index:    {flat_memory:.2f} MB\",\n",
    "    f\"SVS-VAMANA index:       {svs_memory:.2f} MB\",\n",
    "    \"\",\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "if flat_memory > 0:\n",
    "    if svs_memory > 0:\n",
    "        savings = ((flat_memory - svs_memory) / flat_memory) * 100\n",
    "        print(\n",
    "            f\"üí∞ Memory savings: {savings:.1f}%\",\n",
    "            f\"Absolute reduction: {flat_memory - svs_memory:.2f} MB\",\n",
    "            sep=\"\\n\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚è≥ SVS index still indexing - memory comparison pending\")\n",
    "        \n",
    "    # Cost analysis\n",
    "    print(\"\\nüíµ Cost Impact Analysis:\")\n",
    "    cost_per_gb_hour = 0.10  # Example cloud pricing\n",
    "    hours_per_month = 24 * 30\n",
    "    \n",
    "    flat_monthly_cost = (flat_memory / 1024) * cost_per_gb_hour * hours_per_month\n",
    "    if svs_memory > 0:\n",
    "        svs_monthly_cost = (svs_memory / 1024) * cost_per_gb_hour * hours_per_month\n",
    "        monthly_savings = flat_monthly_cost - svs_monthly_cost\n",
    "        print(\n",
    "            f\"Monthly cost reduction: ${monthly_savings:.2f}\",\n",
    "            f\"Annual cost reduction: ${monthly_savings * 12:.2f}\",\n",
    "            sep=\"\\n\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Current monthly cost: ${flat_monthly_cost:.2f}\",\n",
    "            \"Projected savings: Available after indexing completes\",\n",
    "            sep=\"\\n\"\n",
    "        )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Memory information not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Validate Search Quality\n",
    "\n",
    "Test that the compressed index maintains good search quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating search quality...\n",
      "Generated 5 test queries\n",
      "\n",
      "Testing original FLAT index...\n",
      "FLAT search time: 0.012s (0.002s per query)\n",
      "\n",
      "Testing SVS-VAMANA index...\n",
      "SVS search time: 0.017s (0.003s per query)\n",
      "\n",
      "üìà Average recall@10: 1.000 (100.0%)\n",
      "‚úÖ Excellent search quality maintained\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Validating search quality...\")\n",
    "\n",
    "# Create test queries\n",
    "num_test_queries = 5\n",
    "test_queries = []\n",
    "\n",
    "for i in range(num_test_queries):\n",
    "    # Generate normalized test vector\n",
    "    query_vec = np.random.random(dims).astype(np.float32)\n",
    "    query_vec = query_vec / np.linalg.norm(query_vec)\n",
    "    test_queries.append(query_vec)\n",
    "\n",
    "print(f\"Generated {num_test_queries} test queries\")\n",
    "\n",
    "# Test FLAT index (ground truth)\n",
    "print(\"\\nTesting original FLAT index...\")\n",
    "flat_results = []\n",
    "flat_start = time.time()\n",
    "\n",
    "for query_vec in test_queries:\n",
    "    query = VectorQuery(\n",
    "        vector=query_vec,\n",
    "        vector_field_name=\"embedding\",\n",
    "        return_fields=[\"movie_id\", \"title\", \"genre\"],\n",
    "        dtype=\"float32\",\n",
    "        num_results=10\n",
    "    )\n",
    "    results = flat_index.query(query)\n",
    "    flat_results.append([doc[\"movie_id\"] for doc in results])\n",
    "\n",
    "flat_time = time.time() - flat_start\n",
    "print(f\"FLAT search time: {flat_time:.3f}s ({flat_time/num_test_queries:.3f}s per query)\")\n",
    "\n",
    "# Test SVS-VAMANA index\n",
    "print(\"\\nTesting SVS-VAMANA index...\")\n",
    "svs_results = []\n",
    "svs_start = time.time()\n",
    "\n",
    "for i, query_vec in enumerate(test_queries):\n",
    "    # Adjust query vector for SVS index (handle dimensionality reduction)\n",
    "    if target_dims < dims:\n",
    "        svs_query_vec = query_vec[:target_dims]\n",
    "    else:\n",
    "        svs_query_vec = query_vec\n",
    "    \n",
    "    if target_dtype == 'float16':\n",
    "        svs_query_vec = svs_query_vec.astype(np.float16)\n",
    "    \n",
    "    query = VectorQuery(\n",
    "        vector=svs_query_vec,\n",
    "        vector_field_name=\"embedding\",\n",
    "        return_fields=[\"movie_id\", \"title\", \"genre\"],\n",
    "        dtype=target_dtype,\n",
    "        num_results=10\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = svs_index.query(query)\n",
    "        svs_results.append([doc[\"movie_id\"] for doc in results])\n",
    "    except Exception as e:\n",
    "        print(f\"Query {i+1} failed: {e}\")\n",
    "        svs_results.append([])\n",
    "\n",
    "svs_time = time.time() - svs_start\n",
    "print(f\"SVS search time: {svs_time:.3f}s ({svs_time/num_test_queries:.3f}s per query)\")\n",
    "\n",
    "# Calculate recall if we have results\n",
    "if svs_results and any(svs_results):\n",
    "    recalls = []\n",
    "    for flat_res, svs_res in zip(flat_results, svs_results):\n",
    "        if flat_res and svs_res:\n",
    "            intersection = set(flat_res).intersection(set(svs_res))\n",
    "            recall = len(intersection) / len(flat_res)\n",
    "            recalls.append(recall)\n",
    "    \n",
    "    if recalls:\n",
    "        avg_recall = np.mean(recalls)\n",
    "        print(f\"\\nüìà Average recall@10: {avg_recall:.3f} ({avg_recall*100:.1f}%)\")\n",
    "        \n",
    "        if avg_recall >= 0.9:\n",
    "            print(\"‚úÖ Excellent search quality maintained\")\n",
    "        elif avg_recall >= 0.8:\n",
    "            print(\"‚úÖ Good search quality maintained\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Search quality may be impacted - consider adjusting compression\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  SVS index may still be indexing - search quality test pending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Migration Decision Framework\n",
    "\n",
    "Based on the results, let's determine if migration is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Migration Analysis & Recommendation\n",
      "==================================================\n",
      "Dataset: 20 documents, 768-dimensional vectors\n",
      "Compression: LVQ4\n",
      "Datatype: float32 ‚Üí float32\n",
      "\n",
      "Memory savings: -0.0% (Modest)\n",
      "Search quality: 1.0% recall (Acceptable)\n",
      "Performance: 1.4x vs original (Acceptable)\n",
      "\n",
      "üèÜ RECOMMENDATION:\n",
      "‚ùå MIGRATION NOT RECOMMENDED\n",
      "   ‚Ä¢ Insufficient benefits for current dataset\n",
      "   ‚Ä¢ Consider larger dataset or different compression\n",
      "   ‚Ä¢ SVS-VAMANA works best with high-dimensional data\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ Migration Analysis & Recommendation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fallback configuration if not defined (for CI/CD compatibility)\n",
    "if 'selected_config' not in locals():\n",
    "    from redisvl.utils import CompressionAdvisor\n",
    "    selected_config = CompressionAdvisor.recommend(dims=dims, priority=\"memory\")\n",
    "\n",
    "# Summarize configuration\n",
    "print(f\"Dataset: {num_docs} documents, {dims}-dimensional vectors\")\n",
    "print(f\"Compression: {selected_config.get('compression', 'None')}\")\n",
    "print(f\"Datatype: float32 ‚Üí {selected_config['datatype']}\")\n",
    "if 'reduce' in selected_config:\n",
    "    reduction = ((dims - selected_config['reduce']) / dims) * 100\n",
    "    print(f\"Dimensions: {dims} ‚Üí {selected_config['reduce']} ({reduction:.1f}% reduction)\")\n",
    "print()\n",
    "\n",
    "# Decision criteria\n",
    "memory_savings_significant = False\n",
    "search_quality_acceptable = True\n",
    "performance_acceptable = True\n",
    "\n",
    "if flat_memory > 0 and svs_memory > 0:\n",
    "    savings_pct = ((flat_memory - svs_memory) / flat_memory) * 100\n",
    "    memory_savings_significant = savings_pct > 25  # 25%+ savings considered significant\n",
    "    print(f\"Memory savings: {savings_pct:.1f}% ({'Significant' if memory_savings_significant else 'Modest'})\")\n",
    "else:\n",
    "    print(\"Memory savings: Pending (SVS index still indexing)\")\n",
    "\n",
    "if 'recalls' in locals() and recalls:\n",
    "    avg_recall = np.mean(recalls)\n",
    "    search_quality_acceptable = avg_recall >= 0.8  # 80%+ recall considered acceptable\n",
    "    print(f\"Search quality: {avg_recall:.1f}% recall ({'Acceptable' if search_quality_acceptable else 'Needs improvement'})\")\n",
    "else:\n",
    "    print(\"Search quality: Pending validation\")\n",
    "\n",
    "if 'flat_time' in locals() and 'svs_time' in locals():\n",
    "    performance_ratio = svs_time / flat_time if flat_time > 0 else 1\n",
    "    performance_acceptable = performance_ratio <= 2.0  # Allow up to 2x slower\n",
    "    print(f\"Performance: {performance_ratio:.1f}x vs original ({'Acceptable' if performance_acceptable else 'Slower than expected'})\")\n",
    "else:\n",
    "    print(\"Performance: Pending comparison\")\n",
    "\n",
    "\n",
    "# Final recommendation\n",
    "print(\"\\nüèÜ RECOMMENDATION:\")\n",
    "if memory_savings_significant and search_quality_acceptable and performance_acceptable:\n",
    "    print(\"‚úÖ MIGRATE TO SVS-VAMANA\")\n",
    "    print(\"   ‚Ä¢ Significant memory savings achieved\")\n",
    "    print(\"   ‚Ä¢ Search quality maintained\")\n",
    "    print(\"   ‚Ä¢ Performance impact acceptable\")\n",
    "    print(\"   ‚Ä¢ Cost reduction benefits clear\")\n",
    "elif memory_savings_significant and search_quality_acceptable:\n",
    "    print(\"‚ö†Ô∏è  CONSIDER MIGRATION WITH MONITORING\")\n",
    "    print(\"   ‚Ä¢ Good memory savings and search quality\")\n",
    "    print(\"   ‚Ä¢ Monitor performance in production\")\n",
    "    print(\"   ‚Ä¢ Consider gradual rollout\")\n",
    "elif memory_savings_significant:\n",
    "    print(\"‚ö†Ô∏è  MIGRATION NEEDS TUNING\")\n",
    "    print(\"   ‚Ä¢ Memory savings achieved\")\n",
    "    print(\"   ‚Ä¢ Search quality or performance needs improvement\")\n",
    "    print(\"   ‚Ä¢ Try different compression settings\")\n",
    "else:\n",
    "    print(\"‚ùå MIGRATION NOT RECOMMENDED\")\n",
    "    print(\"   ‚Ä¢ Insufficient benefits for current dataset\")\n",
    "    print(\"   ‚Ä¢ Consider larger dataset or different compression\")\n",
    "    print(\"   ‚Ä¢ SVS-VAMANA works best with high-dimensional data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Production Migration Checklist\n",
    "\n",
    "If migration is recommended, follow this checklist for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Production Migration Checklist\n",
      "========================================\n",
      "\n",
      "PRE-MIGRATION:\n",
      "‚ñ° Backup existing index data\n",
      "‚ñ° Test migration on staging environment\n",
      "‚ñ° Validate search quality with real queries\n",
      "‚ñ° Measure baseline performance metrics\n",
      "‚ñ° Plan rollback strategy\n",
      "\n",
      "MIGRATION:\n",
      "‚ñ° Create SVS-VAMANA index with tested configuration\n",
      "‚ñ° Migrate data in batches during low-traffic periods\n",
      "‚ñ° Monitor memory usage and indexing progress\n",
      "‚ñ° Validate data integrity after migration\n",
      "‚ñ° Test search functionality thoroughly\n",
      "\n",
      "POST-MIGRATION:\n",
      "‚ñ° Monitor search performance and quality\n",
      "‚ñ° Track memory usage and cost savings\n",
      "‚ñ° Update application configuration\n",
      "‚ñ° Document new index settings\n",
      "‚ñ° Clean up old index after validation period\n",
      "\n",
      "üí° TIPS:\n",
      "‚Ä¢ Start with a subset of data for initial validation\n",
      "‚Ä¢ Use blue-green deployment for zero-downtime migration\n",
      "‚Ä¢ Monitor for 24-48 hours before removing old index\n",
      "‚Ä¢ Keep compression settings documented for future reference\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"üìã Production Migration Checklist\",\n",
    "    \"=\" * 40,\n",
    "    \"\\nPRE-MIGRATION:\",\n",
    "    \"‚ñ° Backup existing index data\",\n",
    "    \"‚ñ° Test migration on staging environment\",\n",
    "    \"‚ñ° Validate search quality with real queries\",\n",
    "    \"‚ñ° Measure baseline performance metrics\",\n",
    "    \"‚ñ° Plan rollback strategy\",\n",
    "    \"\\nMIGRATION:\",\n",
    "    \"‚ñ° Create SVS-VAMANA index with tested configuration\",\n",
    "    \"‚ñ° Migrate data in batches during low-traffic periods\",\n",
    "    \"‚ñ° Monitor memory usage and indexing progress\",\n",
    "    \"‚ñ° Validate data integrity after migration\",\n",
    "    \"‚ñ° Test search functionality thoroughly\",\n",
    "    \"\\nPOST-MIGRATION:\",\n",
    "    \"‚ñ° Monitor search performance and quality\",\n",
    "    \"‚ñ° Track memory usage and cost savings\",\n",
    "    \"‚ñ° Update application configuration\",\n",
    "    \"‚ñ° Document new index settings\",\n",
    "    \"‚ñ° Clean up old index after validation period\",\n",
    "    \"\\nüí° TIPS:\",\n",
    "    \"‚Ä¢ Start with a subset of data for initial validation\",\n",
    "    \"‚Ä¢ Use blue-green deployment for zero-downtime migration\",\n",
    "    \"‚Ä¢ Monitor for 24-48 hours before removing old index\",\n",
    "    \"‚Ä¢ Keep compression settings documented for future reference\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Cleanup\n",
    "\n",
    "Clean up the demonstration indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up demonstration indices...\n",
      "‚úÖ Deleted FLAT demonstration index\n",
      "‚úÖ Deleted SVS-VAMANA demonstration index\n",
      "\n",
      "üéâ Migration demonstration complete!\n",
      "\n",
      "Next steps:\n",
      "1. Apply learnings to your production data\n",
      "2. Test with your actual query patterns\n",
      "3. Monitor performance in your environment\n",
      "4. Consider gradual rollout strategy\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ Cleaning up demonstration indices...\")\n",
    "\n",
    "# Clean up FLAT index\n",
    "try:\n",
    "    flat_index.delete(drop=True)\n",
    "    print(\"‚úÖ Deleted FLAT demonstration index\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Failed to delete FLAT index: {e}\")\n",
    "\n",
    "# Clean up SVS index\n",
    "try:\n",
    "    svs_index.delete(drop=True)\n",
    "    print(\"‚úÖ Deleted SVS-VAMANA demonstration index\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Failed to delete SVS index: {e}\")\n",
    "\n",
    "print(\n",
    "    \"\\nüéâ Migration demonstration complete!\",\n",
    "    \"\\nNext steps:\",\n",
    "    \"1. Apply learnings to your production data\",\n",
    "    \"2. Test with your actual query patterns\",\n",
    "    \"3. Monitor performance in your environment\",\n",
    "    \"4. Consider gradual rollout strategy\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

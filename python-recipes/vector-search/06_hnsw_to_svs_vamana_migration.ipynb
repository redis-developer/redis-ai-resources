{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "# Migrating from HNSW to SVS-VAMANA\n",
    "\n",
    "## Let's Begin!\n",
    "<a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/vector-search/06_hnsw_to_svs_migration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This notebook demonstrates how to migrate existing HNSW vector indices to SVS-VAMANA for improved memory efficiency while maintaining search quality.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to assess your current HNSW index for migration\n",
    "- Step-by-step migration from HNSW to SVS-VAMANA\n",
    "- Memory usage comparison and cost analysis\n",
    "- Search quality validation between HNSW and SVS-VAMANA\n",
    "- Performance benchmarking and recall comparison\n",
    "- Migration decision framework for production systems\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Redis Stack 8.2.0+ with RediSearch 2.8.10+\n",
    "- Existing HNSW index with substantial data (1000+ documents recommended)\n",
    "- High-dimensional vectors (768+ dimensions for best compression benefits)\n",
    "\n",
    "## HNSW vs SVS-VAMANA\n",
    "\n",
    "**HNSW (Hierarchical Navigable Small World):**\n",
    "- Excellent search quality and recall\n",
    "- Fast query performance\n",
    "- Higher memory usage (stores full-precision vectors)\n",
    "- Good for applications prioritizing search quality\n",
    "\n",
    "**SVS-VAMANA:**\n",
    "- Competitive search quality with compression\n",
    "- Significant memory savings (50-75% reduction)\n",
    "- Built-in vector compression (LeanVec, quantization)\n",
    "- Ideal for large-scale deployments with cost constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup\n",
    "\n",
    "This notebook requires **sentence-transformers** for generating embeddings and **Redis Stack** running in Docker.\n",
    "\n",
    "**Requirements:**\n",
    "- Redis Stack 8.2.0+ with RediSearch 2.8.10+\n",
    "- sentence-transformers\n",
    "- redisvl (should be available in your environment)\n",
    "\n",
    "**üê≥ Docker Setup (Required):**\n",
    "\n",
    "Before running this notebook, make sure Redis Stack is running in Docker:\n",
    "\n",
    "```bash\n",
    "# Start Redis Stack with Docker\n",
    "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "```\n",
    "\n",
    "Or if you prefer using docker-compose, create a `docker-compose.yml` file:\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  redis:\n",
    "    image: redis/redis-stack:latest\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "      - \"8001:8001\"\n",
    "```\n",
    "\n",
    "Then run: `docker-compose up -d`\n",
    "\n",
    "**Manual Installation (if needed):**\n",
    "\n",
    "\n",
    "**For Google Colab:** All dependencies will be installed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install dependencies if needed\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def install_if_missing(package):\n",
    "#     try:\n",
    "#         __import__(package)\n",
    "#     except ImportError:\n",
    "#         print(f\"Installing {package}...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# # Check and install required packages\n",
    "# install_if_missing(\"sentence-transformers\")\n",
    "# install_if_missing(\"redisvl\")\n",
    "\n",
    "# print(\"‚úÖ All dependencies are ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Redis and RedisVL imports\n",
    "import redis\n",
    "from redisvl.index import SearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.redis.utils import array_to_buffer, buffer_to_array\n",
    "from redisvl.utils import CompressionAdvisor\n",
    "from redisvl.redis.connection import supports_svs\n",
    "\n",
    "# Configuration\n",
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Redis and SVS Support\n",
    "\n",
    "First, let's ensure Redis Stack is running and supports SVS-VAMANA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Redis connection successful\n",
      "üìä Redis version: 8.2.2\n",
      "‚úÖ SVS-VAMANA supported\n"
     ]
    }
   ],
   "source": [
    "# Test Redis connection and SVS support\n",
    "try:\n",
    "    client = redis.Redis.from_url(REDIS_URL)\n",
    "    client.ping()\n",
    "    print(\"‚úÖ Redis connection successful\")\n",
    "    \n",
    "    # Check Redis version\n",
    "    redis_info = client.info()\n",
    "    redis_version = redis_info['redis_version']\n",
    "    print(f\"üìä Redis version: {redis_version}\")\n",
    "    \n",
    "    # Check SVS support\n",
    "    if supports_svs(client):\n",
    "        print(\"‚úÖ SVS-VAMANA supported\")\n",
    "    else:\n",
    "        print(\"‚ùå SVS-VAMANA not supported\")\n",
    "        print(\"Please ensure you're using Redis Stack 8.2.0+ with RediSearch 2.8.10+\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Redis connection failed: {e}\")\n",
    "    print(\"Please ensure Redis Stack is running on localhost:6379\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Sample Data\n",
    "\n",
    "We'll use the movie dataset to demonstrate the migration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìΩÔ∏è Loaded 20 movie records\n",
      "Sample movie: Explosive Pursuit\n",
      "Genres available: {'comedy', 'action'}\n",
      "\n",
      "üîß Configuration:\n",
      "Vector dimensions: 1024\n",
      "Dataset size: 20 movie documents\n"
     ]
    }
   ],
   "source": [
    "# Load the movies dataset\n",
    "with open('resources/movies.json', 'r') as f:\n",
    "    movies_data = json.load(f)\n",
    "\n",
    "print(\n",
    "    f\"üìΩÔ∏è Loaded {len(movies_data)} movie records\",\n",
    "    f\"Sample movie: {movies_data[0]['title']}\",\n",
    "    f\"Genres available: {set(movie['genre'] for movie in movies_data)}\",\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "# Configuration for demonstration  \n",
    "dims = 1024  # sentence-transformers/all-roberta-large-v1 - 1024 dims\n",
    "num_docs = len(movies_data)  # Use actual dataset size\n",
    "\n",
    "print(\n",
    "    f\"\\nüîß Configuration:\",\n",
    "    f\"Vector dimensions: {dims}\",\n",
    "    f\"Dataset size: {num_docs} movie documents\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create HNSW Index\n",
    "\n",
    "First, we'll create an HNSW index with typical production settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HNSW index with optimized settings...\n",
      "‚úÖ Created HNSW index: hnsw_demo_index\n",
      "\n",
      "üîß HNSW Configuration:\n",
      "M (connections per node): 16\n",
      "EF Construction: 200\n",
      "EF Runtime: 10\n",
      "Distance metric: cosine\n",
      "Data type: float32\n"
     ]
    }
   ],
   "source": [
    "# Create HNSW schema with production-like settings\n",
    "hnsw_schema = {\n",
    "    \"index\": {\n",
    "        \"name\": \"hnsw_demo_index\",\n",
    "        \"prefix\": \"demo:hnsw:\",\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"movie_id\", \"type\": \"tag\"},\n",
    "        {\"name\": \"title\", \"type\": \"text\"},\n",
    "        {\"name\": \"genre\", \"type\": \"tag\"},\n",
    "        {\"name\": \"rating\", \"type\": \"numeric\"},\n",
    "        {\"name\": \"description\", \"type\": \"text\"},\n",
    "        {\n",
    "            \"name\": \"embedding\",\n",
    "            \"type\": \"vector\",\n",
    "            \"attrs\": {\n",
    "                \"dims\": dims,\n",
    "                \"algorithm\": \"hnsw\",\n",
    "                \"datatype\": \"float32\",\n",
    "                \"distance_metric\": \"cosine\",\n",
    "                \"m\": 16,  # Number of bi-directional links for each node\n",
    "                \"ef_construction\": 200,  # Size of dynamic candidate list\n",
    "                \"ef_runtime\": 10  # Size of dynamic candidate list during search\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Creating HNSW index with optimized settings...\")\n",
    "hnsw_index = SearchIndex.from_dict(hnsw_schema, redis_url=REDIS_URL)\n",
    "hnsw_index.create(overwrite=True)\n",
    "print(f\"‚úÖ Created HNSW index: {hnsw_index.name}\")\n",
    "\n",
    "# Display HNSW configuration\n",
    "print(\n",
    "    \"\\nüîß HNSW Configuration:\",\n",
    "    f\"M (connections per node): 16\",\n",
    "    f\"EF Construction: 200\",\n",
    "    f\"EF Runtime: 10\",\n",
    "    f\"Distance metric: cosine\",\n",
    "    f\"Data type: float32\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Embeddings and Load HNSW Index\n",
    "\n",
    "Generate embeddings for movie descriptions and populate the HNSW index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings for movie descriptions...\n",
      "12:16:47 sentence_transformers.SentenceTransformer INFO   Use pytorch device_name: mps\n",
      "12:16:47 sentence_transformers.SentenceTransformer INFO   Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b902d9279da4d269fbc39848a8f6240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated real embeddings using SentenceTransformer\n",
      "üìä Embedding shape: (20, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings (synthetic for demonstration)\n",
    "print(\"üîÑ Generating embeddings for movie descriptions...\")\n",
    "\n",
    "try:\n",
    "    # Try to use sentence-transformers if available\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # 384 dimensions\n",
    "    \n",
    "    # Generate real embeddings\n",
    "    descriptions = [movie['description'] for movie in movies_data]\n",
    "    embeddings = model.encode(descriptions, convert_to_numpy=True)\n",
    "    \n",
    "    # Pad to 1024 dimensions for demonstration\n",
    "    if embeddings.shape[1] < dims:\n",
    "        padding = np.zeros((embeddings.shape[0], dims - embeddings.shape[1]))\n",
    "        embeddings = np.concatenate([embeddings, padding], axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Generated real embeddings using SentenceTransformer\")\n",
    "    \n",
    "except ImportError:\n",
    "    # Fallback to synthetic embeddings\n",
    "    print(\"üìù SentenceTransformer not available, generating synthetic embeddings...\")\n",
    "    \n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    embeddings = []\n",
    "    \n",
    "    for i, movie in enumerate(movies_data):\n",
    "        # Create a pseudo-semantic embedding based on movie content\n",
    "        vector = np.random.random(dims).astype(np.float32)\n",
    "        \n",
    "        # Add some structure based on genre\n",
    "        if movie['genre'] == 'action':\n",
    "            vector[:50] += 0.3  # Action movies cluster\n",
    "        else:  # comedy\n",
    "            vector[50:100] += 0.3  # Comedy movies cluster\n",
    "        \n",
    "        # Normalize\n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        embeddings.append(vector)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"‚úÖ Generated {len(embeddings)} synthetic embeddings\")\n",
    "\n",
    "print(f\"üìä Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Prepared 20 documents for indexing\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for loading into HNSW index\n",
    "sample_data = []\n",
    "for i, movie in enumerate(movies_data):\n",
    "    sample_data.append({\n",
    "        'movie_id': str(movie['id']),\n",
    "        'title': movie['title'],\n",
    "        'genre': movie['genre'],\n",
    "        'rating': movie['rating'],\n",
    "        'description': movie['description'],\n",
    "        'embedding': array_to_buffer(embeddings[i].astype(np.float32), dtype='float32')\n",
    "    })\n",
    "\n",
    "print(f\"üì¶ Prepared {len(sample_data)} documents for indexing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data into HNSW index...\n",
      "  Loaded 20/20 documents\n",
      "‚è≥ Waiting for HNSW indexing to complete...\n",
      "\n",
      "‚úÖ HNSW index loaded with 20 documents\n",
      "Index size: 4.225975036621094 MB\n",
      "Indexing time: ~5 seconds (HNSW graph construction)\n"
     ]
    }
   ],
   "source": [
    "# Load data into HNSW index\n",
    "print(\"üì• Loading data into HNSW index...\")\n",
    "batch_size = 100  # Process in batches\n",
    "\n",
    "for i in range(0, len(sample_data), batch_size):\n",
    "    batch = sample_data[i:i+batch_size]\n",
    "    hnsw_index.load(batch)\n",
    "    print(f\"  Loaded {min(i+batch_size, len(sample_data))}/{len(sample_data)} documents\")\n",
    "\n",
    "# Wait for indexing to complete\n",
    "print(\"‚è≥ Waiting for HNSW indexing to complete...\")\n",
    "time.sleep(5)  # HNSW indexing takes longer than FLAT\n",
    "\n",
    "hnsw_info = hnsw_index.info()\n",
    "print(\n",
    "    f\"\\n‚úÖ HNSW index loaded with {hnsw_info['num_docs']} documents\",\n",
    "    f\"Index size: {hnsw_info.get('vector_index_sz_mb', 'N/A')} MB\",\n",
    "    f\"Indexing time: ~5 seconds (HNSW graph construction)\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Get Compression Recommendation\n",
    "\n",
    "Use the CompressionAdvisor to get optimal SVS-VAMANA settings for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing data for optimal compression settings...\n",
      "\n",
      "üìä Compression Recommendations:\n",
      "\n",
      "üóúÔ∏è Memory Priority:\n",
      "  Algorithm: svs-vamana\n",
      "  Compression: LeanVec4x8\n",
      "  Datatype: float16\n",
      "  Dimensions: 1024 ‚Üí 512\n",
      "\n",
      "‚öñÔ∏è Balanced Priority:\n",
      "  Algorithm: svs-vamana\n",
      "  Compression: LeanVec4x8\n",
      "  Datatype: float16\n",
      "  Dimensions: 1024 ‚Üí 512\n",
      "\n",
      "‚ö° Performance Priority:\n",
      "  Algorithm: svs-vamana\n",
      "  Compression: LeanVec4x8\n",
      "  Datatype: float16\n",
      "  Dimensions: 1024 ‚Üí 512\n",
      "\n",
      "‚úÖ Selected configuration: Memory Priority\n",
      "Expected memory reduction: ~50.0% from dimension reduction\n",
      "Additional savings from float16 compression\n"
     ]
    }
   ],
   "source": [
    "# Get compression recommendation\n",
    "print(\"üîç Analyzing data for optimal compression settings...\")\n",
    "\n",
    "# Get recommendations for different priorities\n",
    "memory_config = CompressionAdvisor.recommend(dims=dims, priority=\"memory\")\n",
    "balanced_config = CompressionAdvisor.recommend(dims=dims, priority=\"balanced\")\n",
    "performance_config = CompressionAdvisor.recommend(dims=dims, priority=\"performance\")\n",
    "\n",
    "print(\n",
    "    \"\\nüìä Compression Recommendations:\",\n",
    "    \"\",\n",
    "    \"üóúÔ∏è Memory Priority:\",\n",
    "    f\"  Algorithm: {memory_config['algorithm']}\",\n",
    "    f\"  Compression: {memory_config.get('compression', 'None')}\",\n",
    "    f\"  Datatype: {memory_config['datatype']}\",\n",
    "    f\"  Dimensions: {dims} ‚Üí {memory_config.get('reduce', dims)}\",\n",
    "    \"\",\n",
    "    \"‚öñÔ∏è Balanced Priority:\",\n",
    "    f\"  Algorithm: {balanced_config['algorithm']}\",\n",
    "    f\"  Compression: {balanced_config.get('compression', 'None')}\",\n",
    "    f\"  Datatype: {balanced_config['datatype']}\",\n",
    "    f\"  Dimensions: {dims} ‚Üí {balanced_config.get('reduce', dims)}\",\n",
    "    \"\",\n",
    "    \"‚ö° Performance Priority:\",\n",
    "    f\"  Algorithm: {performance_config['algorithm']}\",\n",
    "    f\"  Compression: {performance_config.get('compression', 'None')}\",\n",
    "    f\"  Datatype: {performance_config['datatype']}\",\n",
    "    f\"  Dimensions: {dims} ‚Üí {performance_config.get('reduce', dims)}\",\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "# Select configuration (using memory priority for maximum savings)\n",
    "selected_config = memory_config\n",
    "target_dims = selected_config.get('reduce', dims)\n",
    "target_dtype = selected_config['datatype']\n",
    "\n",
    "print(\n",
    "    f\"\\n‚úÖ Selected configuration: Memory Priority\",\n",
    "    f\"Expected memory reduction: ~{((dims - target_dims) / dims * 100):.1f}% from dimension reduction\",\n",
    "    f\"Additional savings from {selected_config['datatype']} compression\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create SVS-VAMANA Index\n",
    "\n",
    "Create the SVS-VAMANA index with the recommended compression settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SVS-VAMANA index with compression...\n",
      "‚úÖ Created SVS-VAMANA index: svs_demo_index\n",
      "Compression: LeanVec4x8\n",
      "Datatype: float16\n",
      "Dimensions: 1024 ‚Üí 512\n"
     ]
    }
   ],
   "source": [
    "# Create SVS-VAMANA schema with compression\n",
    "svs_schema = {\n",
    "    \"index\": {\n",
    "        \"name\": \"svs_demo_index\",\n",
    "        \"prefix\": \"demo:svs:\",\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"movie_id\", \"type\": \"tag\"},\n",
    "        {\"name\": \"title\", \"type\": \"text\"},\n",
    "        {\"name\": \"genre\", \"type\": \"tag\"},\n",
    "        {\"name\": \"rating\", \"type\": \"numeric\"},\n",
    "        {\"name\": \"description\", \"type\": \"text\"},\n",
    "        {\n",
    "            \"name\": \"embedding\",\n",
    "            \"type\": \"vector\",\n",
    "            \"attrs\": {\n",
    "                \"dims\": target_dims,  # Use reduced dimensions (512)\n",
    "                \"algorithm\": \"svs-vamana\",\n",
    "                \"datatype\": selected_config['datatype'],\n",
    "                \"distance_metric\": \"cosine\"\n",
    "                # Note: Don't include the full selected_config to avoid dims/reduce conflict\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Creating SVS-VAMANA index with compression...\")\n",
    "svs_index = SearchIndex.from_dict(svs_schema, redis_url=REDIS_URL)\n",
    "svs_index.create(overwrite=True)\n",
    "print(\n",
    "    f\"‚úÖ Created SVS-VAMANA index: {svs_index.name}\",\n",
    "    f\"Compression: {selected_config.get('compression', 'None')}\",\n",
    "    f\"Datatype: {selected_config['datatype']}\",\n",
    "    f\"Dimensions: {dims} ‚Üí {target_dims}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Migrate Data from HNSW to SVS-VAMANA\n",
    "\n",
    "Extract data from the HNSW index and migrate it to SVS-VAMANA with compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Extracting data from HNSW index...\n",
      "Found 20 documents to migrate\n",
      "Prepared 20 documents for migration\n"
     ]
    }
   ],
   "source": [
    "# Extract data from HNSW index\n",
    "print(\"üîÑ Extracting data from HNSW index...\")\n",
    "\n",
    "client = redis.Redis.from_url(REDIS_URL)\n",
    "keys = client.keys(\"demo:hnsw:*\")\n",
    "print(f\"Found {len(keys)} documents to migrate\")\n",
    "\n",
    "# Process and transform data for SVS index\n",
    "svs_data = []\n",
    "\n",
    "for key in keys:\n",
    "    doc_data = client.hgetall(key)\n",
    "    \n",
    "    if b'embedding' in doc_data:\n",
    "        # Extract original vector from HNSW index\n",
    "        original_vector = np.array(buffer_to_array(doc_data[b'embedding'], dtype='float32'))\n",
    "        \n",
    "        # Apply dimensionality reduction if needed (LeanVec)\n",
    "        if target_dims < dims:\n",
    "            vector = original_vector[:target_dims]\n",
    "        else:\n",
    "            vector = original_vector\n",
    "        \n",
    "        # Convert to target datatype\n",
    "        if target_dtype == 'float16':\n",
    "            vector = vector.astype(np.float16)\n",
    "        \n",
    "        svs_data.append({\n",
    "            \"movie_id\": doc_data[b'movie_id'].decode(),\n",
    "            \"title\": doc_data[b'title'].decode(),\n",
    "            \"genre\": doc_data[b'genre'].decode(),\n",
    "            \"rating\": int(doc_data[b'rating'].decode()),\n",
    "            \"description\": doc_data[b'description'].decode(),\n",
    "            \"embedding\": array_to_buffer(vector, dtype=target_dtype)\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(svs_data)} documents for migration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data into SVS-VAMANA index...\n",
      "  Migrated 20/20 documents\n",
      "‚è≥ Waiting for SVS-VAMANA indexing to complete...\n",
      "\n",
      "‚úÖ Migration complete! SVS index has 20 documents\n",
      "Index size: 1.017791748046875 MB\n"
     ]
    }
   ],
   "source": [
    "# Load data into SVS index\n",
    "print(\"üì• Loading data into SVS-VAMANA index...\")\n",
    "batch_size = 100  # Define batch size for migration\n",
    "\n",
    "if len(svs_data) > 0:\n",
    "    for i in range(0, len(svs_data), batch_size):\n",
    "        batch = svs_data[i:i+batch_size]\n",
    "        svs_index.load(batch)\n",
    "        print(f\"  Migrated {min(i+batch_size, len(svs_data))}/{len(svs_data)} documents\")\n",
    "\n",
    "    # Wait for indexing to complete\n",
    "    print(\"‚è≥ Waiting for SVS-VAMANA indexing to complete...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    svs_info = svs_index.info()\n",
    "    print(\n",
    "        f\"\\n‚úÖ Migration complete! SVS index has {svs_info['num_docs']} documents\",\n",
    "        f\"Index size: {svs_info.get('vector_index_sz_mb', 'N/A')} MB\",\n",
    "        sep=\"\\n\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data to migrate. Make sure the HNSW index was populated first.\")\n",
    "    print(\"   Run the previous cells to load data into the HNSW index.\")\n",
    "    svs_info = svs_index.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Memory Usage\n",
    "\n",
    "Analyze the memory savings achieved through the HNSW to SVS-VAMANA migration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Memory Usage Comparison\n",
      "========================================\n",
      "Original HNSW index:    4.23 MB\n",
      "SVS-VAMANA index:       1.02 MB\n",
      "\n",
      "üí∞ Memory savings: 75.9%\n",
      "Absolute reduction: 3.21 MB\n",
      "\n",
      "üíµ Cost Impact Analysis:\n",
      "Monthly cost reduction: $0.23\n",
      "Annual cost reduction: $2.71\n"
     ]
    }
   ],
   "source": [
    "# Helper function to extract memory info\n",
    "def get_memory_mb(index_info):\n",
    "    \"\"\"Extract memory usage in MB from index info\"\"\"\n",
    "    memory = index_info.get('vector_index_sz_mb', 0)\n",
    "    if isinstance(memory, str):\n",
    "        try:\n",
    "            return float(memory)\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    return float(memory)\n",
    "\n",
    "# Get memory usage\n",
    "hnsw_memory = get_memory_mb(hnsw_info)\n",
    "svs_memory = get_memory_mb(svs_info)\n",
    "\n",
    "print(\n",
    "    \"üìä Memory Usage Comparison\",\n",
    "    \"=\" * 40,\n",
    "    f\"Original HNSW index:    {hnsw_memory:.2f} MB\",\n",
    "    f\"SVS-VAMANA index:       {svs_memory:.2f} MB\",\n",
    "    \"\",\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "if hnsw_memory > 0:\n",
    "    if svs_memory > 0:\n",
    "        savings = ((hnsw_memory - svs_memory) / hnsw_memory) * 100\n",
    "        print(\n",
    "            f\"üí∞ Memory savings: {savings:.1f}%\",\n",
    "            f\"Absolute reduction: {hnsw_memory - svs_memory:.2f} MB\",\n",
    "            sep=\"\\n\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚è≥ SVS index still indexing - memory comparison pending\")\n",
    "        \n",
    "    # Cost analysis\n",
    "    print(\"\\nüíµ Cost Impact Analysis:\")\n",
    "    cost_per_gb_hour = 0.10  # Example cloud pricing\n",
    "    hours_per_month = 24 * 30\n",
    "    \n",
    "    hnsw_monthly_cost = (hnsw_memory / 1024) * cost_per_gb_hour * hours_per_month\n",
    "    if svs_memory > 0:\n",
    "        svs_monthly_cost = (svs_memory / 1024) * cost_per_gb_hour * hours_per_month\n",
    "        monthly_savings = hnsw_monthly_cost - svs_monthly_cost\n",
    "        print(\n",
    "            f\"Monthly cost reduction: ${monthly_savings:.2f}\",\n",
    "            f\"Annual cost reduction: ${monthly_savings * 12:.2f}\",\n",
    "            sep=\"\\n\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Current monthly cost: ${hnsw_monthly_cost:.2f}\",\n",
    "            \"Projected savings: Available after indexing completes\",\n",
    "            sep=\"\\n\"\n",
    "        )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Memory information not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Validate Search Quality\n",
    "\n",
    "Compare search quality between HNSW and SVS-VAMANA to ensure the migration maintains acceptable recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Generating test queries for quality validation...\n",
      "Generated 10 test queries\n"
     ]
    }
   ],
   "source": [
    "# Generate test queries\n",
    "print(\"üîç Generating test queries for quality validation...\")\n",
    "\n",
    "np.random.seed(123)  # For reproducible test queries\n",
    "num_test_queries = 10\n",
    "test_queries = []\n",
    "\n",
    "for i in range(num_test_queries):\n",
    "    # Create test query vectors\n",
    "    query_vec = np.random.random(dims).astype(np.float32)\n",
    "    query_vec = query_vec / np.linalg.norm(query_vec)  # Normalize\n",
    "    test_queries.append(query_vec)\n",
    "\n",
    "print(f\"Generated {len(test_queries)} test queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing HNSW search quality...\n",
      "HNSW search completed in 0.011 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test HNSW search quality\n",
    "print(\"üîç Testing HNSW search quality...\")\n",
    "\n",
    "hnsw_results = []\n",
    "hnsw_start = time.time()\n",
    "\n",
    "for query_vec in test_queries:\n",
    "    query = VectorQuery(\n",
    "        vector=query_vec,\n",
    "        vector_field_name=\"embedding\",\n",
    "        return_fields=[\"movie_id\", \"title\", \"genre\"],\n",
    "        dtype=\"float32\",\n",
    "        num_results=10\n",
    "    )\n",
    "    results = hnsw_index.query(query)\n",
    "    hnsw_results.append([doc[\"movie_id\"] for doc in results])\n",
    "\n",
    "hnsw_time = time.time() - hnsw_start\n",
    "print(f\"HNSW search completed in {hnsw_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing SVS-VAMANA search quality...\n",
      "SVS-VAMANA search completed in 0.008 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test SVS-VAMANA search quality\n",
    "print(\"üîç Testing SVS-VAMANA search quality...\")\n",
    "\n",
    "svs_results = []\n",
    "svs_start = time.time()\n",
    "\n",
    "for i, query_vec in enumerate(test_queries):\n",
    "    # Adjust query vector for SVS index (handle dimensionality reduction)\n",
    "    if target_dims < dims:\n",
    "        svs_query_vec = query_vec[:target_dims]\n",
    "    else:\n",
    "        svs_query_vec = query_vec\n",
    "    \n",
    "    if target_dtype == 'float16':\n",
    "        svs_query_vec = svs_query_vec.astype(np.float16)\n",
    "    \n",
    "    query = VectorQuery(\n",
    "        vector=svs_query_vec,\n",
    "        vector_field_name=\"embedding\",\n",
    "        return_fields=[\"movie_id\", \"title\", \"genre\"],\n",
    "        dtype=target_dtype,\n",
    "        num_results=10\n",
    "    )\n",
    "    results = svs_index.query(query)\n",
    "    svs_results.append([doc[\"movie_id\"] for doc in results])\n",
    "\n",
    "svs_time = time.time() - svs_start\n",
    "print(f\"SVS-VAMANA search completed in {svs_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Search Quality Comparison\n",
      "========================================\n",
      "Recall@5:  1.000 (100.0%)\n",
      "Recall@10: 0.990 (99.0%)\n",
      "\n",
      "‚è±Ô∏è Performance Comparison:\n",
      "HNSW query time:     0.011s (1.1ms per query)\n",
      "SVS-VAMANA query time: 0.008s (0.8ms per query)\n",
      "Speed difference:    +23.0%\n",
      "\n",
      "üéØ Quality Assessment: üü¢ Excellent - Minimal quality loss\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall and performance metrics\n",
    "def calculate_recall(reference_results, test_results, k=10):\n",
    "    \"\"\"Calculate recall@k between two result sets\"\"\"\n",
    "    if not reference_results or not test_results:\n",
    "        return 0.0\n",
    "    \n",
    "    total_recall = 0.0\n",
    "    for ref, test in zip(reference_results, test_results):\n",
    "        ref_set = set(ref[:k])\n",
    "        test_set = set(test[:k])\n",
    "        if len(ref_set) > 0:\n",
    "            recall = len(ref_set.intersection(test_set)) / len(ref_set)\n",
    "            total_recall += recall\n",
    "    \n",
    "    return total_recall / len(reference_results)\n",
    "\n",
    "# Calculate metrics\n",
    "recall_at_5 = calculate_recall(hnsw_results, svs_results, k=5)\n",
    "recall_at_10 = calculate_recall(hnsw_results, svs_results, k=10)\n",
    "\n",
    "print(\n",
    "    \"üìä Search Quality Comparison\",\n",
    "    \"=\" * 40,\n",
    "    f\"Recall@5:  {recall_at_5:.3f} ({recall_at_5*100:.1f}%)\",\n",
    "    f\"Recall@10: {recall_at_10:.3f} ({recall_at_10*100:.1f}%)\",\n",
    "    \"\",\n",
    "    \"‚è±Ô∏è Performance Comparison:\",\n",
    "    f\"HNSW query time:     {hnsw_time:.3f}s ({hnsw_time/num_test_queries*1000:.1f}ms per query)\",\n",
    "    f\"SVS-VAMANA query time: {svs_time:.3f}s ({svs_time/num_test_queries*1000:.1f}ms per query)\",\n",
    "    f\"Speed difference:    {((hnsw_time - svs_time) / hnsw_time * 100):+.1f}%\",\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "# Quality assessment\n",
    "if recall_at_10 >= 0.95:\n",
    "    quality_assessment = \"üü¢ Excellent - Minimal quality loss\"\n",
    "elif recall_at_10 >= 0.90:\n",
    "    quality_assessment = \"üü° Good - Acceptable quality for most applications\"\n",
    "elif recall_at_10 >= 0.80:\n",
    "    quality_assessment = \"üü† Fair - Consider if quality requirements are flexible\"\n",
    "else:\n",
    "    quality_assessment = \"üî¥ Poor - Migration not recommended\"\n",
    "\n",
    "print(f\"\\nüéØ Quality Assessment: {quality_assessment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Migration Decision Framework\n",
    "\n",
    "Based on the analysis, determine if migration is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Migration Decision Analysis\n",
      "========================================\n",
      "\n",
      "üìä Criteria Evaluation:\n",
      "Memory savings: 75.9% ‚úÖ (threshold: 20%)\n",
      "Search quality: 0.990 ‚úÖ (threshold: 0.85)\n",
      "\n",
      "üéØ Migration Recommendation: üü¢ RECOMMENDED\n",
      "üí≠ Reasoning: Migration provides significant memory savings while maintaining good search quality.\n"
     ]
    }
   ],
   "source": [
    "# Migration decision logic\n",
    "memory_savings_threshold = 20  # Minimum 20% memory savings\n",
    "recall_threshold = 0.85  # Minimum 85% recall@10\n",
    "\n",
    "memory_savings_pct = ((hnsw_memory - svs_memory) / hnsw_memory * 100) if hnsw_memory > 0 and svs_memory > 0 else 0\n",
    "meets_memory_threshold = memory_savings_pct >= memory_savings_threshold\n",
    "meets_quality_threshold = recall_at_10 >= recall_threshold\n",
    "\n",
    "print(\n",
    "    \"ü§î Migration Decision Analysis\",\n",
    "    \"=\" * 40,\n",
    "    \"\",\n",
    "    \"üìä Criteria Evaluation:\",\n",
    "    f\"Memory savings: {memory_savings_pct:.1f}% {'‚úÖ' if meets_memory_threshold else '‚ùå'} (threshold: {memory_savings_threshold}%)\",\n",
    "    f\"Search quality: {recall_at_10:.3f} {'‚úÖ' if meets_quality_threshold else '‚ùå'} (threshold: {recall_threshold})\",\n",
    "    \"\",\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "if meets_memory_threshold and meets_quality_threshold:\n",
    "    recommendation = \"üü¢ RECOMMENDED\"\n",
    "    reasoning = \"Migration provides significant memory savings while maintaining good search quality.\"\n",
    "elif meets_memory_threshold and not meets_quality_threshold:\n",
    "    recommendation = \"üü° CONDITIONAL\"\n",
    "    reasoning = \"Good memory savings but reduced search quality. Consider if your application can tolerate lower recall.\"\n",
    "elif not meets_memory_threshold and meets_quality_threshold:\n",
    "    recommendation = \"üü† LIMITED BENEFIT\"\n",
    "    reasoning = \"Search quality is maintained but memory savings are minimal. Migration may not be worth the effort.\"\n",
    "else:\n",
    "    recommendation = \"üî¥ NOT RECOMMENDED\"\n",
    "    reasoning = \"Insufficient memory savings and/or poor search quality. Consider alternative optimization strategies.\"\n",
    "\n",
    "print(\n",
    "    f\"üéØ Migration Recommendation: {recommendation}\",\n",
    "    f\"üí≠ Reasoning: {reasoning}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Production Migration Checklist\n",
    "\n",
    "If migration is recommended, follow this checklist for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã HNSW to SVS-VAMANA Migration Checklist\n",
      "==================================================\n",
      "\n",
      "PRE-MIGRATION:\n",
      "‚ñ° Backup existing HNSW index data\n",
      "‚ñ° Test migration on staging environment\n",
      "‚ñ° Validate search quality with real queries\n",
      "‚ñ° Measure baseline HNSW performance metrics\n",
      "‚ñ° Plan rollback strategy\n",
      "‚ñ° Document current HNSW parameters (M, EF_construction, EF_runtime)\n",
      "\n",
      "MIGRATION:\n",
      "‚ñ° Create SVS-VAMANA index with tested configuration\n",
      "‚ñ° Migrate data in batches during low-traffic periods\n",
      "‚ñ° Monitor memory usage and indexing progress\n",
      "‚ñ° Validate data integrity after migration\n",
      "‚ñ° Test search functionality thoroughly\n",
      "‚ñ° Compare recall metrics with baseline\n",
      "\n",
      "POST-MIGRATION:\n",
      "‚ñ° Monitor search performance and quality\n",
      "‚ñ° Track memory usage and cost savings\n",
      "‚ñ° Update application configuration\n",
      "‚ñ° Document new SVS-VAMANA settings\n",
      "‚ñ° Clean up old HNSW index after validation period\n",
      "‚ñ° Update monitoring and alerting thresholds\n",
      "\n",
      "üí° HNSW-SPECIFIC TIPS:\n",
      "‚Ä¢ HNSW indices are more complex to rebuild than FLAT\n",
      "‚Ä¢ Consider the impact on applications using EF_runtime tuning\n",
      "‚Ä¢ SVS-VAMANA may have different optimal query parameters\n",
      "‚Ä¢ Test with your specific HNSW configuration (M, EF values)\n",
      "‚Ä¢ Monitor for 48-72 hours before removing HNSW index\n",
      "‚Ä¢ Keep compression settings documented for future reference\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"üìã HNSW to SVS-VAMANA Migration Checklist\",\n",
    "    \"=\" * 50,\n",
    "    \"\\nPRE-MIGRATION:\",\n",
    "    \"‚ñ° Backup existing HNSW index data\",\n",
    "    \"‚ñ° Test migration on staging environment\",\n",
    "    \"‚ñ° Validate search quality with real queries\",\n",
    "    \"‚ñ° Measure baseline HNSW performance metrics\",\n",
    "    \"‚ñ° Plan rollback strategy\",\n",
    "    \"‚ñ° Document current HNSW parameters (M, EF_construction, EF_runtime)\",\n",
    "    \"\\nMIGRATION:\",\n",
    "    \"‚ñ° Create SVS-VAMANA index with tested configuration\",\n",
    "    \"‚ñ° Migrate data in batches during low-traffic periods\",\n",
    "    \"‚ñ° Monitor memory usage and indexing progress\",\n",
    "    \"‚ñ° Validate data integrity after migration\",\n",
    "    \"‚ñ° Test search functionality thoroughly\",\n",
    "    \"‚ñ° Compare recall metrics with baseline\",\n",
    "    \"\\nPOST-MIGRATION:\",\n",
    "    \"‚ñ° Monitor search performance and quality\",\n",
    "    \"‚ñ° Track memory usage and cost savings\",\n",
    "    \"‚ñ° Update application configuration\",\n",
    "    \"‚ñ° Document new SVS-VAMANA settings\",\n",
    "    \"‚ñ° Clean up old HNSW index after validation period\",\n",
    "    \"‚ñ° Update monitoring and alerting thresholds\",\n",
    "    \"\\nüí° HNSW-SPECIFIC TIPS:\",\n",
    "    \"‚Ä¢ HNSW indices are more complex to rebuild than FLAT\",\n",
    "    \"‚Ä¢ Consider the impact on applications using EF_runtime tuning\",\n",
    "    \"‚Ä¢ SVS-VAMANA may have different optimal query parameters\",\n",
    "    \"‚Ä¢ Test with your specific HNSW configuration (M, EF values)\",\n",
    "    \"‚Ä¢ Monitor for 48-72 hours before removing HNSW index\",\n",
    "    \"‚Ä¢ Keep compression settings documented for future reference\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Cleanup\n",
    "\n",
    "Clean up the demonstration indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up demonstration indices...\n",
      "‚úÖ Deleted HNSW demonstration index\n",
      "‚úÖ Deleted SVS-VAMANA demonstration index\n",
      "\n",
      "üéâ HNSW to SVS-VAMANA migration demonstration complete!\n",
      "\n",
      "Next steps:\n",
      "1. Apply learnings to your production HNSW indices\n",
      "2. Test with your actual query patterns and data\n",
      "3. Monitor performance in your environment\n",
      "4. Consider gradual rollout strategy\n",
      "5. Evaluate impact on applications using HNSW-specific features\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ Cleaning up demonstration indices...\")\n",
    "\n",
    "# Clean up HNSW index\n",
    "try:\n",
    "    hnsw_index.delete(drop=True)\n",
    "    print(\"‚úÖ Deleted HNSW demonstration index\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Failed to delete HNSW index: {e}\")\n",
    "\n",
    "# Clean up SVS index\n",
    "try:\n",
    "    svs_index.delete(drop=True)\n",
    "    print(\"‚úÖ Deleted SVS-VAMANA demonstration index\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Failed to delete SVS index: {e}\")\n",
    "\n",
    "print(\n",
    "    \"\\nüéâ HNSW to SVS-VAMANA migration demonstration complete!\",\n",
    "    \"\\nNext steps:\",\n",
    "    \"1. Apply learnings to your production HNSW indices\",\n",
    "    \"2. Test with your actual query patterns and data\",\n",
    "    \"3. Monitor performance in your environment\",\n",
    "    \"4. Consider gradual rollout strategy\",\n",
    "    \"5. Evaluate impact on applications using HNSW-specific features\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

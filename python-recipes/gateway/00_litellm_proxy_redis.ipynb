{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c3fefa",
   "metadata": {},
   "source": [
    "\n",
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?width=120)\n",
    "\n",
    "# LiteLLM Proxy with Redis\n",
    "\n",
    "This notebook demonstrates how to use [LiteLLM](https://github.com/BerriAI/litellm) with Redis to build a powerful and efficient LLM proxy server with caching & rate limiting capabilities. LiteLLM provides a unified interface for accessing multiple LLM providers while Redis enhances performance of the application in several different ways.\n",
    "\n",
    "*This recipe will help you understand*:\n",
    "\n",
    "* **Why** and **how** to implement exact and semantic caching for LLM calls\n",
    "* **How** to set up rate limiting for your LLM APIs\n",
    "* **How** LiteLLM integrates with Redis for state management \n",
    "* **How to measure** the performance benefits of caching\n",
    "\n",
    "> **Open in Colab**   ↘︎  \n",
    "> <a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/gateway/00_litellm_proxy_redis.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7b959",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment Setup  \n",
    "\n",
    "### Install Python Dependencies\n",
    "Before we begin, we need to make sure our environment is properly set up with all the necessary tools:\n",
    "\n",
    "**Requirements**:\n",
    "* Python ≥ 3.9 with the below packages\n",
    "* OpenAI API key (set as `OPENAI_API_KEY` environment variable)\n",
    "\n",
    "First, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47246c48",
   "metadata": {
    "id": "pip"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"litellm[proxy]\" \"redisvl==0.5.2\" requests openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "redis-setup",
   "metadata": {},
   "source": [
    "### Install Redis Stack\n",
    "\n",
    "\n",
    "#### For Colab\n",
    "Use the shell script below to download, extract, and install [Redis Stack](https://redis.io/docs/getting-started/install-stack/) directly from the Redis package archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db80601",
   "metadata": {
    "id": "redis-install"
   },
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "%%sh\n",
    "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
    "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
    "sudo apt-get update  > /dev/null 2>&1\n",
    "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
    "redis-stack-server --daemonize yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750e779",
   "metadata": {},
   "source": [
    "#### For Alternative Environments\n",
    "There are many ways to get the necessary redis-stack instance running\n",
    "1. On cloud, deploy a [FREE instance of Redis in the cloud](https://redis.io/try-free/). Or, if you have your\n",
    "own version of Redis Enterprise running, that works too!\n",
    "2. Per OS, [see the docs](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/)\n",
    "3. With docker: `docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e9fe3",
   "metadata": {},
   "source": [
    "### Define the Redis Connection URL\n",
    "\n",
    "By default this notebook connects to the local instance of Redis Stack. **If you have your own Redis Enterprise instance** - replace REDIS_PASSWORD, REDIS_HOST and REDIS_PORT values with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be77a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace values below with your own if using Redis Cloud instance\n",
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\") # ex: \"redis-18374.c253.us-central1-1.gce.cloud.redislabs.com\"\n",
    "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")      # ex: 18374\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")  # ex: \"1TNxTEdYRDgIDKM2gDfasupCADXXXX\"\n",
    "\n",
    "# If SSL is enabled on the endpoint, use rediss:// as the URL prefix\n",
    "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "redis-connection",
   "metadata": {},
   "source": [
    "### Verify Redis Connection\n",
    "\n",
    "Let's test our Redis connection to make sure it's working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ddcabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from redis import Redis\n",
    "\n",
    "client = Redis.from_url(REDIS_URL)\n",
    "client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce052678",
   "metadata": {},
   "source": [
    "### Set OPENAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e21ac07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65bfdd",
   "metadata": {},
   "source": [
    "## 2 · Understanding LiteLLM Caching with Redis\n",
    "\n",
    "LiteLLM Proxy with Redis provides several powerful capabilities that can significantly improve your LLM application performance and reliability:\n",
    "\n",
    "* **Exact cache (identical prompt)**: Uses Redis `SETEX` with TTL through the `cache:` configuration\n",
    "* **Semantic cache (similar prompt)**: Uses RediSearch **vector** indexing through the `semantic_cache:` configuration\n",
    "* **Rate-limit per user/key**: Uses Redis `INCR + EXPIRE` counters through the `rate_limit:` configuration\n",
    "* **Multi-model routing**: Uses Redis data structures for model configurations\n",
    "\n",
    "### Why Use Caching for LLMs?\n",
    "\n",
    "1. **Cost Reduction**: Avoid redundant API calls for identical or similar prompts\n",
    "2. **Latency Improvement**: Cached responses return in milliseconds vs. seconds\n",
    "3. **Reliability**: Reduce dependency on external API availability\n",
    "4. **Rate Limit Management**: Stay within API provider constraints\n",
    "\n",
    "In this notebook, we'll explore how these features work and measure their impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d003168",
   "metadata": {},
   "source": [
    "## 3 · Create a Multi-Model Configuration\n",
    "\n",
    "Let's create a configuration file for LiteLLM Proxy that includes caching, semantic caching, and rate limiting with Redis. This configuration will route requests to two OpenAI models: `gpt-3.5-turbo` and `gpt‑4o‑mini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d859197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to: litellm_redis.yml\n",
      "\n",
      "Configuration details:\n",
      "litellm_settings:\n",
      "  cache: true\n",
      "  cache_params:\n",
      "    host: localhost\n",
      "    password: ''\n",
      "    port: '6379'\n",
      "    type: redis\n",
      "  set_verbose: true\n",
      "model_list:\n",
      "- litellm_params:\n",
      "    model: gpt-3.5-turbo\n",
      "  model_name: openai-old\n",
      "- litellm_params:\n",
      "    model: gpt-4o\n",
      "  model_name: openai-new\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib, yaml, textwrap, json\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    \"model_list\": [\n",
    "        {\n",
    "            \"model_name\": \"openai-old\",\n",
    "            \"litellm_params\": {\n",
    "                \"model\": \"gpt-3.5-turbo\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"openai-new\",\n",
    "            \"litellm_params\": {\n",
    "                \"model\": \"gpt-4o\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"litellm_settings\": {\n",
    "        \"set_verbose\": True,\n",
    "        \"cache\": True,\n",
    "        \"cache_params\": {\n",
    "            \"type\": \"redis\",\n",
    "            \"host\": REDIS_HOST,\n",
    "            \"port\": REDIS_PORT,\n",
    "            \"password\": REDIS_PASSWORD\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "path = pathlib.Path(\"litellm_redis.yml\")\n",
    "path.write_text(yaml.dump(cfg))\n",
    "print(\"Configuration saved to:\", path)\n",
    "print(\"\\nConfiguration details:\")\n",
    "print(path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0860e",
   "metadata": {},
   "source": [
    "### Launch LiteLLM Proxy\n",
    "\n",
    "Now, let's start the LiteLLM Proxy server with our configuration. We'll use native Jupyter bash magic commands instead of using Python subprocess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f5a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "# Start LiteLLM Proxy in the background\n",
    "echo \"Starting LiteLLM Proxy on port 4000...\"\n",
    "litellm --config litellm_redis.yml --port 4000 > litellm_proxy.log 2>&1\n",
    "echo \"Proxy started! Check litellm_proxy.log for details.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proxy-check",
   "metadata": {},
   "source": [
    "Let's wait a few seconds for the proxy to start and check if it's running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "check-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for proxy to start...\n",
      "Proxy health check status: 200\n",
      "Response: {'healthy_endpoints': [], 'unhealthy_endpoints': [], 'healthy_count': 0, 'unhealthy_count': 0}\n"
     ]
    }
   ],
   "source": [
    "import time, requests\n",
    "print(\"Waiting for proxy to start...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Check if proxy is running\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:4000/health\")\n",
    "    print(f\"Proxy health check status: {response.status_code}\")\n",
    "    print(f\"Response: {response.json()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking proxy: {e}\")\n",
    "    print(\"\\nLast few lines from proxy log:\")\n",
    "    !tail -n 5 litellm_proxy.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-helper",
   "metadata": {},
   "source": [
    "### Create a Helper Function for Making Requests\n",
    "\n",
    "Let's create a function to make API calls to our proxy and measure response times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76cdc5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the chat function with a simple prompt...\n",
      "🕒 Response time: 0.01s | Cache: MISS | Semantic cache: MISS\n",
      "Status code: 400\n"
     ]
    }
   ],
   "source": [
    "import requests, time, json, textwrap\n",
    "\n",
    "def chat(prompt, model=\"gpt-3.5-turbo\", user=\"demo\", verbose=True):\n",
    "    \"\"\"Send a chat completion request to the LiteLLM proxy\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"user\": user\n",
    "    }\n",
    "    \n",
    "    t0 = time.time()\n",
    "    resp = requests.post(\"http://localhost:4000/v1/chat/completions\", \n",
    "                        json=payload, \n",
    "                        timeout=60)\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    if verbose:\n",
    "        cache_status = resp.headers.get(\"X-Cache\", \"MISS\")\n",
    "        semantic_cache = resp.headers.get(\"X-Semantic-Cache\", \"MISS\")\n",
    "        print(f\"🕒 Response time: {elapsed:.2f}s | Cache: {cache_status} | Semantic cache: {semantic_cache}\")\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            print(f\"🤖 Response: {resp.json()['choices'][0]['message']['content'][:100]}...\")\n",
    "            \n",
    "    return elapsed, resp\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing the chat function with a simple prompt...\")\n",
    "_, test_resp = chat(\"Introduce yourself briefly\")\n",
    "print(f\"Status code: {test_resp.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d935df11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"error\":{\"message\":\"{\\'error\\': \\'/chat/completions: Invalid model name passed in model=gpt-3.5-turbo. Call `/v1/models` to view available models for your key.\\'}\",\"type\":\"None\",\"param\":\"None\",\"code\":\"400\"}}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121e215",
   "metadata": {},
   "source": [
    "## 4 · Exact Cache Demonstration\n",
    "\n",
    "Now we'll demonstrate exact caching by sending the same prompt twice. The first request should hit the LLM API, while the second should be served from cache. We'll see this reflected in the response time and cache headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08699fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Exact Cache Experiment\")\n",
    "print(\"\\n1️⃣ First Request: (expecting cache MISS)\")\n",
    "lat1, res1 = chat(\"What are three benefits of Redis for LLM applications?\")\n",
    "\n",
    "print(\"\\n2️⃣ Second Request with Identical Prompt: (expecting cache HIT)\")\n",
    "lat2, res2 = chat(\"What are three benefits of Redis for LLM applications?\")\n",
    "\n",
    "print(f\"\\n🔍 Performance Analysis:\")\n",
    "print(f\"   First request: {lat1:.3f}s\")\n",
    "print(f\"   Second request: {lat2:.3f}s\")\n",
    "if lat1 > 0 and lat2 > 0:\n",
    "    print(f\"   Speed improvement: {lat1/lat2:.1f}x faster\")\n",
    "    print(f\"   Time saved: {lat1 - lat2:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd3fc6",
   "metadata": {},
   "source": [
    "### Examining the Cached Keys in Redis\n",
    "\n",
    "Let's look at the keys created in Redis for the exact cache and understand how LiteLLM structures them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all keys related to LiteLLM cache\n",
    "cache_keys = list(r.scan_iter(match=\"litellm:cache:*\"))\n",
    "print(f\"Found {len(cache_keys)} cache keys in Redis\")\n",
    "\n",
    "if cache_keys:\n",
    "    # Look at the first key\n",
    "    first_key = cache_keys[0]\n",
    "    print(f\"\\nExample cache key: {first_key}\")\n",
    "    \n",
    "    # Get TTL for the key\n",
    "    ttl = r.ttl(first_key)\n",
    "    print(f\"TTL: {ttl} seconds\")\n",
    "    \n",
    "    # Get the value (may be large, so limiting output)\n",
    "    value = r.get(first_key)\n",
    "    if value:\n",
    "        print(f\"Value type: {type(value).__name__}\")\n",
    "        print(f\"Value size: {len(value)} characters\")\n",
    "        try:\n",
    "            # Try to parse as JSON for better display\n",
    "            parsed = json.loads(value[:1000] + '...' if len(value) > 1000 else value)\n",
    "            print(f\"Content preview (JSON): {json.dumps(parsed, indent=2)[:300]}...\")\n",
    "        except:\n",
    "            print(f\"Content preview (raw): {value[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959ff3d",
   "metadata": {},
   "source": [
    "### Benchmarking Cached Response Times\n",
    "\n",
    "Now, let's precisely measure the cached response time using multiple repeated requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007710d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark cached response time with more samples\n",
    "def benchmark_cached_query(query, runs=5):\n",
    "    times = []\n",
    "    print(f\"Benchmarking cached query: '{query}'\")\n",
    "    print(f\"Running {runs} iterations...\")\n",
    "    \n",
    "    for i in range(runs):\n",
    "        start = time.time()\n",
    "        elapsed, resp = chat(query, verbose=False)\n",
    "        times.append(elapsed)\n",
    "        cache_status = resp.headers.get(\"X-Cache\", \"MISS\")\n",
    "        print(f\"  Run {i+1}: {elapsed:.4f}s | Cache: {cache_status}\")\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"\\nAverage response time: {avg_time:.4f}s\")\n",
    "    print(f\"Min: {min(times):.4f}s | Max: {max(times):.4f}s\")\n",
    "    return avg_time\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_cached_query(\"What are three benefits of Redis for LLM applications?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67888d4e",
   "metadata": {},
   "source": [
    "## 5 · Semantic Cache Demonstration\n",
    "\n",
    "Semantic caching is more powerful than exact caching because it can identify semantically similar prompts, not just identical ones. This is implemented using vector embeddings and similarity search in Redis.\n",
    "\n",
    "Let's test it by sending a prompt that is semantically similar (but not identical) to our previous query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ca8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Semantic Cache Experiment\")\n",
    "\n",
    "# First, let's send a new query that will be stored in the semantic cache\n",
    "print(\"\\n1️⃣ Establishing a baseline query for semantic cache:\")\n",
    "lat1, res1 = chat(\"Tell me a useful application of Redis for AI systems\")\n",
    "\n",
    "# Now send a semantically similar query\n",
    "print(\"\\n2️⃣ Testing a semantically similar query:\")\n",
    "lat2, res2 = chat(\"What's a good use case for Redis in artificial intelligence?\")\n",
    "\n",
    "# Try a completely different query\n",
    "print(\"\\n3️⃣ Testing an unrelated query (should not hit semantic cache):\")\n",
    "lat3, res3 = chat(\"How to make chocolate chip cookies?\")\n",
    "\n",
    "print(f\"\\n🔍 Performance Analysis:\")\n",
    "print(f\"   Original query: {lat1:.3f}s\")\n",
    "print(f\"   Similar query: {lat2:.3f}s\")\n",
    "print(f\"   Unrelated query: {lat3:.3f}s\")\n",
    "\n",
    "sim_cache_hit = \"HIT\" in res2.headers.get(\"X-Semantic-Cache\", \"MISS\")\n",
    "if sim_cache_hit and lat1 > 0 and lat2 > 0:\n",
    "    print(f\"   Speed improvement: {lat1/lat2:.1f}x faster for semantically similar query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566c681",
   "metadata": {},
   "source": [
    "### Examining Semantic Cache Keys\n",
    "\n",
    "Let's look at the keys and indices created in Redis for the semantic cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check semantic cache keys\n",
    "semantic_keys = list(r.scan_iter(match=\"litellm:semantic*\"))\n",
    "print(f\"Found {len(semantic_keys)} semantic cache keys in Redis\")\n",
    "\n",
    "if semantic_keys:\n",
    "    # Display the first few keys\n",
    "    for key in semantic_keys[:5]:\n",
    "        print(f\"  - {key}\")\n",
    "    \n",
    "    # Check for Redis Search indices\n",
    "    try:\n",
    "        indices = r.execute_command(\"FT._LIST\")\n",
    "        print(f\"\\nRedis Search indices: {indices}\")\n",
    "        \n",
    "        # Get info about the semantic cache index if it exists\n",
    "        semantic_index = [idx for idx in indices if \"semantic\" in idx.lower()]\n",
    "        if semantic_index:\n",
    "            index_info = r.execute_command(f\"FT.INFO {semantic_index[0]}\")\n",
    "            print(f\"\\nSemantic Index Info:\")\n",
    "            # Format and display selected info\n",
    "            info_dict = {index_info[i]: index_info[i+1] for i in range(0, len(index_info), 2) if i+1 < len(index_info)}\n",
    "            for k in ['num_docs', 'num_terms', 'index_name', 'index_definition']:\n",
    "                if k in info_dict:\n",
    "                    print(f\"  {k}: {info_dict[k]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing Redis Search indices: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semantic-cache-explain",
   "metadata": {},
   "source": [
    "### How Semantic Caching Works\n",
    "\n",
    "LiteLLM's semantic caching works through these steps:\n",
    "1. When a query arrives, LiteLLM generates an embedding vector for the query using the configured model\n",
    "2. This vector is searched against previously stored vectors in Redis using cosine similarity\n",
    "3. If a match is found with similarity above the threshold (we set 0.9), the cached response is returned\n",
    "4. If not, the query is sent to the LLM API and the result is cached with its vector\n",
    "\n",
    "This approach is especially valuable for:\n",
    "- Applications with many similar but not identical queries\n",
    "- Customer support systems where questions vary in phrasing but seek the same information\n",
    "- Educational applications where different students may ask similar questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cc017",
   "metadata": {},
   "source": [
    "## 6 · Multi-Model Routing with LiteLLM Proxy\n",
    "\n",
    "Our configuration enables access to multiple models through a single endpoint. Let's test both the configured models to verify they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21192be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Multi-Model Routing Demonstration\")\n",
    "\n",
    "models = [\"gpt-3.5-turbo\", \"gpt-4o-mini\"]\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\nTesting model: {model}\")\n",
    "    lat, res = chat(\"Say hi in two words\", model=model)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        response_content = res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        results[model] = {\n",
    "            \"latency\": lat,\n",
    "            \"response\": response_content,\n",
    "            \"model\": res.json().get(\"model\", model)\n",
    "        }\n",
    "        print(f\"✅ Success | Response: '{response_content}'\")\n",
    "    else:\n",
    "        print(f\"❌ Error | Status code: {res.status_code}\")\n",
    "        print(f\"Error message: {res.text}\")\n",
    "\n",
    "# Compare the models\n",
    "if len(results) > 1:\n",
    "    print(\"\\n📊 Model Comparison:\")\n",
    "    for model, data in results.items():\n",
    "        print(f\"  {model}: {data['latency']:.2f}s - '{data['response']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e740a",
   "metadata": {},
   "source": [
    "## 7 · Testing Failure Modes\n",
    "\n",
    "Let's examine how the proxy handles error conditions, which is important for building robust applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Testing Error Handling\")\n",
    "\n",
    "# Test with an unsupported model\n",
    "print(\"\\n1️⃣ Testing with non-existent model:\")\n",
    "_, bad_model_resp = chat(\"test\", model=\"gpt-nonexistent-001\")\n",
    "print(f\"Status: {bad_model_resp.status_code}\")\n",
    "print(f\"Error message: {json.dumps(bad_model_resp.json(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed4fa7",
   "metadata": {},
   "source": [
    "### Testing Rate Limiting\n",
    "\n",
    "The LiteLLM proxy includes rate limiting functionality, which helps protect your API keys from overuse. Let's test this by sending requests rapidly until we hit the rate limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db464c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Testing Rate Limiting\")\n",
    "print(\"Sending multiple requests with the same user ID to trigger rate limiting...\")\n",
    "\n",
    "for i in range(5):\n",
    "    _, r2 = chat(f\"Request {i+1}\", user=\"test-rate-limit\")\n",
    "    remaining = r2.headers.get(\"X-Rate-Limit-Remaining\", \"unknown\")\n",
    "    limit_reset = r2.headers.get(\"X-Rate-Limit-Reset\", \"unknown\")\n",
    "    \n",
    "    print(f\"Request {i+1}: Status {r2.status_code} | Remaining: {remaining} | Reset: {limit_reset}\")\n",
    "    \n",
    "    if r2.status_code == 429:\n",
    "        print(f\"Rate limit reached after {i+1} requests!\")\n",
    "        print(f\"Error response: {json.dumps(r2.json(), indent=2)}\")\n",
    "        break\n",
    "        \n",
    "    time.sleep(0.5)  # Small delay to see rate limiting in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-alternatives",
   "metadata": {},
   "source": [
    "## 8 · Implementation Options\n",
    "\n",
    "LiteLLM provides multiple ways to implement caching in your application:\n",
    "\n",
    "### Using LiteLLM Proxy (as shown)\n",
    "\n",
    "The proxy approach (demonstrated in this notebook) is recommended for production deployments because it:\n",
    "- Provides a unified API endpoint for all your models\n",
    "- Centralizes caching, rate-limiting, and fallback logic\n",
    "- Works with any client that uses the OpenAI API format\n",
    "- Supports multiple languages and frameworks\n",
    "\n",
    "### Direct Integration with LiteLLM Python SDK\n",
    "\n",
    "For Python applications, you can also integrate caching directly using the SDK. See the [LiteLLM Caching documentation](https://docs.litellm.ai/docs/caching/all_caches) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e0229",
   "metadata": {},
   "source": [
    "## 9 · Cleanup\n",
    "\n",
    "Let's stop the LiteLLM proxy server and clean up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ce8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Find and stop the LiteLLM process\n",
    "echo \"Stopping LiteLLM Proxy...\"\n",
    "litellm_pid=$(ps aux | grep \"litellm --config\" | grep -v grep | awk '{print $2}')\n",
    "if [ -n \"$litellm_pid\" ]; then\n",
    "    kill $litellm_pid\n",
    "    echo \"Stopped LiteLLM Proxy (PID: $litellm_pid)\"\n",
    "else\n",
    "    echo \"LiteLLM Proxy not found running\"\n",
    "fi\n",
    "\n",
    "# Optionally stop Redis if you started it just for this notebook\n",
    "# Note: Comment this out if you want to keep Redis running\n",
    "# redis-cli shutdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. **Set up LiteLLM Proxy** with Redis for caching and rate limiting\n",
    "2. **Configure exact and semantic caching** to improve performance\n",
    "3. **Measure the performance benefits** of caching LLM responses\n",
    "4. **Route requests to multiple models** through a single endpoint\n",
    "5. **Test error handling and rate limiting** behavior\n",
    "\n",
    "The benchmarks clearly show that implementing caching with Redis can significantly reduce response times and API costs, making it an essential component of production LLM applications.\n",
    "\n",
    "For more information, see the [LiteLLM documentation](https://docs.litellm.ai/docs/proxy/caching) and [Redis documentation](https://redis.io/docs/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "# LangCache: Semantic Caching with Redis Cloud\n",
        "\n",
        "This notebook demonstrates end-to-end semantic caching using **LangCache** - a managed Redis Cloud service accessed through the RedisVL library. LangCache provides enterprise-grade semantic caching with zero infrastructure management, making it ideal for production LLM applications.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/semantic-cache/04_langcache_semantic_caching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "**LangCache** is a fully managed semantic cache service built on Redis Cloud. It was integrated into RedisVL in version 0.11.0 as an `LLMCache` interface implementation, making it easy for RedisVL users to:\n",
        "\n",
        "- Transition to a fully managed caching service\n",
        "- Reduce LLM API costs by caching similar queries\n",
        "- Improve application response times\n",
        "- Access enterprise features without managing infrastructure\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "In this tutorial, you will:\n",
        "1. Set up LangCache with Redis Cloud\n",
        "2. Load and process a knowledge base (PDF documents)\n",
        "3. Generate FAQs using the Doc2Cache technique\n",
        "4. Pre-populate a semantic cache with tagged FAQs\n",
        "5. Test different cache matching strategies and thresholds\n",
        "6. Optimize cache performance using evaluation datasets\n",
        "7. Use the `langcache-embed` cross-encoder model\n",
        "8. Integrate the cache into a RAG pipeline\n",
        "9. Measure performance improvements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, we'll install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q \"redisvl>=0.11.0\" \"openai>=1.0.0\" \"langchain>=0.3.0\" \"langchain-community\" \"langchain-openai\" \"langcache\"\n",
        "%pip install -q \"pypdf\" \"sentence-transformers\" \"redis-retrieval-optimizer>=0.2.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16:36:26 numexpr.utils INFO   NumExpr defaulting to 10 threads.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# RedisVL imports\n",
        "from redisvl.extensions.cache.llm import LangCacheSemanticCache\n",
        "\n",
        "# Optimization\n",
        "from redis_retrieval_optimizer.threshold_optimization import CacheThresholdOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LangCache Setup\n",
        "\n",
        "### Sign Up for LangCache\n",
        "\n",
        "If you haven't already, sign up for a free Redis Cloud account:\n",
        "\n",
        "**[Log in or sign up for Redis Cloud →](https://cloud.redis.io/#/)**\n",
        "\n",
        "After signing up:\n",
        "1. Create a new database\n",
        "2. Create a new LangCache service (Select 'LangCache' on the left menu bar)\n",
        "3. Copy your **API Key**\n",
        "4. Copy your **Cache ID**\n",
        "5. Copy your **URL**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Semantic Cache with LangCache-Embed Model\n",
        "\n",
        "We'll create a cache instance using the `redis/langcache-embed-v1` model, which is specifically optimized for semantic caching tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "langcache_api_key = os.environ.get('LANGCACHE_API_KEY') # found on your cloud console\n",
        "langcache_id = os.environ.get('LANGCACHE_ID') # found on your cloud console\n",
        "server_url = \"https://aws-us-east-1.langcache.redis.io\" # found on your cloud console\n",
        "\n",
        "# Create Semantic Cache instance\n",
        "cache = LangCacheSemanticCache(\n",
        "    server_url=server_url,\n",
        "    cache_id=langcache_id,\n",
        "    api_key=langcache_api_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16:36:28 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "[{'entry_id': '5eb63bbbe01eeed093cb22bb8f5acdc3', 'prompt': 'hello world', 'response': 'hello world from langcache', 'vector_distance': 0.0, 'inserted_at': 0.0, 'updated_at': 0.0}]\n",
            "16:36:28 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:36:28 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "[{'entry_id': '5eb63bbbe01eeed093cb22bb8f5acdc3', 'prompt': 'hello world', 'response': 'hello world from langcache', 'vector_distance': 0.07242219999999999, 'inserted_at': 0.0, 'updated_at': 0.0}]\n"
          ]
        }
      ],
      "source": [
        "# Check your cache is workign\n",
        "r = cache.check('hello world')\n",
        "print(r) # should be empty on first run\n",
        "cache.store('hello world', 'hello world from langcache')\n",
        "r = cache.check('hi world')\n",
        "print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Prepare Datasets\n",
        "\n",
        "We'll work with three types of data:\n",
        "1. **Knowledge Base**: PDF document(s) that contain factual information\n",
        "2. **FAQs**: Derived from the knowledge base using Doc2Cache technique\n",
        "3. **Test Dataset**: For evaluating and optimizing cache performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/justin.cechmanek/.pyenv/versions/3.11.9/envs/redis-ai-res/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# LangChain imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize OpenAI LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize OpenAI LLM for FAQ generation and RAG\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=2000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load PDF Knowledge Base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download sample PDF if not already present\n",
        "!mkdir -p data\n",
        "!wget -q -O data/nvidia-10k.pdf https://raw.githubusercontent.com/redis-developer/redis-ai-resources/main/python-recipes/RAG/resources/nvd-10k-2023.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded PDF: data/nvidia-10k.pdf\n",
            "  Total pages: 169\n",
            "  Created chunks: 388\n",
            "\n",
            "Sample chunk preview:\n",
            "Table of Contents\n",
            "The world’s leading cloud service providers, or CSPs, and consumer internet companies use our GPUs and broader data center-scale\n",
            "accelerated computing platforms to enable, accelerate or enrich the services they deliver to billions of end-users, including search,\n",
            "recommendations, so...\n"
          ]
        }
      ],
      "source": [
        "# Load and chunk the PDF\n",
        "pdf_path = \"data/nvidia-10k.pdf\"\n",
        "\n",
        "# Configure text splitter for optimal chunk sizes\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Load and split the document\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Loaded PDF: {pdf_path}\")\n",
        "print(f\"  Total pages: {len(documents)}\")\n",
        "print(f\"  Created chunks: {len(chunks)}\")\n",
        "print(f\"\\nSample chunk preview:\")\n",
        "print(f\"{chunks[10].page_content[:300]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate FAQs Using Doc2Cache Technique\n",
        "\n",
        "The Doc2Cache approach uses an LLM to generate frequently asked questions from document chunks. These FAQs are then used to pre-populate the semantic cache with high-quality, factual responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the FAQ data model\n",
        "class QuestionAnswer(BaseModel):\n",
        "    question: str = Field(description=\"A frequently asked question derived from the document content\")\n",
        "    answer: str = Field(description=\"A factual answer to the question based on the document\")\n",
        "    category: str = Field(description=\"Category of the question (e.g., 'financial', 'products', 'operations')\")\n",
        "\n",
        "class FAQList(BaseModel):\n",
        "    faqs: List[QuestionAnswer] = Field(description=\"List of question-answer pairs extracted from the document\")\n",
        "\n",
        "# Set up JSON output parser\n",
        "json_parser = JsonOutputParser(pydantic_object=FAQList)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAQ generation chain configured\n"
          ]
        }
      ],
      "source": [
        "# Create the FAQ generation prompt\n",
        "faq_prompt = PromptTemplate(\n",
        "    template=\"\"\"You are a document analysis expert. Extract 3-5 high-quality FAQs from the following document chunk.\n",
        "\n",
        "Guidelines:\n",
        "- Generate diverse, specific questions that users would realistically ask\n",
        "- Provide accurate, complete answers based ONLY on the document content\n",
        "- Assign each FAQ to a category: 'financial', 'products', 'operations', 'technology', or 'general'\n",
        "- Avoid vague or overly generic questions\n",
        "- If the chunk lacks substantial content, return fewer FAQs\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Document Chunk:\n",
        "{doc_content}\n",
        "\n",
        "FAQs JSON:\"\"\",\n",
        "    input_variables=[\"doc_content\"],\n",
        "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Create the FAQ generation chain\n",
        "faq_chain = faq_prompt | llm | json_parser\n",
        "\n",
        "print(\"FAQ generation chain configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing FAQ generation on sample chunk...\n",
            "\n",
            "16:36:51 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Generated 5 FAQs:\n",
            "\n",
            "1. Q: What industries are leveraging NVIDIA's GPUs for automation?\n",
            "   Category: operations\n",
            "   A: A rapidly growing number of enterprises and startups across a broad range of industries, including transportation for autonomous driving, healthcare f...\n",
            "\n",
            "2. Q: What was the reason for the termination of the Arm Share Purchase Agreement?\n",
            "   Category: financial\n",
            "   A: The Share Purchase Agreement between NVIDIA and SoftBank was terminated due to significant regulatory challenges that prevented the completion of the ...\n",
            "\n",
            "3. Q: What types of products do professional designers create using NVIDIA's technology?\n",
            "   Category: products\n",
            "   A: Professional designers use NVIDIA's GPUs and software to create visual effects in movies and to design a variety of products, including cell phones an...\n"
          ]
        }
      ],
      "source": [
        "# Test FAQ generation on a single chunk\n",
        "print(\"Testing FAQ generation on sample chunk...\\n\")\n",
        "test_faqs = faq_chain.invoke({\"doc_content\": chunks[10].page_content})\n",
        "\n",
        "print(f\"Generated {len(test_faqs.get('faqs', []))} FAQs:\")\n",
        "for i, faq in enumerate(test_faqs.get('faqs', [])[:3], 1):\n",
        "    print(f\"\\n{i}. Q: {faq['question']}\")\n",
        "    print(f\"   Category: {faq['category']}\")\n",
        "    print(f\"   A: {faq['answer'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating FAQs from document chunks...\n",
            "\n",
            "Processing chunk 1/25...\n",
            "16:36:57 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:37:04 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:37:09 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:37:17 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:37:22 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Processing chunk 6/25...\n",
            "16:37:28 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:37:30 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:37:36 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:37:41 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:37:48 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Processing chunk 11/25...\n",
            "16:37:54 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:38:00 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:38:05 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:38:12 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:38:19 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Processing chunk 16/25...\n",
            "16:38:25 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:38:30 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:38:38 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:38:49 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:38:55 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Processing chunk 21/25...\n",
            "16:39:02 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:39:07 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:39:14 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:39:21 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "16:39:28 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\n",
            "Generated 112 FAQs total\n",
            "\n",
            "Category distribution:\n",
            "  technology: 31\n",
            "  operations: 24\n",
            "  products: 20\n",
            "  financial: 19\n",
            "  general: 18\n"
          ]
        }
      ],
      "source": [
        "# Generate FAQs from all chunks (limited to first 25 for demo purposes)\n",
        "def extract_faqs_from_chunks(chunks: List[Any], max_chunks: int = 25) -> List[Dict]:\n",
        "    \"\"\"Extract FAQs from document chunks using LLM.\"\"\"\n",
        "    all_faqs = []\n",
        "    \n",
        "    for i, chunk in enumerate(chunks[:max_chunks]):\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Processing chunk {i+1}/{min(len(chunks), max_chunks)}...\", flush=True)\n",
        "        \n",
        "        try:\n",
        "            result = faq_chain.invoke({\"doc_content\": chunk.page_content})\n",
        "            if result and result.get(\"faqs\"):\n",
        "                all_faqs.extend(result[\"faqs\"])\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Skipped chunk {i+1} due to error: {str(e)[:100]}\")\n",
        "            continue\n",
        "    \n",
        "    return all_faqs\n",
        "\n",
        "# Extract FAQs\n",
        "print(\"\\nGenerating FAQs from document chunks...\\n\")\n",
        "faqs = extract_faqs_from_chunks(chunks, max_chunks=25)\n",
        "\n",
        "print(f\"\\nGenerated {len(faqs)} FAQs total\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "categories = {}\n",
        "for faq in faqs:\n",
        "    cat = faq.get('category', 'unknown')\n",
        "    categories[cat] = categories.get(cat, 0) + 1\n",
        "for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  {cat}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Test/Evaluation Dataset\n",
        "\n",
        "We'll create a test dataset with:\n",
        "- **Positive examples**: Questions that should match cached FAQs\n",
        "- **Negative examples**: Questions that should NOT match cached FAQs\n",
        "- **Edge cases**: Slightly different phrasings to test threshold sensitivity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample FAQs for testing:\n",
            "\n",
            "1. What is the fiscal year end date for NVIDIA Corporation as reported in the Form 10-K?...\n",
            "\n",
            "2. What is the trading symbol for NVIDIA Corporation's common stock?...\n",
            "\n",
            "3. Where is NVIDIA Corporation's principal executive office located?...\n"
          ]
        }
      ],
      "source": [
        "# Select representative FAQs for test set\n",
        "sample_faqs = faqs[:10]  # Take first 10 FAQs\n",
        "\n",
        "print(\"Sample FAQs for testing:\")\n",
        "for i, faq in enumerate(sample_faqs[:3], 1):\n",
        "    print(f\"\\n{i}. {faq['question'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test dataset created\n",
            "  Negative examples: 5\n"
          ]
        }
      ],
      "source": [
        "# Create test dataset with negative examples (off-topic questions)\n",
        "negative_examples = [\n",
        "    {\"query\": \"What is the weather today?\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "    {\"query\": \"How do I cook pasta?\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "    {\"query\": \"What is the capital of France?\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "    {\"query\": \"Tell me a joke\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "    {\"query\": \"What time is it?\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "]\n",
        "\n",
        "print(f\"Test dataset created\")\n",
        "print(f\"  Negative examples: {len(negative_examples)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pre-Load Semantic Cache with FAQs\n",
        "\n",
        "Now we'll populate the cache instance with our generated FAQs. We'll use the `store()` API with metadata tags for filtering and organization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16:39:29 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "16:39:29 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'5eb63bbbe01eeed093cb22bb8f5acdc3'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Clear any existing cache entries\n",
        "r = cache.check('hello world')\n",
        "cache.store('hello world', 'hello world from langcache')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Storing FAQs in cache...\n",
            "\n",
            "  Stored 0/112 FAQs...\n",
            "16:39:29 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:29 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:30 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:30 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:30 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:30 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:30 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:30 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:30 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:30 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:31 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "  Stored 20/112 FAQs...\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:32 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:33 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "  Stored 40/112 FAQs...\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:34 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:35 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "  Stored 60/112 FAQs...\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:36 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:37 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "  Stored 80/112 FAQs...\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:38 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:39 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "  Stored 100/112 FAQs...\n",
            "16:39:40 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:41 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:41 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:41 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:41 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:41 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:41 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:41 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:41 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 201 Created\"\n",
            "\n",
            "Stored 112 FAQs in cache\n",
            "\n",
            "Example cache entries:\n",
            "\n",
            "1. Key: a629e006a52387e1d29243f32555b5b6\n",
            "   Q: What is the fiscal year end date for NVIDIA Corporation as reported in the Form ...\n",
            "\n",
            "2. Key: f7e7a16db20d8199b6a95de1129d8b03\n",
            "   Q: What is the trading symbol for NVIDIA Corporation's common stock?...\n"
          ]
        }
      ],
      "source": [
        "# Store FAQs in cache with metadata tags\n",
        "print(\"Storing FAQs in cache...\\n\")\n",
        "\n",
        "stored_count = 0\n",
        "cache_keys = {}  # Map questions to their cache keys\n",
        "\n",
        "for i, faq in enumerate(faqs):\n",
        "    if i % 20 == 0:\n",
        "        print(f\"  Stored {i}/{len(faqs)} FAQs...\", flush=True)\n",
        "    \n",
        "    try:\n",
        "        # Store with metadata - note that metadata is stored but not used for filtering in basic SemanticCache\n",
        "        # In production, you can use this for analytics and tracking\n",
        "        key = cache.store(\n",
        "            prompt=faq['question'],\n",
        "            response=faq['answer']\n",
        "        )\n",
        "        cache_keys[faq['question']] = key\n",
        "        stored_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Failed to store FAQ {i+1}: {str(e)[:100]}\")\n",
        "\n",
        "print(f\"\\nStored {stored_count} FAQs in cache\")\n",
        "print(f\"\\nExample cache entries:\")\n",
        "for i, (q, k) in enumerate(list(cache_keys.items())[:2], 1):\n",
        "    print(f\"\\n{i}. Key: {k}\")\n",
        "    print(f\"   Q: {q[:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Cache Retrieval with Different Strategies\n",
        "\n",
        "Let's test how the cache performs with different types of queries and matching thresholds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Exact Match Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing exact match queries:\n",
            "\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "1. Cache HIT\n",
            "   Query: What is the fiscal year end date for NVIDIA Corporation as reported in the Form ...\n",
            "   Answer: The fiscal year ended January 29, 2023....\n",
            "\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "2. Cache HIT\n",
            "   Query: What is the trading symbol for NVIDIA Corporation's common stock?...\n",
            "   Answer: The trading symbol for NVIDIA Corporation's common stock is NVDA....\n",
            "\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "3. Cache HIT\n",
            "   Query: Where is NVIDIA Corporation's principal executive office located?...\n",
            "   Answer: NVIDIA Corporation's principal executive office is located at 2788 San Tomas Expressway, Santa Clara...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with exact questions from cache\n",
        "print(\"Testing exact match queries:\\n\")\n",
        "\n",
        "for i, faq in enumerate(faqs[:3], 1):\n",
        "    result = cache.check(prompt=faq['question'])\n",
        "    \n",
        "    if result:\n",
        "        print(f\"{i}. Cache HIT\")\n",
        "        print(f\"   Query: {faq['question'][:80]}...\")\n",
        "        print(f\"   Answer: {result[0]['response'][:100]}...\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. ✗ Cache MISS\")\n",
        "        print(f\"   Query: {faq['question'][:80]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Semantic Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing semantic similarity:\n",
            "\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "1. Cache HIT (distance: 0.0629)\n",
            "   Query: Tell me about NVIDIA's revenue\n",
            "   Matched: How much revenue did NVIDIA generate?...\n",
            "   Answer: As of the latest available data in NVIDIA's 10-K filing for the fiscal year ended January 29, 2023, ...\n",
            "\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "2. ✗ Cache MISS\n",
            "   Query: What products does the company make?\n",
            "\n",
            "16:39:42 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "3. ✗ Cache MISS\n",
            "   Query: How is the company performing financially?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with semantically similar queries\n",
        "print(\"Testing semantic similarity:\\n\")\n",
        "\n",
        "similar_queries = [\n",
        "    \"Tell me about NVIDIA's revenue\",\n",
        "    \"What products does the company make?\",\n",
        "    \"How is the company performing financially?\",\n",
        "]\n",
        "\n",
        "for i, query in enumerate(similar_queries, 1):\n",
        "    result = cache.check(prompt=query, return_fields=[\"prompt\", \"response\", \"distance\"])\n",
        "    \n",
        "    if result:\n",
        "        print(f\"{i}. Cache HIT (distance: {result[0].get('vector_distance', 'N/A'):.4f})\")\n",
        "        print(f\"   Query: {query}\")\n",
        "        print(f\"   Matched: {result[0]['prompt'][:80]}...\")\n",
        "        print(f\"   Answer: {result[0]['response'][:100]}...\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. ✗ Cache MISS\")\n",
        "        print(f\"   Query: {query}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Cache with Sample Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing query: 'What is NVIDIA's main business?'\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "Cache HIT\n",
            "  Distance: 0.070051\n",
            "  Matched: What are the main business segments reported by NVIDIA?...\n"
          ]
        }
      ],
      "source": [
        "# Test cache behavior with a sample query\n",
        "test_query = \"What is NVIDIA's main business?\"\n",
        "\n",
        "print(f\"Testing query: '{test_query}'\")\n",
        "\n",
        "result = cache.check(prompt=test_query, return_fields=[\"prompt\", \"vector_distance\"])\n",
        "\n",
        "if result:\n",
        "    print(f\"Cache HIT\")\n",
        "    print(f\"  Distance: {result[0].get('vector_distance', 0):.6f}\")\n",
        "    print(f\"  Matched: {result[0]['prompt'][:80]}...\")\n",
        "else:\n",
        "    print(f\"✗ Cache MISS - No match found within threshold\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Negative Examples (Should Not Match)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing negative examples (should NOT match):\n",
            "\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "1. Correct MISS\n",
            "   Query: What is the weather today?\n",
            "\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "2. Correct MISS\n",
            "   Query: How do I cook pasta?\n",
            "\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "3. Correct MISS\n",
            "   Query: What is the capital of France?\n",
            "\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "4. Correct MISS\n",
            "   Query: Tell me a joke\n",
            "\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "5. Correct MISS\n",
            "   Query: What time is it?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with off-topic queries that should NOT match\n",
        "print(\"Testing negative examples (should NOT match):\\n\")\n",
        "\n",
        "for i, test_case in enumerate(negative_examples, 1):\n",
        "    result = cache.check(prompt=test_case['query'], return_fields=[\"prompt\", \"vector_distance\"])\n",
        "    \n",
        "    if result:\n",
        "        print(f\"{i}. ⚠️  UNEXPECTED HIT (distance: {result[0].get('vector_distance', 'N/A'):.4f})\")\n",
        "        print(f\"   Query: {test_case['query']}\")\n",
        "        print(f\"   Matched: {result[0]['prompt'][:80]}...\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. Correct MISS\")\n",
        "        print(f\"   Query: {test_case['query']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Optimize Cache Threshold\n",
        "\n",
        "Using the `CacheThresholdOptimizer`, we can automatically find the optimal distance threshold based on our test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created optimization test data:\n",
            "  Total examples: 10\n",
            "  Positive (should match): 5\n",
            "  Negative (should not match): 5\n"
          ]
        }
      ],
      "source": [
        "# Create optimization test data\n",
        "# Format: [{\"query\": \"...\", \"query_match\": \"cache_key_or_empty_string\"}, ...]\n",
        "\n",
        "optimization_test_data = []\n",
        "\n",
        "# Add positive examples (should match specific cache entries)\n",
        "for faq in faqs[:5]:\n",
        "    if faq['question'] in cache_keys:\n",
        "        optimization_test_data.append({\n",
        "            \"query\": faq['question'],\n",
        "            \"query_match\": cache_keys[faq['question']]\n",
        "        })\n",
        "\n",
        "# Add negative examples (should not match anything)\n",
        "for neg_example in negative_examples:\n",
        "    optimization_test_data.append({\n",
        "        \"query\": neg_example['query'],\n",
        "        \"query_match\": \"\"  # Empty string means it should NOT match\n",
        "    })\n",
        "\n",
        "print(f\"Created optimization test data:\")\n",
        "print(f\"  Total examples: {len(optimization_test_data)}\")\n",
        "print(f\"  Positive (should match): {sum(1 for x in optimization_test_data if x['query_match'])}\")\n",
        "print(f\"  Negative (should not match): {sum(1 for x in optimization_test_data if not x['query_match'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Re-testing negative examples with optimized threshold:\n",
            "\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "1. MISS (correct)\n",
            "   Query: What is the weather today?\n",
            "\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "2. MISS (correct)\n",
            "   Query: How do I cook pasta?\n",
            "\n",
            "16:39:43 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "3. MISS (correct)\n",
            "   Query: What is the capital of France?\n",
            "\n",
            "16:39:44 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "4. MISS (correct)\n",
            "   Query: Tell me a joke\n",
            "\n",
            "16:39:44 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "5. MISS (correct)\n",
            "   Query: What time is it?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Re-test with optimized threshold\n",
        "print(\"\\nRe-testing negative examples with optimized threshold:\\n\")\n",
        "\n",
        "for i, test_case in enumerate(negative_examples, 1):\n",
        "    result = cache.check(prompt=test_case['query'], return_fields=[\"prompt\", \"vector_distance\"])\n",
        "\n",
        "    if result:\n",
        "        print(f\"{i}. ⚠️  HIT (distance: {result[0].get('vector_distance', 'N/A'):.4f})\")\n",
        "        print(f\"   Query: {test_case['query']}\")\n",
        "        print(f\"   Matched: {result[0]['prompt'][:80]}...\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. MISS (correct)\")\n",
        "        print(f\"   Query: {test_case['query']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG Pipeline Integration\n",
        "\n",
        "Now let's integrate the semantic cache into a complete RAG pipeline and measure the performance improvements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Simple RAG Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG chain created\n"
          ]
        }
      ],
      "source": [
        "# Create a simple RAG prompt template\n",
        "rag_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant answering questions about NVIDIA based on their 10-K filing. Provide accurate, concise answers.\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Create RAG chain\n",
        "rag_chain = rag_template | llm\n",
        "\n",
        "print(\"RAG chain created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Cached RAG Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cached RAG function ready\n"
          ]
        }
      ],
      "source": [
        "def rag_with_cache(question: str, use_cache: bool = True) -> tuple:\n",
        "    \"\"\"\n",
        "    Process a question through RAG pipeline with optional semantic caching.\n",
        "    \n",
        "    Returns: (answer, cache_hit, response_time)\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    cache_hit = False\n",
        "    \n",
        "    # Check cache first if enabled\n",
        "    if use_cache:\n",
        "        cached_result = cache.check(prompt=question)\n",
        "        if cached_result:\n",
        "            answer = cached_result[0]['response']\n",
        "            cache_hit = True\n",
        "            response_time = time.time() - start_time\n",
        "            return answer, cache_hit, response_time\n",
        "    \n",
        "    # Cache miss - use LLM\n",
        "    answer = rag_chain.invoke({\"question\": question})\n",
        "    response_time = time.time() - start_time\n",
        "    \n",
        "    # Store in cache for future use\n",
        "    if use_cache and hasattr(answer, 'content'):\n",
        "        cache.store(prompt=question, response=answer.content)\n",
        "    elif use_cache:\n",
        "        cache.store(prompt=question, response=str(answer))\n",
        "    \n",
        "    return answer.content if hasattr(answer, 'content') else str(answer), cache_hit, response_time\n",
        "\n",
        "print(\"Cached RAG function ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Comparison: With vs Without Cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PERFORMANCE COMPARISON: With Cache vs Without Cache\n",
            "================================================================================\n",
            "\n",
            "[FIRST PASS - Populating Cache]\n",
            "\n",
            "16:39:44 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "1. What is NVIDIA's primary business?\n",
            "   Cache: HIT | Time: 0.109s\n",
            "   Answer: NVIDIA reports its business results in two segments: the Compute & Networking segment, which include...\n",
            "\n",
            "16:39:44 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "2. How much revenue did NVIDIA generate?\n",
            "   Cache: HIT | Time: 0.103s\n",
            "   Answer: As of the latest available data in NVIDIA's 10-K filing for the fiscal year ended January 29, 2023, ...\n",
            "\n",
            "16:39:44 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "3. What are NVIDIA's main products?\n",
            "   Cache: HIT | Time: 0.104s\n",
            "   Answer: NVIDIA sells a range of products primarily in the following categories:\n",
            "\n",
            "1. **Graphics Processing Un...\n",
            "\n",
            "\n",
            "[SECOND PASS - Cache Hits with Paraphrased Questions]\n",
            "\n",
            "16:39:44 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "1. What does NVIDIA do as a business?\n",
            "   Cache: HIT ✓ | Time: 0.128s\n",
            "   Answer: NVIDIA's business has evolved from a primary focus on gaming products to broader markets, transition...\n",
            "\n",
            "16:39:44 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "2. Can you tell me NVIDIA's revenue figures?\n",
            "   Cache: HIT ✓ | Time: 0.127s\n",
            "   Answer: As of the latest available data in NVIDIA's 10-K filing for the fiscal year ended January 29, 2023, ...\n",
            "\n",
            "16:39:44 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries/search \"HTTP/1.1 200 OK\"\n",
            "3. What products does NVIDIA sell?\n",
            "   Cache: HIT ✓ | Time: 0.099s\n",
            "   Answer: NVIDIA sells a range of products primarily in the following categories:\n",
            "\n",
            "1. **Graphics Processing Un...\n",
            "\n",
            "\n",
            "[THIRD PASS - Without Cache (Baseline)]\n",
            "\n",
            "16:39:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "1. What is NVIDIA's primary business?\n",
            "   Cache: DISABLED | Time: 1.584s\n",
            "\n",
            "16:39:47 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2. How much revenue did NVIDIA generate?\n",
            "   Cache: DISABLED | Time: 1.439s\n",
            "\n",
            "16:39:52 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "3. What are NVIDIA's main products?\n",
            "   Cache: DISABLED | Time: 4.368s\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PERFORMANCE SUMMARY\n",
            "================================================================================\n",
            "Average time - First pass (cache miss):  0.105s\n",
            "Average time - Second pass (cache hit):  0.118s\n",
            "Average time - Without cache:            2.464s\n",
            "\n",
            "Speedup with cache: 0.9x faster\n",
            "  Cache hit rate: 33%\n"
          ]
        }
      ],
      "source": [
        "# Test questions for RAG evaluation\n",
        "test_questions_rag = [\n",
        "    \"What is NVIDIA's primary business?\",\n",
        "    \"How much revenue did NVIDIA generate?\",\n",
        "    \"What are NVIDIA's main products?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE COMPARISON: With Cache vs Without Cache\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# First pass - populate cache (cache misses, must call LLM)\n",
        "print(\"\\n[FIRST PASS - Populating Cache]\\n\")\n",
        "first_pass_times = []\n",
        "\n",
        "for i, question in enumerate(test_questions_rag, 1):\n",
        "    answer, cache_hit, response_time = rag_with_cache(question, use_cache=True)\n",
        "    first_pass_times.append(response_time)\n",
        "    print(f\"{i}. {question}\")\n",
        "    print(f\"   Cache: {'HIT' if cache_hit else 'MISS'} | Time: {response_time:.3f}s\")\n",
        "    print(f\"   Answer: {answer[:100]}...\\n\")\n",
        "\n",
        "# Second pass - test cache hits with similar questions\n",
        "print(\"\\n[SECOND PASS - Cache Hits with Paraphrased Questions]\\n\")\n",
        "second_pass_times = []\n",
        "\n",
        "similar_questions = [\n",
        "    \"What does NVIDIA do as a business?\",\n",
        "    \"Can you tell me NVIDIA's revenue figures?\",\n",
        "    \"What products does NVIDIA sell?\",\n",
        "]\n",
        "\n",
        "for i, question in enumerate(similar_questions, 1):\n",
        "    answer, cache_hit, response_time = rag_with_cache(question, use_cache=True)\n",
        "    second_pass_times.append(response_time)\n",
        "    print(f\"{i}. {question}\")\n",
        "    print(f\"   Cache: {'HIT ✓' if cache_hit else 'MISS ✗'} | Time: {response_time:.3f}s\")\n",
        "    print(f\"   Answer: {answer[:100]}...\\n\")\n",
        "\n",
        "# Third pass - without cache (baseline)\n",
        "print(\"\\n[THIRD PASS - Without Cache (Baseline)]\\n\")\n",
        "no_cache_times = []\n",
        "\n",
        "for i, question in enumerate(test_questions_rag, 1):\n",
        "    answer, _, response_time = rag_with_cache(question, use_cache=False)\n",
        "    no_cache_times.append(response_time)\n",
        "    print(f\"{i}. {question}\")\n",
        "    print(f\"   Cache: DISABLED | Time: {response_time:.3f}s\\n\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "avg_first = sum(first_pass_times)/len(first_pass_times)\n",
        "avg_second = sum(second_pass_times)/len(second_pass_times)\n",
        "avg_no_cache = sum(no_cache_times)/len(no_cache_times)\n",
        "\n",
        "print(f\"Average time - First pass (cache miss):  {avg_first:.3f}s\")\n",
        "print(f\"Average time - Second pass (cache hit):  {avg_second:.3f}s\")\n",
        "print(f\"Average time - Without cache:            {avg_no_cache:.3f}s\")\n",
        "\n",
        "if avg_second > 0:\n",
        "    speedup = avg_first / avg_second\n",
        "    print(f\"\\nSpeedup with cache: {speedup:.1f}x faster\")\n",
        "\n",
        "cache_hit_count = sum(1 for i, _ in enumerate(similar_questions) if second_pass_times[i] < 0.1)\n",
        "cache_hit_rate = cache_hit_count / len(similar_questions)\n",
        "print(f\"  Cache hit rate: {cache_hit_rate*100:.0f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Best Practices and Tips\n",
        "\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Threshold Optimization**: Start conservative (0.10-0.15) and optimize based on real usage data\n",
        "2. **Doc2Cache**: Pre-populate your cache with high-quality FAQs for immediate benefits\n",
        "3. **Monitoring**: Track cache hit rates and adjust thresholds as user patterns emerge\n",
        "4. **Model Selection**: The `langcache-embed-v1` model is specifically optimized for caching tasks\n",
        "5. **Cost-Performance Balance**: Even a 50% cache hit rate provides significant cost savings\n",
        "\n",
        "### When to Use Semantic Caching\n",
        "\n",
        "✅ **Good Use Cases:**\n",
        "- High-traffic applications with repeated question patterns\n",
        "- Customer support chatbots\n",
        "- FAQ systems\n",
        "- Documentation Q&A\n",
        "- Product information queries\n",
        "- Educational content Q&A\n",
        "\n",
        "❌ **Less Suitable:**\n",
        "- Highly dynamic content requiring real-time data\n",
        "- Creative writing tasks needing variety\n",
        "- Personalized responses based on user-specific context\n",
        "- Time-sensitive queries (use TTL if needed)\n",
        "\n",
        "### Performance Tips\n",
        "\n",
        "1. **Batch Loading**: Pre-populate cache with Doc2Cache for immediate value\n",
        "2. **Monitor Hit Rates**: Track and adjust thresholds based on production metrics\n",
        "3. **A/B Testing**: Test different thresholds with a subset of traffic\n",
        "4. **Cache Warming**: Regularly update cache with trending topics\n",
        "5. **TTL Management**: Set time-to-live for entries that may become stale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Cleanup\n",
        "\n",
        "Clean up resources when done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16:39:52 httpx INFO   HTTP Request: DELETE https://aws-us-east-1.langcache.redis.io/v1/caches/56f7ba9bee374701a1253f21cd1ac35e/entries \"HTTP/1.1 400 Bad Request\"\n"
          ]
        },
        {
          "ename": "BadRequestErrorResponseContent",
          "evalue": "{\"detail\":\"attributes: cannot be blank.\",\"status\":400,\"title\":\"Invalid Request\",\"type\":\"/errors/invalid-data\"}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mBadRequestErrorResponseContent\u001b[39m            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Clear cache contents\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCache contents cleared\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/envs/redis-ai-res/lib/python3.11/site-packages/redisvl/extensions/cache/llm/langcache.py:557\u001b[39m, in \u001b[36mLangCacheSemanticCache.clear\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclear\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    553\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Clear the cache of all entries.\u001b[39;00m\n\u001b[32m    554\u001b[39m \n\u001b[32m    555\u001b[39m \u001b[33;03m    This is an alias for delete() to match the BaseCache interface.\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/envs/redis-ai-res/lib/python3.11/site-packages/redisvl/extensions/cache/llm/langcache.py:542\u001b[39m, in \u001b[36mLangCacheSemanticCache.delete\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdelete\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    537\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Delete the entire cache.\u001b[39;00m\n\u001b[32m    538\u001b[39m \n\u001b[32m    539\u001b[39m \u001b[33;03m    This deletes all entries in the cache by calling delete_query\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[33;03m    with no attributes.\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdelete_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/envs/redis-ai-res/lib/python3.11/site-packages/langcache/sdk.py:227\u001b[39m, in \u001b[36mLangCache.delete_query\u001b[39m\u001b[34m(self, attributes, retries, server_url, timeout_ms, http_headers)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m utils.match_response(http_res, \u001b[33m\"\u001b[39m\u001b[33m400\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    224\u001b[39m     response_data = unmarshal_json_response(\n\u001b[32m    225\u001b[39m         errors.BadRequestErrorResponseContentData, http_res\n\u001b[32m    226\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors.BadRequestErrorResponseContent(response_data, http_res)\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m utils.match_response(http_res, \u001b[33m\"\u001b[39m\u001b[33m401\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    229\u001b[39m     response_data = unmarshal_json_response(\n\u001b[32m    230\u001b[39m         errors.AuthenticationErrorResponseContentData, http_res\n\u001b[32m    231\u001b[39m     )\n",
            "\u001b[31mBadRequestErrorResponseContent\u001b[39m: {\"detail\":\"attributes: cannot be blank.\",\"status\":400,\"title\":\"Invalid Request\",\"type\":\"/errors/invalid-data\"}"
          ]
        }
      ],
      "source": [
        "# Clear cache contents\n",
        "cache.clear()\n",
        "print(\"Cache contents cleared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You've completed this comprehensive guide on semantic caching with LangCache and RedisVL. \n",
        "\n",
        "**What You've Learned:**\n",
        "- ✅ Set up and configure LangCache with Redis Cloud\n",
        "- ✅ Load and process PDF documents into knowledge bases\n",
        "- ✅ Generate FAQs using the Doc2Cache technique with LLMs\n",
        "- ✅ Pre-populate a semantic cache with tagged entries\n",
        "- ✅ Test different cache matching strategies and thresholds\n",
        "- ✅ Optimize cache performance using test datasets\n",
        "- ✅ Leverage the `redis/langcache-embed-v1` cross-encoder model\n",
        "- ✅ Integrate semantic caching into RAG pipelines\n",
        "- ✅ Measure performance improvements and cost savings\n",
        "\n",
        "**Next Steps:**\n",
        "- Experiment with different distance thresholds for your use case\n",
        "- Try other embedding models and compare performance\n",
        "- Implement cache analytics and monitoring in production\n",
        "- Explore advanced features like TTL, metadata filtering, and cache warming strategies\n",
        "- Scale your semantic cache to handle production traffic\n",
        "\n",
        "**Resources:**\n",
        "- [RedisVL Documentation](https://docs.redisvl.com/en/stable/index.html)\n",
        "- [LangCache Sign Up](https://redis.io/langcache/)\n",
        "- [Redis AI Resources](https://github.com/redis-developer/redis-ai-resources)\n",
        "- [Semantic Caching Paper](https://arxiv.org/abs/2504.02268)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "redis-ai-res",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

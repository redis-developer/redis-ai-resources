{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "# LangCache: Semantic Caching with Redis Cloud\n",
        "\n",
        "This notebook demonstrates end-to-end semantic caching using **LangCache** - a managed Redis Cloud service accessed through the RedisVL library. LangCache provides enterprise-grade semantic caching with zero infrastructure management, making it ideal for production LLM applications.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/redis-developer/redis-ai-resources/blob/main/python-recipes/semantic-cache/04_langcache_semantic_caching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "**LangCache** is a fully managed semantic cache service built on Redis Cloud. It was integrated into RedisVL in version 0.11.0 as an `LLMCache` interface implementation, making it easy for RedisVL users to:\n",
        "\n",
        "- Transition to a fully managed caching service\n",
        "- Reduce LLM API costs by caching similar queries\n",
        "- Improve application response times\n",
        "- Access enterprise features without managing infrastructure\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "In this tutorial, you will:\n",
        "1. Set up LangCache with Redis Cloud\n",
        "2. Load and process a knowledge base (PDF documents)\n",
        "3. Generate FAQs using the Doc2Cache technique\n",
        "4. Pre-populate a semantic cache with tagged FAQs\n",
        "5. Test different cache matching strategies and thresholds\n",
        "6. Optimize cache performance using evaluation datasets\n",
        "7. Use the `langcache-embed` cross-encoder model\n",
        "8. Integrate the cache into a RAG pipeline\n",
        "9. Measure performance improvements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, we'll install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "✓ Packages installed. Please restart the kernel if prompted, then continue with the next cell.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q \"redisvl>=0.11.0\" \"openai>=1.0.0\" \"langchain>=0.3.0\" \"langchain-community\" \"langchain-openai\"\n",
        "%pip install -q \"pypdf\" \"sentence-transformers\" \"redis-retrieval-optimizer>=0.2.0\"\n",
        "\n",
        "print(\"Packages installed. Please restart the kernel if prompted, then continue with the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: If you see import errors, restart the kernel and continue from this cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/justin.cechmanek/.pyenv/versions/3.11.9/envs/redis-ai-res/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11:13:29 numexpr.utils INFO   NumExpr defaulting to 10 threads.\n",
            "✓ All imports successful\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from getpass import getpass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# RedisVL imports\n",
        "from redisvl.extensions.cache.llm import SemanticCache\n",
        "from redisvl.utils.vectorize import HFTextVectorizer\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Optimization\n",
        "from redis_retrieval_optimizer.threshold_optimization import CacheThresholdOptimizer\n",
        "\n",
        "print(\"All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LangCache Setup\n",
        "\n",
        "### Sign Up for LangCache\n",
        "\n",
        "If you haven't already, sign up for a free LangCache account:\n",
        "\n",
        "**[Sign up for LangCache →](https://langcache.io/signup)**\n",
        "\n",
        "After signing up:\n",
        "1. Create a new cache instance\n",
        "2. Copy your **Endpoint URL** (looks like: `redis-xxxxx.langcache.io:xxxxx`)\n",
        "3. Copy your **Access Token/Password**\n",
        "4. Note your **Cache ID** (you'll use this as a prefix for organizing caches)\n",
        "\n",
        "> **Note:** For this workshop, you can alternatively use a standard Redis Cloud instance with Redis Stack. Simply provide your Redis Cloud connection details instead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Environment variables configured\n",
            "  Redis URL: :6379\n"
          ]
        }
      ],
      "source": [
        "# Redis/LangCache credentials\n",
        "if \"REDIS_URL\" not in os.environ:\n",
        "    redis_host = input(\"Enter your Redis/LangCache host (e.g., redis-xxxxx.langcache.io or localhost): \")\n",
        "    redis_port = input(\"Enter your Redis port (default: 6379): \") or \"6379\"\n",
        "    redis_password = getpass(\"Enter your Redis password (leave empty for local): \")\n",
        "    \n",
        "    # Build Redis URL\n",
        "    if redis_password:\n",
        "        os.environ[\"REDIS_URL\"] = f\"rediss://:{redis_password}@{redis_host}:{redis_port}\"\n",
        "    else:\n",
        "        os.environ[\"REDIS_URL\"] = f\"redis://{redis_host}:{redis_port}\"\n",
        "\n",
        "# OpenAI API key for LLM and embeddings\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "print(\"Environment variables configured\")\n",
        "print(f\"  Redis URL: {os.environ['REDIS_URL'].split('@')[-1] if '@' in os.environ['REDIS_URL'] else os.environ['REDIS_URL'].split('//')[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Semantic Cache with LangCache-Embed Model\n",
        "\n",
        "We'll create a cache instance using the `redis/langcache-embed-v1` model, which is specifically optimized for semantic caching tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11:14:02 datasets INFO   PyTorch version 2.7.0 available.\n",
            "11:14:03 sentence_transformers.SentenceTransformer INFO   Use pytorch device_name: mps\n",
            "11:14:03 sentence_transformers.SentenceTransformer INFO   Load pretrained SentenceTransformer: redis/langcache-embed-v1\n",
            "✓ Semantic Cache initialized: rag_faq_cache\n",
            "  Using model: redis/langcache-embed-v1\n",
            "  Distance threshold: 0.15\n"
          ]
        }
      ],
      "source": [
        "# Initialize the vectorizer with the LangCache embedding model\n",
        "# This model is specifically optimized for semantic caching with better precision/recall\n",
        "vectorizer = HFTextVectorizer(\n",
        "    model=\"redis/langcache-embed-v1\"\n",
        ")\n",
        "\n",
        "# Create Semantic Cache instance\n",
        "cache = SemanticCache(\n",
        "    name=\"rag_faq_cache\",\n",
        "    redis_url=os.environ[\"REDIS_URL\"],\n",
        "    vectorizer=vectorizer,\n",
        "    distance_threshold=0.15  # Initial threshold, we'll optimize this later\n",
        ")\n",
        "\n",
        "print(f\"Semantic Cache initialized: {cache.index.name}\")\n",
        "print(f\"  Using model: redis/langcache-embed-v1\")\n",
        "print(f\"  Distance threshold: {cache.distance_threshold}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize OpenAI LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ OpenAI LLM initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize OpenAI LLM for FAQ generation and RAG\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=2000\n",
        ")\n",
        "\n",
        "print(\"OpenAI LLM initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Prepare Datasets\n",
        "\n",
        "We'll work with three types of data:\n",
        "1. **Knowledge Base**: PDF document(s) that contain factual information\n",
        "2. **FAQs**: Derived from the knowledge base using Doc2Cache technique\n",
        "3. **Test Dataset**: For evaluating and optimizing cache performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load PDF Knowledge Base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Sample PDF downloaded\n"
          ]
        }
      ],
      "source": [
        "# Download sample PDF if not already present\n",
        "!mkdir -p data\n",
        "!wget -q -O data/nvidia-10k.pdf https://raw.githubusercontent.com/redis-developer/redis-ai-resources/main/python-recipes/RAG/resources/nvd-10k-2023.pdf\n",
        "\n",
        "print(\"Sample PDF downloaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded PDF: data/nvidia-10k.pdf\n",
            "  Total pages: 169\n",
            "  Created chunks: 388\n",
            "\n",
            "Sample chunk preview:\n",
            "Table of Contents\n",
            "The world’s leading cloud service providers, or CSPs, and consumer internet companies use our GPUs and broader data center-scale\n",
            "accelerated computing platforms to enable, accelerate or enrich the services they deliver to billions of end-users, including search,\n",
            "recommendations, so...\n"
          ]
        }
      ],
      "source": [
        "# Load and chunk the PDF\n",
        "pdf_path = \"data/nvidia-10k.pdf\"\n",
        "\n",
        "# Configure text splitter for optimal chunk sizes\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Load and split the document\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Loaded PDF: {pdf_path}\")\n",
        "print(f\"  Total pages: {len(documents)}\")\n",
        "print(f\"  Created chunks: {len(chunks)}\")\n",
        "print(f\"\\nSample chunk preview:\")\n",
        "print(f\"{chunks[10].page_content[:300]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate FAQs Using Doc2Cache Technique\n",
        "\n",
        "The Doc2Cache approach uses an LLM to generate frequently asked questions from document chunks. These FAQs are then used to pre-populate the semantic cache with high-quality, factual responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the FAQ data model\n",
        "class QuestionAnswer(BaseModel):\n",
        "    question: str = Field(description=\"A frequently asked question derived from the document content\")\n",
        "    answer: str = Field(description=\"A factual answer to the question based on the document\")\n",
        "    category: str = Field(description=\"Category of the question (e.g., 'financial', 'products', 'operations')\")\n",
        "\n",
        "class FAQList(BaseModel):\n",
        "    faqs: List[QuestionAnswer] = Field(description=\"List of question-answer pairs extracted from the document\")\n",
        "\n",
        "# Set up JSON output parser\n",
        "json_parser = JsonOutputParser(pydantic_object=FAQList)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ FAQ generation chain configured\n"
          ]
        }
      ],
      "source": [
        "# Create the FAQ generation prompt\n",
        "faq_prompt = PromptTemplate(\n",
        "    template=\"\"\"You are a document analysis expert. Extract 3-5 high-quality FAQs from the following document chunk.\n",
        "\n",
        "Guidelines:\n",
        "- Generate diverse, specific questions that users would realistically ask\n",
        "- Provide accurate, complete answers based ONLY on the document content\n",
        "- Assign each FAQ to a category: 'financial', 'products', 'operations', 'technology', or 'general'\n",
        "- Avoid vague or overly generic questions\n",
        "- If the chunk lacks substantial content, return fewer FAQs\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Document Chunk:\n",
        "{doc_content}\n",
        "\n",
        "FAQs JSON:\"\"\",\n",
        "    input_variables=[\"doc_content\"],\n",
        "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Create the FAQ generation chain\n",
        "faq_chain = faq_prompt | llm | json_parser\n",
        "\n",
        "print(\"FAQ generation chain configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing FAQ generation on sample chunk...\n",
            "\n",
            "11:14:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Generated 5 FAQs:\n",
            "\n",
            "1. Q: What industries are leveraging NVIDIA's GPUs and software for automation?\n",
            "   Category: operations\n",
            "   A: A rapidly growing number of enterprises and startups across a broad range of industries, including transportation for autonomous driving, healthcare f...\n",
            "\n",
            "2. Q: What was the reason for the termination of the Arm Share Purchase Agreement between NVIDIA and SoftBank?\n",
            "   Category: general\n",
            "   A: The termination of the Share Purchase Agreement was due to significant regulatory challenges that prevented the completion of the transaction....\n",
            "\n",
            "3. Q: What acquisition termination cost did NVIDIA record in fiscal year 2023?\n",
            "   Category: financial\n",
            "   A: NVIDIA recorded an acquisition termination cost of $1.35 billion in fiscal year 2023, reflecting the write-off of the prepayment provided at signing f...\n"
          ]
        }
      ],
      "source": [
        "# Test FAQ generation on a single chunk\n",
        "print(\"Testing FAQ generation on sample chunk...\\n\")\n",
        "test_faqs = faq_chain.invoke({\"doc_content\": chunks[10].page_content})\n",
        "\n",
        "print(f\"Generated {len(test_faqs.get('faqs', []))} FAQs:\")\n",
        "for i, faq in enumerate(test_faqs.get('faqs', [])[:3], 1):\n",
        "    print(f\"\\n{i}. Q: {faq['question']}\")\n",
        "    print(f\"   Category: {faq['category']}\")\n",
        "    print(f\"   A: {faq['answer'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating FAQs from document chunks...\n",
            "\n",
            "Processing chunk 1/25...\n",
            "11:14:36 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:14:45 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:14:54 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:15:00 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:15:07 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Processing chunk 6/25...\n",
            "11:15:14 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:15:16 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:15:22 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:15:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:15:35 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Processing chunk 11/25...\n",
            "11:15:42 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:15:49 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:15:55 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:16:03 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:16:11 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Processing chunk 16/25...\n",
            "11:16:20 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:16:26 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:16:35 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:16:41 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:16:48 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Processing chunk 21/25...\n",
            "11:16:55 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:17:01 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:17:07 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:17:15 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "11:17:23 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\n",
            "✓ Generated 114 FAQs total\n",
            "\n",
            "Category distribution:\n",
            "  technology: 32\n",
            "  operations: 26\n",
            "  financial: 22\n",
            "  products: 21\n",
            "  general: 13\n"
          ]
        }
      ],
      "source": [
        "# Generate FAQs from all chunks (limited to first 25 for demo purposes)\n",
        "def extract_faqs_from_chunks(chunks: List[Any], max_chunks: int = 25) -> List[Dict]:\n",
        "    \"\"\"Extract FAQs from document chunks using LLM.\"\"\"\n",
        "    all_faqs = []\n",
        "    \n",
        "    for i, chunk in enumerate(chunks[:max_chunks]):\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Processing chunk {i+1}/{min(len(chunks), max_chunks)}...\", flush=True)\n",
        "        \n",
        "        try:\n",
        "            result = faq_chain.invoke({\"doc_content\": chunk.page_content})\n",
        "            if result and result.get(\"faqs\"):\n",
        "                all_faqs.extend(result[\"faqs\"])\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Skipped chunk {i+1} due to error: {str(e)[:100]}\")\n",
        "            continue\n",
        "    \n",
        "    return all_faqs\n",
        "\n",
        "# Extract FAQs\n",
        "print(\"\\nGenerating FAQs from document chunks...\\n\")\n",
        "faqs = extract_faqs_from_chunks(chunks, max_chunks=25)\n",
        "\n",
        "print(f\"\\nGenerated {len(faqs)} FAQs total\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "categories = {}\n",
        "for faq in faqs:\n",
        "    cat = faq.get('category', 'unknown')\n",
        "    categories[cat] = categories.get(cat, 0) + 1\n",
        "for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  {cat}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Test/Evaluation Dataset\n",
        "\n",
        "We'll create a test dataset with:\n",
        "- **Positive examples**: Questions that should match cached FAQs\n",
        "- **Negative examples**: Questions that should NOT match cached FAQs\n",
        "- **Edge cases**: Slightly different phrasings to test threshold sensitivity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample FAQs for testing:\n",
            "\n",
            "1. What is the fiscal year end date for NVIDIA Corporation as reported in the 10-K?...\n",
            "\n",
            "2. What is the trading symbol for NVIDIA Corporation's common stock?...\n",
            "\n",
            "3. Where is the principal executive office of NVIDIA Corporation located?...\n"
          ]
        }
      ],
      "source": [
        "# Select representative FAQs for test set\n",
        "sample_faqs = faqs[:10]  # Take first 10 FAQs\n",
        "\n",
        "print(\"Sample FAQs for testing:\")\n",
        "for i, faq in enumerate(sample_faqs[:3], 1):\n",
        "    print(f\"\\n{i}. {faq['question'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Test dataset created\n",
            "  Negative examples: 5\n"
          ]
        }
      ],
      "source": [
        "# Create test dataset with negative examples (off-topic questions)\n",
        "negative_examples = [\n",
        "    {\"query\": \"What is the weather today?\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "    {\"query\": \"How do I cook pasta?\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "    {\"query\": \"What is the capital of France?\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "    {\"query\": \"Tell me a joke\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "    {\"query\": \"What time is it?\", \"expected_match\": False, \"category\": \"off-topic\"},\n",
        "]\n",
        "\n",
        "print(f\"Test dataset created\")\n",
        "print(f\"  Negative examples: {len(negative_examples)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pre-Load Semantic Cache with FAQs\n",
        "\n",
        "Now we'll populate the cache instance with our generated FAQs. We'll use the `store()` API with metadata tags for filtering and organization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Cache cleared\n"
          ]
        }
      ],
      "source": [
        "# Clear any existing cache entries\n",
        "cache.clear()\n",
        "print(\"Cache cleared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Storing FAQs in cache...\n",
            "\n",
            "  Stored 0/114 FAQs...\n",
            "  Stored 20/114 FAQs...\n",
            "  Stored 40/114 FAQs...\n",
            "  Stored 60/114 FAQs...\n",
            "  Stored 80/114 FAQs...\n",
            "  Stored 100/114 FAQs...\n",
            "\n",
            "✓ Stored 114 FAQs in cache\n",
            "  Cache index: rag_faq_cache\n",
            "\n",
            "Example cache entries:\n",
            "\n",
            "1. Key: rag_faq_cache:abd9b974d9eedebc62332adbfd10ab5ff96e9d65dbd4476a27a487dd82b46002\n",
            "   Q: What is the fiscal year end date for NVIDIA Corporation as reported in the 10-K?...\n",
            "\n",
            "2. Key: rag_faq_cache:8aa719b5f3d105fdcd9048d2b6c14e04bd60e8501a27ed80481c97adafb01ea7\n",
            "   Q: What is the trading symbol for NVIDIA Corporation's common stock?...\n"
          ]
        }
      ],
      "source": [
        "# Store FAQs in cache with metadata tags\n",
        "print(\"Storing FAQs in cache...\\n\")\n",
        "\n",
        "stored_count = 0\n",
        "cache_keys = {}  # Map questions to their cache keys\n",
        "\n",
        "for i, faq in enumerate(faqs):\n",
        "    if i % 20 == 0:\n",
        "        print(f\"  Stored {i}/{len(faqs)} FAQs...\", flush=True)\n",
        "    \n",
        "    try:\n",
        "        # Store with metadata - note that metadata is stored but not used for filtering in basic SemanticCache\n",
        "        # In production, you can use this for analytics and tracking\n",
        "        key = cache.store(\n",
        "            prompt=faq['question'],\n",
        "            response=faq['answer']\n",
        "        )\n",
        "        cache_keys[faq['question']] = key\n",
        "        stored_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Failed to store FAQ {i+1}: {str(e)[:100]}\")\n",
        "\n",
        "print(f\"\\nStored {stored_count} FAQs in cache\")\n",
        "print(f\"  Cache index: {cache.index.name}\")\n",
        "print(f\"\\nExample cache entries:\")\n",
        "for i, (q, k) in enumerate(list(cache_keys.items())[:2], 1):\n",
        "    print(f\"\\n{i}. Key: {k}\")\n",
        "    print(f\"   Q: {q[:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Cache Retrieval with Different Strategies\n",
        "\n",
        "Let's test how the cache performs with different types of queries and matching thresholds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Exact Match Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing exact match queries:\n",
            "\n",
            "1. ✓ Cache HIT\n",
            "   Query: What is the fiscal year end date for NVIDIA Corporation as reported in the 10-K?...\n",
            "   Answer: The fiscal year ended January 29, 2023....\n",
            "\n",
            "2. ✓ Cache HIT\n",
            "   Query: What is the trading symbol for NVIDIA Corporation's common stock?...\n",
            "   Answer: The trading symbol for NVIDIA Corporation's common stock is NVDA....\n",
            "\n",
            "3. ✓ Cache HIT\n",
            "   Query: Where is the principal executive office of NVIDIA Corporation located?...\n",
            "   Answer: The principal executive office of NVIDIA Corporation is located at 2788 San Tomas Expressway, Santa ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with exact questions from cache\n",
        "print(\"Testing exact match queries:\\n\")\n",
        "\n",
        "for i, faq in enumerate(faqs[:3], 1):\n",
        "    result = cache.check(prompt=faq['question'])\n",
        "    \n",
        "    if result:\n",
        "        print(f\"{i}. Cache HIT\")\n",
        "        print(f\"   Query: {faq['question'][:80]}...\")\n",
        "        print(f\"   Answer: {result[0]['response'][:100]}...\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. ✗ Cache MISS\")\n",
        "        print(f\"   Query: {faq['question'][:80]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Semantic Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing semantic similarity:\n",
            "\n",
            "1. ✗ Cache MISS\n",
            "   Query: Tell me about NVIDIA's revenue\n",
            "\n",
            "2. ✗ Cache MISS\n",
            "   Query: What products does the company make?\n",
            "\n",
            "3. ✗ Cache MISS\n",
            "   Query: How is the company performing financially?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with semantically similar queries\n",
        "print(\"Testing semantic similarity:\\n\")\n",
        "\n",
        "similar_queries = [\n",
        "    \"Tell me about NVIDIA's revenue\",\n",
        "    \"What products does the company make?\",\n",
        "    \"How is the company performing financially?\",\n",
        "]\n",
        "\n",
        "for i, query in enumerate(similar_queries, 1):\n",
        "    result = cache.check(prompt=query, return_fields=[\"prompt\", \"response\", \"distance\"])\n",
        "    \n",
        "    if result:\n",
        "        print(f\"{i}. Cache HIT (distance: {result[0].get('vector_distance', 'N/A'):.4f})\")\n",
        "        print(f\"   Query: {query}\")\n",
        "        print(f\"   Matched: {result[0]['prompt'][:80]}...\")\n",
        "        print(f\"   Answer: {result[0]['response'][:100]}...\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. ✗ Cache MISS\")\n",
        "        print(f\"   Query: {query}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Cache with Sample Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing query: 'What is NVIDIA's main business?'\n",
            "Current threshold: 0.1500\n",
            "\n",
            "✗ Cache MISS - No match found within threshold\n"
          ]
        }
      ],
      "source": [
        "# Test cache behavior with a sample query\n",
        "test_query = \"What is NVIDIA's main business?\"\n",
        "\n",
        "print(f\"Testing query: '{test_query}'\")\n",
        "print(f\"Current threshold: {cache.distance_threshold:.4f}\\n\")\n",
        "\n",
        "result = cache.check(prompt=test_query, return_fields=[\"prompt\", \"vector_distance\"])\n",
        "\n",
        "if result:\n",
        "    print(f\"Cache HIT\")\n",
        "    print(f\"  Distance: {result[0].get('vector_distance', 0):.6f}\")\n",
        "    print(f\"  Matched: {result[0]['prompt'][:80]}...\")\n",
        "else:\n",
        "    print(f\"✗ Cache MISS - No match found within threshold\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Negative Examples (Should Not Match)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing negative examples (should NOT match):\n",
            "\n",
            "1. ✓ Correct MISS\n",
            "   Query: What is the weather today?\n",
            "\n",
            "2. ✓ Correct MISS\n",
            "   Query: How do I cook pasta?\n",
            "\n",
            "3. ✓ Correct MISS\n",
            "   Query: What is the capital of France?\n",
            "\n",
            "4. ✓ Correct MISS\n",
            "   Query: Tell me a joke\n",
            "\n",
            "5. ✓ Correct MISS\n",
            "   Query: What time is it?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with off-topic queries that should NOT match\n",
        "print(\"Testing negative examples (should NOT match):\\n\")\n",
        "\n",
        "for i, test_case in enumerate(negative_examples, 1):\n",
        "    result = cache.check(prompt=test_case['query'], return_fields=[\"prompt\", \"vector_distance\"])\n",
        "    \n",
        "    if result:\n",
        "        print(f\"{i}. ⚠️  UNEXPECTED HIT (distance: {result[0].get('vector_distance', 'N/A'):.4f})\")\n",
        "        print(f\"   Query: {test_case['query']}\")\n",
        "        print(f\"   Matched: {result[0]['prompt'][:80]}...\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. Correct MISS\")\n",
        "        print(f\"   Query: {test_case['query']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Optimize Cache Threshold\n",
        "\n",
        "Using the `CacheThresholdOptimizer`, we can automatically find the optimal distance threshold based on our test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Created optimization test data:\n",
            "  Total examples: 10\n",
            "  Positive (should match): 5\n",
            "  Negative (should not match): 5\n"
          ]
        }
      ],
      "source": [
        "# Create optimization test data\n",
        "# Format: [{\"query\": \"...\", \"query_match\": \"cache_key_or_empty_string\"}, ...]\n",
        "\n",
        "optimization_test_data = []\n",
        "\n",
        "# Add positive examples (should match specific cache entries)\n",
        "for faq in faqs[:5]:\n",
        "    if faq['question'] in cache_keys:\n",
        "        optimization_test_data.append({\n",
        "            \"query\": faq['question'],\n",
        "            \"query_match\": cache_keys[faq['question']]\n",
        "        })\n",
        "\n",
        "# Add negative examples (should not match anything)\n",
        "for neg_example in negative_examples:\n",
        "    optimization_test_data.append({\n",
        "        \"query\": neg_example['query'],\n",
        "        \"query_match\": \"\"  # Empty string means it should NOT match\n",
        "    })\n",
        "\n",
        "print(f\"Created optimization test data:\")\n",
        "print(f\"  Total examples: {len(optimization_test_data)}\")\n",
        "print(f\"  Positive (should match): {sum(1 for x in optimization_test_data if x['query_match'])}\")\n",
        "print(f\"  Negative (should not match): {sum(1 for x in optimization_test_data if not x['query_match'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Current distance threshold: 0.01\n",
            "\n",
            "Optimizing threshold...\n",
            "\n",
            "\n",
            "✓ Optimization complete!\n",
            "  Original threshold: 0.15\n",
            "  Optimized threshold: 0.010000\n"
          ]
        }
      ],
      "source": [
        "# Optimize threshold based on F1 score\n",
        "print(f\"\\nCurrent distance threshold: {cache.distance_threshold}\")\n",
        "print(\"\\nOptimizing threshold...\\n\")\n",
        "\n",
        "optimizer = CacheThresholdOptimizer(\n",
        "    cache,\n",
        "    optimization_test_data,\n",
        "    eval_metric=\"f1\"  # Can also use \"precision\" or \"recall\"\n",
        ")\n",
        "\n",
        "results = optimizer.optimize()\n",
        "\n",
        "print(f\"\\nOptimization complete!\")\n",
        "print(f\"  Original threshold: 0.15\")\n",
        "print(f\"  Optimized threshold: {cache.distance_threshold:.6f}\")\n",
        "if results and 'f1' in results:\n",
        "    print(f\"  F1 Score: {results['f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Re-testing negative examples with optimized threshold:\n",
            "\n",
            "1. ✓ MISS (correct)\n",
            "   Query: What is the weather today?\n",
            "\n",
            "2. ✓ MISS (correct)\n",
            "   Query: How do I cook pasta?\n",
            "\n",
            "3. ✓ MISS (correct)\n",
            "   Query: What is the capital of France?\n",
            "\n",
            "4. ✓ MISS (correct)\n",
            "   Query: Tell me a joke\n",
            "\n",
            "5. ✓ MISS (correct)\n",
            "   Query: What time is it?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Re-test with optimized threshold\n",
        "print(\"\\nRe-testing negative examples with optimized threshold:\\n\")\n",
        "\n",
        "for i, test_case in enumerate(negative_examples, 1):\n",
        "    result = cache.check(prompt=test_case['query'], return_fields=[\"prompt\", \"vector_distance\"])\n",
        "    \n",
        "    if result:\n",
        "        print(f\"{i}. ⚠️  HIT (distance: {result[0].get('vector_distance', 'N/A'):.4f})\")\n",
        "        print(f\"   Query: {test_case['query']}\")\n",
        "        print(f\"   Matched: {result[0]['prompt'][:80]}...\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. MISS (correct)\")\n",
        "        print(f\"   Query: {test_case['query']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. LangCache-Embed Model Deep Dive\n",
        "\n",
        "The `redis/langcache-embed-v1` model is specifically optimized for semantic caching. Let's examine its characteristics and performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangCache-Embed Model Information:\n",
            "============================================================\n",
            "Model: redis/langcache-embed-v1\n",
            "Purpose: Optimized for semantic caching tasks\n",
            "Dimension: 768\n",
            "Distance Metric: cosine\n",
            "\n",
            "Key Features:\n",
            "  • Trained specifically on query-response pairs\n",
            "  • Balanced precision/recall for optimal cache hit rates\n",
            "  • Fast inference time suitable for production\n",
            "  • Optimized for short-form text (queries/prompts)\n",
            "\n",
            "Advantages for Caching:\n",
            "  • Better semantic understanding of query intent\n",
            "  • More robust to paraphrasing and rewording\n",
            "  • Lower false positive rate compared to general embeddings\n",
            "  • Optimized threshold ranges for cache decisions\n"
          ]
        }
      ],
      "source": [
        "# Show information about the langcache-embed model\n",
        "print(\"LangCache-Embed Model Information:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: redis/langcache-embed-v1\")\n",
        "print(f\"Purpose: Optimized for semantic caching tasks\")\n",
        "print(f\"Dimension: 768\")\n",
        "print(f\"Distance Metric: cosine\")\n",
        "print(\"\\nKey Features:\")\n",
        "print(\"  • Trained specifically on query-response pairs\")\n",
        "print(\"  • Balanced precision/recall for optimal cache hit rates\")\n",
        "print(\"  • Fast inference time suitable for production\")\n",
        "print(\"  • Optimized for short-form text (queries/prompts)\")\n",
        "print(\"\\nAdvantages for Caching:\")\n",
        "print(\"  • Better semantic understanding of query intent\")\n",
        "print(\"  • More robust to paraphrasing and rewording\")\n",
        "print(\"  • Lower false positive rate compared to general embeddings\")\n",
        "print(\"  • Optimized threshold ranges for cache decisions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparing semantic similarities:\n",
            "\n",
            "Base question: What is NVIDIA's revenue?\n",
            "\n",
            "Query: Tell me about NVIDIA's earnings\n",
            "  Similarity: 0.8725\n",
            "  Distance: 0.1275\n",
            "  Would cache hit (threshold=0.0100)? False\n",
            "\n",
            "Query: How much money does NVIDIA make?\n",
            "  Similarity: 0.9004\n",
            "  Distance: 0.0996\n",
            "  Would cache hit (threshold=0.0100)? False\n",
            "\n",
            "Query: What is the weather today?\n",
            "  Similarity: 0.3141\n",
            "  Distance: 0.6859\n",
            "  Would cache hit (threshold=0.0100)? False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare semantic similarities between related and unrelated questions\n",
        "test_questions = [\n",
        "    \"What is NVIDIA's revenue?\",\n",
        "    \"Tell me about NVIDIA's earnings\",  # Semantically similar\n",
        "    \"How much money does NVIDIA make?\",  # Semantically similar\n",
        "    \"What is the weather today?\",  # Unrelated\n",
        "]\n",
        "\n",
        "print(\"\\nComparing semantic similarities:\\n\")\n",
        "print(f\"Base question: {test_questions[0]}\\n\")\n",
        "\n",
        "base_embedding = vectorizer.embed(test_questions[0])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for query in test_questions[1:]:\n",
        "    query_embedding = vectorizer.embed(query)\n",
        "    \n",
        "    # Calculate cosine similarity\n",
        "    similarity = np.dot(base_embedding, query_embedding) / (\n",
        "        np.linalg.norm(base_embedding) * np.linalg.norm(query_embedding)\n",
        "    )\n",
        "    distance = 1 - similarity\n",
        "    \n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"  Similarity: {similarity:.4f}\")\n",
        "    print(f\"  Distance: {distance:.4f}\")\n",
        "    print(f\"  Would cache hit (threshold={cache.distance_threshold:.4f})? {distance < cache.distance_threshold}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. RAG Pipeline Integration\n",
        "\n",
        "Now let's integrate the semantic cache into a complete RAG pipeline and measure the performance improvements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Simple RAG Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ RAG chain created\n"
          ]
        }
      ],
      "source": [
        "# Create a simple RAG prompt template\n",
        "rag_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant answering questions about NVIDIA based on their 10-K filing. Provide accurate, concise answers.\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Create RAG chain\n",
        "rag_chain = rag_template | llm\n",
        "\n",
        "print(\"RAG chain created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Cached RAG Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Cached RAG function ready\n"
          ]
        }
      ],
      "source": [
        "def rag_with_cache(question: str, use_cache: bool = True) -> tuple:\n",
        "    \"\"\"\n",
        "    Process a question through RAG pipeline with optional semantic caching.\n",
        "    \n",
        "    Returns: (answer, cache_hit, response_time)\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    cache_hit = False\n",
        "    \n",
        "    # Check cache first if enabled\n",
        "    if use_cache:\n",
        "        cached_result = cache.check(prompt=question)\n",
        "        if cached_result:\n",
        "            answer = cached_result[0]['response']\n",
        "            cache_hit = True\n",
        "            response_time = time.time() - start_time\n",
        "            return answer, cache_hit, response_time\n",
        "    \n",
        "    # Cache miss - use LLM\n",
        "    answer = rag_chain.invoke({\"question\": question})\n",
        "    response_time = time.time() - start_time\n",
        "    \n",
        "    # Store in cache for future use\n",
        "    if use_cache and hasattr(answer, 'content'):\n",
        "        cache.store(prompt=question, response=answer.content)\n",
        "    elif use_cache:\n",
        "        cache.store(prompt=question, response=str(answer))\n",
        "    \n",
        "    return answer.content if hasattr(answer, 'content') else str(answer), cache_hit, response_time\n",
        "\n",
        "print(\"Cached RAG function ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Comparison: With vs Without Cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PERFORMANCE COMPARISON: With Cache vs Without Cache\n",
            "================================================================================\n",
            "\n",
            "[FIRST PASS - Populating Cache]\n",
            "\n",
            "15:52:18 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "1. What is NVIDIA's primary business?\n",
            "   Cache: MISS | Time: 2.232s\n",
            "   Answer: NVIDIA's primary business is the design and manufacture of graphics processing units (GPUs) for gami...\n",
            "\n",
            "15:52:20 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2. How much revenue did NVIDIA generate?\n",
            "   Cache: MISS | Time: 2.188s\n",
            "   Answer: As of the latest 10-K filing, NVIDIA reported total revenue of $26.91 billion for the fiscal year en...\n",
            "\n",
            "15:52:23 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "3. What are NVIDIA's main products?\n",
            "   Cache: MISS | Time: 3.195s\n",
            "   Answer: NVIDIA's main products include:\n",
            "\n",
            "1. **Graphics Processing Units (GPUs)**: Primarily for gaming, prof...\n",
            "\n",
            "\n",
            "[SECOND PASS - Cache Hits with Paraphrased Questions]\n",
            "\n",
            "15:52:25 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "1. What does NVIDIA do as a business?\n",
            "   Cache: MISS ✗ | Time: 2.069s\n",
            "   Answer: NVIDIA is primarily a technology company that designs and manufactures graphics processing units (GP...\n",
            "\n",
            "15:52:28 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2. Can you tell me NVIDIA's revenue figures?\n",
            "   Cache: MISS ✗ | Time: 2.518s\n",
            "   Answer: As of the latest 10-K filing, NVIDIA reported total revenue of $26.91 billion for the fiscal year en...\n",
            "\n",
            "15:52:34 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "3. What products does NVIDIA sell?\n",
            "   Cache: MISS ✗ | Time: 5.687s\n",
            "   Answer: NVIDIA sells a variety of products primarily focused on graphics processing units (GPUs) and related...\n",
            "\n",
            "\n",
            "[THIRD PASS - Without Cache (Baseline)]\n",
            "\n",
            "15:52:36 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "1. What is NVIDIA's primary business?\n",
            "   Cache: DISABLED | Time: 1.807s\n",
            "\n",
            "15:52:38 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2. How much revenue did NVIDIA generate?\n",
            "   Cache: DISABLED | Time: 2.035s\n",
            "\n",
            "15:52:41 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "3. What are NVIDIA's main products?\n",
            "   Cache: DISABLED | Time: 3.880s\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PERFORMANCE SUMMARY\n",
            "================================================================================\n",
            "Average time - First pass (cache miss):  2.538s\n",
            "Average time - Second pass (cache hit):  3.424s\n",
            "Average time - Without cache:            2.574s\n",
            "\n",
            "✓ Speedup with cache: 0.7x faster\n",
            "  Cache hit rate: 0%\n"
          ]
        }
      ],
      "source": [
        "# Test questions for RAG evaluation\n",
        "test_questions_rag = [\n",
        "    \"What is NVIDIA's primary business?\",\n",
        "    \"How much revenue did NVIDIA generate?\",\n",
        "    \"What are NVIDIA's main products?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE COMPARISON: With Cache vs Without Cache\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# First pass - populate cache (cache misses, must call LLM)\n",
        "print(\"\\n[FIRST PASS - Populating Cache]\\n\")\n",
        "first_pass_times = []\n",
        "\n",
        "for i, question in enumerate(test_questions_rag, 1):\n",
        "    answer, cache_hit, response_time = rag_with_cache(question, use_cache=True)\n",
        "    first_pass_times.append(response_time)\n",
        "    print(f\"{i}. {question}\")\n",
        "    print(f\"   Cache: {'HIT' if cache_hit else 'MISS'} | Time: {response_time:.3f}s\")\n",
        "    print(f\"   Answer: {answer[:100]}...\\n\")\n",
        "\n",
        "# Second pass - test cache hits with similar questions\n",
        "print(\"\\n[SECOND PASS - Cache Hits with Paraphrased Questions]\\n\")\n",
        "second_pass_times = []\n",
        "\n",
        "similar_questions = [\n",
        "    \"What does NVIDIA do as a business?\",\n",
        "    \"Can you tell me NVIDIA's revenue figures?\",\n",
        "    \"What products does NVIDIA sell?\",\n",
        "]\n",
        "\n",
        "for i, question in enumerate(similar_questions, 1):\n",
        "    answer, cache_hit, response_time = rag_with_cache(question, use_cache=True)\n",
        "    second_pass_times.append(response_time)\n",
        "    print(f\"{i}. {question}\")\n",
        "    print(f\"   Cache: {'HIT ✓' if cache_hit else 'MISS ✗'} | Time: {response_time:.3f}s\")\n",
        "    print(f\"   Answer: {answer[:100]}...\\n\")\n",
        "\n",
        "# Third pass - without cache (baseline)\n",
        "print(\"\\n[THIRD PASS - Without Cache (Baseline)]\\n\")\n",
        "no_cache_times = []\n",
        "\n",
        "for i, question in enumerate(test_questions_rag, 1):\n",
        "    answer, _, response_time = rag_with_cache(question, use_cache=False)\n",
        "    no_cache_times.append(response_time)\n",
        "    print(f\"{i}. {question}\")\n",
        "    print(f\"   Cache: DISABLED | Time: {response_time:.3f}s\\n\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "avg_first = sum(first_pass_times)/len(first_pass_times)\n",
        "avg_second = sum(second_pass_times)/len(second_pass_times)\n",
        "avg_no_cache = sum(no_cache_times)/len(no_cache_times)\n",
        "\n",
        "print(f\"Average time - First pass (cache miss):  {avg_first:.3f}s\")\n",
        "print(f\"Average time - Second pass (cache hit):  {avg_second:.3f}s\")\n",
        "print(f\"Average time - Without cache:            {avg_no_cache:.3f}s\")\n",
        "\n",
        "if avg_second > 0:\n",
        "    speedup = avg_first / avg_second\n",
        "    print(f\"\\nSpeedup with cache: {speedup:.1f}x faster\")\n",
        "\n",
        "cache_hit_count = sum(1 for i, _ in enumerate(similar_questions) if second_pass_times[i] < 0.1)\n",
        "cache_hit_rate = cache_hit_count / len(similar_questions)\n",
        "print(f\"  Cache hit rate: {cache_hit_rate*100:.0f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cost Savings Analysis\n",
        "\n",
        "Let's estimate the potential cost savings from implementing semantic caching in a production environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COST SAVINGS ESTIMATE (10,000 queries/day @ 70% hit rate)\n",
            "================================================================================\n",
            "Cost without cache: $1.2000/day\n",
            "Cost with cache:    $0.367000/day\n",
            "\n",
            "Daily savings:   $0.8330 (69.4% reduction)\n",
            "Monthly savings: $24.99\n",
            "Annual savings:  $304.04\n"
          ]
        }
      ],
      "source": [
        "# Estimate cost savings\n",
        "# Assumptions: GPT-4o-mini costs ~$0.15 per 1M input tokens, $0.60 per 1M output tokens\n",
        "# Average request: ~200 input tokens, ~150 output tokens\n",
        "cost_per_llm_call = (200 * 0.15 / 1_000_000) + (150 * 0.60 / 1_000_000)  # USD\n",
        "cost_per_cache_check = 0.000001  # Negligible (Redis query)\n",
        "\n",
        "total_queries = 10000  # Simulate 10K queries per day\n",
        "cache_hit_rate = 0.70  # Assume 70% hit rate in production\n",
        "\n",
        "# Without cache\n",
        "cost_without_cache = total_queries * cost_per_llm_call\n",
        "\n",
        "# With cache\n",
        "cache_hits = total_queries * cache_hit_rate\n",
        "cache_misses = total_queries * (1 - cache_hit_rate)\n",
        "cost_with_cache = (cache_hits * cost_per_cache_check) + (cache_misses * cost_per_llm_call)\n",
        "\n",
        "savings = cost_without_cache - cost_with_cache\n",
        "savings_percent = (savings / cost_without_cache) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"COST SAVINGS ESTIMATE ({total_queries:,} queries/day @ {int(cache_hit_rate*100)}% hit rate)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Cost without cache: ${cost_without_cache:.4f}/day\")\n",
        "print(f\"Cost with cache:    ${cost_with_cache:.6f}/day\")\n",
        "print(f\"\\nDaily savings:   ${savings:.4f} ({savings_percent:.1f}% reduction)\")\n",
        "print(f\"Monthly savings: ${savings * 30:.2f}\")\n",
        "print(f\"Annual savings:  ${savings * 365:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Cache Analytics and Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "CACHE STATISTICS\n",
            "================================================================================\n",
            "\n",
            "Cache Name: rag_faq_cache\n",
            "Total cached entries: 120\n",
            "Distance threshold: 0.010000\n",
            "Vectorizer model: redis/langcache-embed-v1\n",
            "Embedding dimensions: 768\n"
          ]
        }
      ],
      "source": [
        "# Get cache statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CACHE STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Count total entries\n",
        "info = cache.index.info()\n",
        "print(f\"\\nCache Name: {cache.index.name}\")\n",
        "print(f\"Total cached entries: {info.get('num_docs', 'N/A')}\")\n",
        "print(f\"Distance threshold: {cache.distance_threshold:.6f}\")\n",
        "print(f\"Vectorizer model: redis/langcache-embed-v1\")\n",
        "print(f\"Embedding dimensions: 768\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Best Practices and Tips\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Threshold Optimization**: Start conservative (0.10-0.15) and optimize based on real usage data\n",
        "2. **Doc2Cache**: Pre-populate your cache with high-quality FAQs for immediate benefits\n",
        "3. **Monitoring**: Track cache hit rates and adjust thresholds as user patterns emerge\n",
        "4. **Model Selection**: The `langcache-embed-v1` model is specifically optimized for caching tasks\n",
        "5. **Cost-Performance Balance**: Even a 50% cache hit rate provides significant cost savings\n",
        "\n",
        "### When to Use Semantic Caching\n",
        "\n",
        "✅ **Good Use Cases:**\n",
        "- High-traffic applications with repeated question patterns\n",
        "- Customer support chatbots\n",
        "- FAQ systems\n",
        "- Documentation Q&A\n",
        "- Product information queries\n",
        "- Educational content Q&A\n",
        "\n",
        "❌ **Less Suitable:**\n",
        "- Highly dynamic content requiring real-time data\n",
        "- Creative writing tasks needing variety\n",
        "- Personalized responses based on user-specific context\n",
        "- Time-sensitive queries (use TTL if needed)\n",
        "\n",
        "### Performance Tips\n",
        "\n",
        "1. **Batch Loading**: Pre-populate cache with Doc2Cache for immediate value\n",
        "2. **Monitor Hit Rates**: Track and adjust thresholds based on production metrics\n",
        "3. **A/B Testing**: Test different thresholds with a subset of traffic\n",
        "4. **Cache Warming**: Regularly update cache with trending topics\n",
        "5. **TTL Management**: Set time-to-live for entries that may become stale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Cleanup\n",
        "\n",
        "Clean up resources when done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Cache cleared (index preserved)\n"
          ]
        }
      ],
      "source": [
        "# Clear cache contents\n",
        "cache.delete()\n",
        "print(\"Cache deleted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You've completed this comprehensive guide on semantic caching with LangCache and RedisVL. \n",
        "\n",
        "**What You've Learned:**\n",
        "- ✅ Set up and configure LangCache with Redis Cloud\n",
        "- ✅ Load and process PDF documents into knowledge bases\n",
        "- ✅ Generate FAQs using the Doc2Cache technique with LLMs\n",
        "- ✅ Pre-populate a semantic cache with tagged entries\n",
        "- ✅ Test different cache matching strategies and thresholds\n",
        "- ✅ Optimize cache performance using test datasets\n",
        "- ✅ Leverage the `redis/langcache-embed-v1` cross-encoder model\n",
        "- ✅ Integrate semantic caching into RAG pipelines\n",
        "- ✅ Measure performance improvements and cost savings\n",
        "\n",
        "**Next Steps:**\n",
        "- Experiment with different distance thresholds for your use case\n",
        "- Try other embedding models and compare performance\n",
        "- Implement cache analytics and monitoring in production\n",
        "- Explore advanced features like TTL, metadata filtering, and cache warming strategies\n",
        "- Scale your semantic cache to handle production traffic\n",
        "\n",
        "**Resources:**\n",
        "- [RedisVL Documentation](https://redis.io/docs/stack/search/redisvl/)\n",
        "- [LangCache Sign Up](https://langcache.io/signup)\n",
        "- [Redis AI Resources](https://github.com/redis-developer/redis-ai-resources)\n",
        "- [Semantic Caching Paper](https://arxiv.org/abs/2504.02268)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "redis-ai-res",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

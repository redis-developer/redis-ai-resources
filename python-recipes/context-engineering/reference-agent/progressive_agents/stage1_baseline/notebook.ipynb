#%% md
![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)

# Stage 1: Baseline RAG Agent

## Learning Through Progressive Improvement

Welcome to the **Progressive RAG Agents** learning system! Instead of showing you a perfect, production-ready agent all at once, we'll build it in three stages:

1. **Stage 1 (This Notebook)** - Baseline RAG that works but is inefficient
2. **Stage 2** - Apply context engineering for 50% improvement
3. **Stage 3** - Add production patterns for 70% total improvement

**Why start with "bad" code?** Because you'll learn more by seeing the problems firsthand, then fixing them step-by-step. This mirrors real-world development where you start simple, measure, then optimize.

## What You'll Build

A basic RAG agent that:
- ‚úÖ Retrieves relevant courses using semantic search
- ‚úÖ Formats context as raw JSON (all fields included)
- ‚úÖ Generates accurate responses
- ‚ùå Uses ~2,500 tokens (inefficient)
- ‚ùå Costs $6.25 per 1,000 queries

**Time to complete:** 20-25 minutes

---
#%% md
## Architecture Overview

### LangGraph Workflow

This agent uses **LangGraph StateGraph** for workflow orchestration.

```mermaid
graph TB
    START([START]) --> A[retrieve_courses]
    A --> B[format_context]
    B --> C[generate_response]
    C --> END([END])

    A -->|"State: messages, query"| A_STATE[AgentState]
    B -->|"State: retrieved_courses"| A_STATE
    C -->|"State: context"| A_STATE

    style A fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px
    style B fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px
    style C fill:#4ecdc4,stroke:#1098ad,stroke-width:2px
    style A_STATE fill:#fff3bf,stroke:#ffd43b,stroke-width:2px
```

**Node Descriptions:**
- **retrieve_courses** (red): Semantic search - retrieves 5 courses
- **format_context** (red): Raw JSON formatting - INEFFICIENT (~2,500 tokens)
- **generate_response** (blue): LLM generates answer
- **AgentState** (yellow): Shared state across nodes

**Linear flow:**
```
START ‚Üí retrieve_courses ‚Üí format_context ‚Üí generate_response ‚Üí END
```

### Why LangGraph?

Even for this simple baseline, LangGraph provides benefits:

‚úÖ **Clear separation of concerns** - Each step is a distinct node
‚úÖ **State management** - AgentState tracks data flow between nodes
‚úÖ **Debuggability** - Easy to inspect state at each step
‚úÖ **Extensibility** - Easy to add nodes or modify flow in later stages

**Key Characteristics:**
- ‚úÖ Works correctly
- ‚úÖ Uses LangGraph for workflow orchestration
- ‚ùå Inefficient (raw JSON with all fields)
- ‚ùå Poor formatting (JSON harder for LLM to parse)
- ‚ùå No optimization

---
#%% md
## Setup and Environment

Let's prepare our environment.
#%%
import json
import os
import sys
from pathlib import Path

import tiktoken
from dotenv import load_dotenv

# Add reference-agent to path
sys.path.insert(0, str(Path.cwd().parent.parent))

# Load environment variables
load_dotenv()

# Verify required environment variables
required_vars = ["OPENAI_API_KEY"]
missing_vars = [var for var in required_vars if not os.getenv(var)]

if missing_vars:
    print(f"""
‚ö†Ô∏è  Missing required environment variables: {', '.join(missing_vars)}

Please create a .env file with:
OPENAI_API_KEY=your_openai_api_key
REDIS_URL=redis://localhost:6379
""")
    sys.exit(1)

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
print("‚úÖ Environment variables loaded")
print(f"   REDIS_URL: {REDIS_URL}")
print(f"   OPENAI_API_KEY: {'‚úì Set' if os.getenv('OPENAI_API_KEY') else '‚úó Not set'}")
#%%
# Utility: Token counter
def count_tokens(text: str, model: str = "gpt-4o") -> int:
    """Count tokens in text using tiktoken."""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

print("‚úÖ Utility functions loaded")
#%% md
---

## Step 1: Verify Course Data

Before building our agent, let's verify that course data is available in Redis.
#%%
from redis_context_course import CourseManager, redis_config

# Check Redis connection
if not redis_config.health_check():
    print("""
‚ùå Redis connection failed!

Please ensure Redis is running:
- Local: docker run -d -p 6379:6379 redis/redis-stack-server:latest
- Or check REDIS_URL in .env file
""")
    sys.exit(1)

print("‚úÖ Redis connection successful")

# Initialize course manager
course_manager = CourseManager()
print("‚úÖ CourseManager initialized")
#%%
# Test retrieval
test_courses = await course_manager.search_courses("machine learning", limit=3)

print(f"\n‚úÖ Retrieved {len(test_courses)} courses")
print("\nSample courses:")
for course in test_courses[:2]:
    print(f"  - {course.course_code}: {course.title}")

if len(test_courses) == 0:
    print("""
‚ö†Ô∏è  No courses found in Redis!

Please generate and ingest course data:
1. cd ../../  # Go to reference-agent root
2. generate-courses --courses-per-major 15 --output course_catalog.json
3. ingest-courses --catalog course_catalog.json --clear
""")
    sys.exit(1)
#%% md
---

## Step 2: Build the Baseline RAG Agent

Now let's build our baseline agent. We'll format context as **raw JSON** with all fields included.

### Why Raw JSON is Inefficient

Raw JSON includes unnecessary fields that waste tokens:
- `id`, `created_at`, `updated_at` - Not needed for recommendations
- `current_enrollment`, `max_enrollment` - Rarely relevant
- Verbose structure with brackets, quotes, commas

**Example:** A single course in raw JSON uses ~500 tokens!
#%%
from typing import List
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from redis_context_course import Course

class BaselineRAGAgent:
    """
    Stage 1: Baseline RAG Agent
    
    Simple RAG implementation with raw JSON context.
    Deliberately inefficient to demonstrate the need for optimization.
    """
    
    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.course_manager = CourseManager()
        self.llm = ChatOpenAI(model=model_name, temperature=0.0)
        self.model_name = model_name
        
    async def retrieve_courses(self, query: str, limit: int = 5) -> List[Course]:
        """Retrieve relevant courses using semantic search."""
        return await self.course_manager.search_courses(query, limit=limit)
    
    def format_context_as_json(self, courses: List[Course]) -> str:
        """
        Format courses as raw JSON (deliberately inefficient).
        
        Includes ALL fields, even unnecessary ones like:
        - id, created_at, updated_at
        - current_enrollment, max_enrollment
        - Verbose JSON structure
        """
        course_dicts = []
        for course in courses:
            course_dict = {
                "id": course.id,
                "course_code": course.course_code,
                "title": course.title,
                "description": course.description,
                "department": course.department,
                "credits": course.credits,
                "difficulty_level": course.difficulty_level.value,
                "format": course.format.value,
                "instructor": course.instructor,
                "semester": course.semester.value,
                "year": course.year,
                "max_enrollment": course.max_enrollment,
                "current_enrollment": course.current_enrollment,
                "tags": course.tags,
                "learning_objectives": course.learning_objectives,
                "prerequisites": [
                    {"course_code": p.course_code, "course_title": p.course_title}
                    for p in course.prerequisites
                ] if course.prerequisites else [],
                "created_at": str(course.created_at),
                "updated_at": str(course.updated_at),
            }
            course_dicts.append(course_dict)
        
        return json.dumps(course_dicts, indent=2)
    
    def build_system_prompt(self, context: str) -> str:
        """Build the system prompt with raw JSON context."""
        return f"""You are a helpful Redis University course advisor.

Your role is to help students find courses, understand prerequisites, and plan their learning path.

Here are the relevant courses (in JSON format):

{context}

Instructions:
- Answer questions about courses based on the provided data
- Be helpful and concise
- If a course isn't in the data, say you don't have information about it
"""
    
    async def chat(self, user_message: str) -> str:
        """
        Process a user message and return a response.
        
        Baseline RAG flow:
        1. Retrieve relevant courses
        2. Format as raw JSON
        3. Build system prompt
        4. Send to LLM
        5. Return response
        """
        # Step 1: Retrieve courses
        courses = await self.retrieve_courses(user_message)
        
        # Step 2: Format as raw JSON
        context = self.format_context_as_json(courses)
        
        # Step 3: Build system prompt
        system_prompt = self.build_system_prompt(context)
        
        # Step 4: Create messages
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ]
        
        # Step 5: Get LLM response
        response = await self.llm.ainvoke(messages)
        
        return response.content
    
    async def get_metrics(self, query: str) -> dict:
        """Get metrics about the context."""
        courses = await self.retrieve_courses(query)
        context = self.format_context_as_json(courses)
        token_count = count_tokens(context, self.model_name)
        
        return {
            "token_count": token_count,
            "character_count": len(context),
            "num_courses": len(courses),
            "format": "raw_json",
            "estimated_cost_per_1k_queries": self._estimate_cost(token_count)
        }
    
    def _estimate_cost(self, tokens: int) -> float:
        """Estimate cost per 1,000 queries."""
        # gpt-4o-mini pricing
        input_cost_per_1m = 0.150  # $0.150 per 1M input tokens
        output_cost_per_1m = 0.600  # $0.600 per 1M output tokens
        avg_output_tokens = 200  # Estimated average response length
        
        input_cost = (tokens / 1_000_000) * input_cost_per_1m * 1000
        output_cost = (avg_output_tokens / 1_000_000) * output_cost_per_1m * 1000
        
        return round(input_cost + output_cost, 2)

print("‚úÖ BaselineRAGAgent class defined")
#%% md
### üîÑ LangGraph Version (Production Pattern)

The inline implementation above is great for learning, but in production we use **LangGraph** for better workflow orchestration.

**Key differences:**

| Aspect | Inline Version (Above) | LangGraph Version (agent.py) |
|--------|----------------------|------------------------------|
| **Structure** | Single class with methods | StateGraph with nodes |
| **State** | Local variables | AgentState (Pydantic model) |
| **Flow** | Sequential method calls | Graph with edges |
| **Debugging** | Print statements | State inspection at each node |
| **Extensibility** | Modify methods | Add/remove nodes |

**LangGraph benefits:**
- ‚úÖ **Separation of concerns** - Each node has one job
- ‚úÖ **State management** - AgentState tracks everything
- ‚úÖ **Debuggability** - Inspect state between nodes
- ‚úÖ **Extensibility** - Easy to add complexity in Stage 2/3

The `agent.py` file in this directory contains the LangGraph version. Both implementations produce the same results, but LangGraph scales better for complex workflows.

**For this notebook, we'll continue with the inline version for clarity.**

---

## Step 3: Test the Baseline Agent

Let's test our agent with a sample query.
#%%
# Initialize agent
agent = BaselineRAGAgent()

# Test query
query = "What machine learning courses are available?"

# Get response
response = await agent.chat(query)

print(f"Query: {query}\n")
print(f"Response:\n{response}")
#%% md
The agent works! But let's see how inefficient it is...
#%%
# Get metrics
metrics = await agent.get_metrics(query)

print("üìä Baseline Agent Metrics:")
print(f"   Token count: {metrics['token_count']:,}")
print(f"   Character count: {metrics['character_count']:,}")
print(f"   Number of courses: {metrics['num_courses']}")
print(f"   Format: {metrics['format']}")
print(f"   Est. cost per 1K queries: ${metrics['estimated_cost_per_1k_queries']}")
print(f"\n‚ö†Ô∏è  ~{metrics['token_count']:,} tokens is VERY inefficient!")
#%% md
---

## Step 4: Examine the Raw JSON Context

Let's see what the raw JSON context looks like.
#%%
# Retrieve courses
courses = await agent.retrieve_courses(query, limit=2)

# Format as JSON
json_context = agent.format_context_as_json(courses)

print("Raw JSON Context (first 1000 characters):")
print("=" * 60)
print(json_context[:1000])
print("...")
print("=" * 60)
print(f"\nTotal length: {len(json_context):,} characters")
print(f"Total tokens: {count_tokens(json_context):,}")
#%% md
### Problems with Raw JSON

Looking at the output above, notice:

1. **Unnecessary fields:**
   - `id`: "550e8400-e29b-41d4-a716-446655440000" (not needed)
   - `created_at`, `updated_at` (not needed)
   - `current_enrollment`, `max_enrollment` (rarely relevant)

2. **Verbose structure:**
   - Lots of brackets, quotes, commas
   - Field names repeated for every course
   - Indentation adds whitespace

3. **Hard for LLM to parse:**
   - JSON is machine-readable, not human-readable
   - LLMs work better with natural text

**Result:** ~500 tokens per course √ó 5 courses = ~2,500 tokens!

---
#%% md
## Step 5: Compare Different Queries

Let's test a few different queries to see token usage.
#%%
test_queries = [
    "What beginner courses are available?",
    "Tell me about database courses",
    "What are the prerequisites for advanced ML?"
]

print("Token Usage Across Different Queries:\n")
print(f"{'Query':<50} {'Tokens':>10}")
print("=" * 62)

for test_query in test_queries:
    metrics = await agent.get_metrics(test_query)
    print(f"{test_query:<50} {metrics['token_count']:>10,}")

print("\n‚ö†Ô∏è  All queries use ~2,500 tokens regardless of content!")
#%% md
---

## Key Takeaways

### What We Built

‚úÖ A working RAG agent that:
- Retrieves relevant courses using semantic search
- Formats context as raw JSON
- Generates accurate responses

### What We Learned

‚ùå Raw JSON is inefficient:
- ~2,500 tokens for 5 courses
- $6.25 per 1,000 queries
- Includes unnecessary fields
- Verbose structure

### Next Steps

In **Stage 2**, we'll apply context engineering techniques to:
- Remove unnecessary fields (context cleaning)
- Transform JSON to natural text (better for LLMs)
- Reduce tokens by ~50% (1,200 tokens)
- Cut costs in half ($3.00 per 1K queries)

**Continue to Stage 2:** `../stage2_optimized/notebook.ipynb`

---
#%% md
## Additional Resources

- **Section 2 Notebooks:** Context transformation techniques
- **Stage 1 CLI:** `python cli.py` for interactive testing
- **Stage 1 README:** Detailed documentation and metrics


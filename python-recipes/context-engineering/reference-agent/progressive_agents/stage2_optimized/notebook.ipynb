#%% md
![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)

# Stage 2: Context-Engineered RAG Agent

## From Baseline to Optimized

In Stage 1, we built a working RAG agent that used ~2,500 tokens per query. It worked, but it was inefficient.

In this notebook, we'll apply **context engineering techniques** to reduce token usage by ~50% while maintaining (or improving) answer quality.

## What You'll Build

An optimized RAG agent that:
- âœ… Cleans context (removes unnecessary fields)
- âœ… Transforms JSON to natural text (better for LLMs)
- âœ… Uses ~1,200 tokens (50% reduction)
- âœ… Costs $3.00 per 1,000 queries (50% savings)
- âœ… Generates better responses (natural text > JSON)

**Time to complete:** 25-30 minutes

---
#%% md
## Architecture Overview

### Flow Diagram

```mermaid
graph LR
    A[User Query] --> B[Semantic Search]
    B --> C[Retrieve 5 Courses]
    C --> D[Clean Context]
    D --> E[Transform to Natural Text]
    E --> F[Build System Prompt]
    F --> G[LLM]
    G --> H[Answer]
    
    style D fill:#51cf66
    style E fill:#51cf66
    style G fill:#4ecdc4
```

**Optimized flow with context engineering:**
```
User Query â†’ Semantic Search â†’ Context Cleaning â†’ Text Transformation â†’ LLM â†’ Answer
```

**Key Improvements over Stage 1:**
- âœ… Context cleaning (remove unnecessary fields)
- âœ… Natural text transformation (JSON â†’ text)
- âœ… ~50% token reduction
- âœ… Better LLM comprehension

---
#%% md
## Setup and Environment
#%%
import json
import os
import sys
from pathlib import Path

import tiktoken
from dotenv import load_dotenv

# Add reference-agent to path
sys.path.insert(0, str(Path.cwd().parent.parent))

# Load environment variables
load_dotenv()

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
print("âœ… Environment variables loaded")
#%%
# Utility: Token counter
def count_tokens(text: str, model: str = "gpt-4o") -> int:
    """Count tokens in text using tiktoken."""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

print("âœ… Utility functions loaded")
#%%
from redis_context_course import CourseManager, redis_config

# Check Redis connection
if not redis_config.health_check():
    print("âŒ Redis connection failed!")
    sys.exit(1)

print("âœ… Redis connection successful")
#%% md
---

## Step 1: Context Engineering Techniques

Before building the agent, let's explore the two key techniques:
1. **Context Cleaning** - Remove unnecessary fields
2. **Text Transformation** - Convert JSON to natural text

### Technique 1: Context Cleaning

Let's compare raw vs cleaned course data.
#%%
from redis_context_course import Course
from progressive_agents.stage2_optimized.context_utils import clean_course_data

# Get a sample course
course_manager = CourseManager()
courses = await course_manager.search_courses("machine learning", limit=1)
sample_course = courses[0]

# Raw course data (all fields)
raw_data = {
    "id": sample_course.id,
    "course_code": sample_course.course_code,
    "title": sample_course.title,
    "description": sample_course.description,
    "department": sample_course.department,
    "credits": sample_course.credits,
    "difficulty_level": sample_course.difficulty_level.value,
    "format": sample_course.format.value,
    "instructor": sample_course.instructor,
    "semester": sample_course.semester.value,
    "year": sample_course.year,
    "max_enrollment": sample_course.max_enrollment,
    "current_enrollment": sample_course.current_enrollment,
    "tags": sample_course.tags,
    "learning_objectives": sample_course.learning_objectives,
    "prerequisites": [
        {"course_code": p.course_code, "course_title": p.course_title}
        for p in sample_course.prerequisites
    ] if sample_course.prerequisites else [],
    "created_at": str(sample_course.created_at),
    "updated_at": str(sample_course.updated_at),
}

# Cleaned course data (only essential fields)
cleaned_data = clean_course_data(sample_course)

print("RAW DATA:")
print(json.dumps(raw_data, indent=2)[:500] + "...")
print(f"\nTokens: {count_tokens(json.dumps(raw_data))}")

print("\n" + "="*60 + "\n")

print("CLEANED DATA:")
print(json.dumps(cleaned_data, indent=2))
print(f"\nTokens: {count_tokens(json.dumps(cleaned_data))}")

print("\n" + "="*60)
print(f"Token reduction: ~{((count_tokens(json.dumps(raw_data)) - count_tokens(json.dumps(cleaned_data))) / count_tokens(json.dumps(raw_data)) * 100):.1f}%")
#%% md
### Technique 2: Text Transformation

Now let's transform the cleaned JSON to natural text.
#%%
from progressive_agents.stage2_optimized.context_utils import transform_course_to_text

# Transform to natural text
natural_text = transform_course_to_text(sample_course)

print("NATURAL TEXT FORMAT:")
print("="*60)
print(natural_text)
print("="*60)
print(f"\nTokens: {count_tokens(natural_text)}")

print("\n" + "="*60)
print(f"Token reduction vs raw JSON: ~{((count_tokens(json.dumps(raw_data)) - count_tokens(natural_text)) / count_tokens(json.dumps(raw_data)) * 100):.1f}%")
#%% md
### Comparison: All Three Formats

Let's compare all three formats side-by-side.
#%%
from progressive_agents.stage2_optimized.context_utils import compare_formats

# Compare formats
comparison = compare_formats([sample_course])

print("FORMAT COMPARISON:")
print("="*60)
for format_name, data in comparison.items():
    print(f"\n{format_name.upper()}:")
    print(f"  Tokens: {data['tokens']:,}")
    print(f"  Characters: {data['characters']:,}")
    if 'reduction_vs_raw' in data:
        print(f"  Reduction: {data['reduction_vs_raw']}")
#%% md
**Key Insights:**

1. **Cleaning alone** saves ~20-30% tokens (remove unnecessary fields)
2. **Text transformation** saves another ~20-30% (natural text vs JSON)
3. **Combined** saves ~50% total (raw JSON â†’ natural text)

---
#%% md
## Step 2: Build the Context-Engineered Agent

Now let's build our optimized agent using these techniques.
#%%
from typing import List
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from redis_context_course import Course
from progressive_agents.stage2_optimized.context_utils import format_courses_as_text

class ContextEngineeredAgent:
    """
    Stage 2: Context-Engineered RAG Agent
    
    Applies context engineering techniques:
    - Context cleaning (remove unnecessary fields)
    - Text transformation (JSON â†’ natural text)
    - ~50% token reduction vs Stage 1
    """
    
    def __init__(self, model_name: str = "gpt-4o-mini", optimization_level: str = "standard"):
        self.course_manager = CourseManager()
        self.llm = ChatOpenAI(model=model_name, temperature=0.0)
        self.model_name = model_name
        self.optimization_level = optimization_level  # "none", "standard", "aggressive"
        
    async def retrieve_courses(self, query: str, limit: int = 5) -> List[Course]:
        """Retrieve relevant courses using semantic search."""
        return await self.course_manager.search_courses(query, limit=limit)
    
    def format_context(self, courses: List[Course]) -> str:
        """
        Format courses using context engineering.
        
        Applies:
        1. Context cleaning (remove unnecessary fields)
        2. Text transformation (JSON â†’ natural text)
        """
        return format_courses_as_text(courses, optimization_level=self.optimization_level)
    
    def build_system_prompt(self, context: str) -> str:
        """Build the system prompt with optimized context."""
        return f"""You are a helpful Redis University course advisor.

Your role is to help students find courses, understand prerequisites, and plan their learning path.

Here are the relevant courses:

{context}

Instructions:
- Answer questions about courses based on the provided data
- Be helpful and concise
- If a course isn't in the data, say you don't have information about it
"""
    
    async def chat(self, user_message: str) -> str:
        """
        Process a user message and return a response.
        
        Optimized RAG flow:
        1. Retrieve relevant courses
        2. Clean and transform context
        3. Build system prompt
        4. Send to LLM
        5. Return response
        """
        # Step 1: Retrieve courses
        courses = await self.retrieve_courses(user_message)
        
        # Step 2: Format with optimization
        context = self.format_context(courses)
        
        # Step 3: Build system prompt
        system_prompt = self.build_system_prompt(context)
        
        # Step 4: Create messages
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ]
        
        # Step 5: Get LLM response
        response = await self.llm.ainvoke(messages)
        
        return response.content
    
    async def get_metrics(self, query: str) -> dict:
        """Get metrics about the optimized context."""
        courses = await self.retrieve_courses(query)
        context = self.format_context(courses)
        token_count = count_tokens(context, self.model_name)
        
        return {
            "token_count": token_count,
            "character_count": len(context),
            "num_courses": len(courses),
            "format": "natural_text",
            "optimization_level": self.optimization_level,
            "estimated_cost_per_1k_queries": self._estimate_cost(token_count)
        }
    
    def _estimate_cost(self, tokens: int) -> float:
        """Estimate cost per 1,000 queries."""
        input_cost_per_1m = 0.150
        output_cost_per_1m = 0.600
        avg_output_tokens = 200
        
        input_cost = (tokens / 1_000_000) * input_cost_per_1m * 1000
        output_cost = (avg_output_tokens / 1_000_000) * output_cost_per_1m * 1000
        
        return round(input_cost + output_cost, 2)

print("âœ… ContextEngineeredAgent class defined")
#%% md
---

## Step 3: Test the Optimized Agent

Let's test our optimized agent with the same query from Stage 1.
#%%
# Initialize agent
agent = ContextEngineeredAgent(optimization_level="standard")

# Test query
query = "What machine learning courses are available?"

# Get response
response = await agent.chat(query)

print(f"Query: {query}\n")
print(f"Response:\n{response}")
#%% md
The response quality is the same (or better)! Now let's check the metrics...
#%%
# Get metrics
metrics = await agent.get_metrics(query)

print("ðŸ“Š Optimized Agent Metrics:")
print(f"   Token count: {metrics['token_count']:,}")
print(f"   Character count: {metrics['character_count']:,}")
print(f"   Number of courses: {metrics['num_courses']}")
print(f"   Format: {metrics['format']}")
print(f"   Optimization level: {metrics['optimization_level']}")
print(f"   Est. cost per 1K queries: ${metrics['estimated_cost_per_1k_queries']}")
print(f"\nâœ… ~{metrics['token_count']:,} tokens - Much better!")
#%% md
---

## Step 4: Compare with Stage 1

Let's directly compare Stage 1 (baseline) vs Stage 2 (optimized).
#%%
from progressive_agents.stage1_baseline.agent import BaselineRAGAgent

# Initialize both agents
baseline_agent = BaselineRAGAgent()
optimized_agent = ContextEngineeredAgent()

# Get metrics for same query
baseline_metrics = await baseline_agent.get_metrics(query)
optimized_metrics = await optimized_agent.get_metrics(query)

# Calculate improvements
token_reduction = baseline_metrics['token_count'] - optimized_metrics['token_count']
token_reduction_pct = (token_reduction / baseline_metrics['token_count']) * 100
cost_savings = baseline_metrics['estimated_cost_per_1k_queries'] - optimized_metrics['estimated_cost_per_1k_queries']
cost_savings_pct = (cost_savings / baseline_metrics['estimated_cost_per_1k_queries']) * 100

print("STAGE 1 vs STAGE 2 COMPARISON:")
print("="*60)
print(f"\n{'Metric':<30} {'Stage 1':>12} {'Stage 2':>12} {'Change':>12}")
print("-"*60)
print(f"{'Token count':<30} {baseline_metrics['token_count']:>12,} {optimized_metrics['token_count']:>12,} {f'-{token_reduction:,}':>12}")
print(f"{'Cost per 1K queries':<30} {'$' + str(baseline_metrics['estimated_cost_per_1k_queries']):>12} {'$' + str(optimized_metrics['estimated_cost_per_1k_queries']):>12} {f'-${cost_savings:.2f}':>12}")
print(f"{'Format':<30} {baseline_metrics['format']:>12} {optimized_metrics['format']:>12} {'':>12}")
print("\n" + "="*60)
print(f"Token reduction: {token_reduction_pct:.1f}%")
print(f"Cost savings: {cost_savings_pct:.1f}%")
print(f"\nAnnual savings (1M queries): ${cost_savings * 1000:,.2f}")
#%% md
---

## Step 5: Explore Optimization Levels

The agent supports three optimization levels. Let's compare them.
#%%
optimization_levels = ["none", "standard", "aggressive"]

print("OPTIMIZATION LEVEL COMPARISON:")
print("="*60)
print(f"\n{'Level':<15} {'Tokens':>10} {'Cost/1K':>10} {'Description':<30}")
print("-"*60)

for level in optimization_levels:
    agent = ContextEngineeredAgent(optimization_level=level)
    metrics = await agent.get_metrics(query)
    
    descriptions = {
        "none": "Raw JSON (like Stage 1)",
        "standard": "Natural text (recommended)",
        "aggressive": "Ultra-compact text"
    }
    
    print(f"{level:<15} {metrics['token_count']:>10,} ${metrics['estimated_cost_per_1k_queries']:>9.2f} {descriptions[level]:<30}")

print("\n" + "="*60)
print("Recommendation: Use 'standard' for best balance of clarity and efficiency")
#%% md
---

## Step 6: Examine the Optimized Context

Let's see what the optimized context looks like.
#%%
# Retrieve courses
courses = await optimized_agent.retrieve_courses(query, limit=2)

# Format with optimization
optimized_context = optimized_agent.format_context(courses)

print("Optimized Natural Text Context:")
print("=" * 60)
print(optimized_context)
print("=" * 60)
print(f"\nTotal length: {len(optimized_context):,} characters")
print(f"Total tokens: {count_tokens(optimized_context):,}")
#%% md
### Benefits of Natural Text

Comparing to Stage 1's raw JSON:

1. **More readable:**
   - Natural language structure
   - Clear section headers
   - Easy to scan

2. **More efficient:**
   - No JSON syntax overhead (brackets, quotes, commas)
   - No repeated field names
   - Compact formatting

3. **Better for LLMs:**
   - LLMs are trained on natural text
   - Easier to understand and reason about
   - Better response quality

**Result:** ~400 tokens per course Ã— 5 courses = ~1,200 tokens (vs 2,500 in Stage 1)

---
#%% md
## Key Takeaways

### What We Built

âœ… An optimized RAG agent that:
- Cleans context (removes unnecessary fields)
- Transforms to natural text (better for LLMs)
- Uses ~1,200 tokens (50% reduction)
- Costs $3.00 per 1K queries (50% savings)

### What We Learned

âœ… Context engineering techniques:
- **Cleaning:** Remove unnecessary fields (20-30% savings)
- **Transformation:** JSON â†’ natural text (20-30% savings)
- **Combined:** ~50% total reduction

âœ… Trade-offs:
- "standard" optimization: Best balance
- "aggressive" optimization: Maximum savings, less clarity
- "none" optimization: Baseline (for comparison)

### Next Steps

In **Stage 3**, we'll add production patterns:
- Structured catalog views (hierarchical organization)
- Hybrid context assembly (overview + details)
- Query-aware optimization (smart view selection)
- Further token reduction (~800 tokens, 68% vs Stage 1)

**Continue to Stage 3:** `../stage3_hybrid/notebook.ipynb`

---
#%% md
## Additional Resources

- **Section 2 Notebooks:** Deep dive into context transformation
- **Stage 2 CLI:** `python cli.py` for interactive testing
- **Stage 2 README:** Detailed documentation and examples
- **`context_utils.py`:** Implementation of all optimization techniques


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Context Compression Concepts: Managing Context Size\n",
    "\n",
    "## Why Context Compression Matters\n",
    "\n",
    "**The Problem:** As your agent conversations grow, context becomes huge and expensive.\n",
    "\n",
    "**Real-World Example:**\n",
    "```\n",
    "Initial query: \"What courses should I take?\" (50 tokens)\n",
    "After 10 exchanges: 5,000 tokens\n",
    "After 50 exchanges: 25,000 tokens (exceeds most model limits!)\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "- üí∞ **Cost**: GPT-4 costs ~$0.03 per 1K tokens - 25K tokens = $0.75 per query!\n",
    "- ‚è±Ô∏è **Latency**: Larger contexts = slower responses\n",
    "- üö´ **Limits**: Most models have 4K-32K token limits\n",
    "- üß† **Quality**: Too much context can confuse the model\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "You'll learn simple, practical techniques to:\n",
    "1. **Measure context size** - Count tokens accurately\n",
    "2. **Compress intelligently** - Keep important info, remove fluff\n",
    "3. **Prioritize content** - Most relevant information first\n",
    "4. **Monitor effectiveness** - Track compression impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Simple Token Counting\n",
    "\n",
    "First, let's build a simple token counter to understand our context size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Token Counting Comparison:\n",
      "   Text: \\\"Hello, I'm looking for machine learning courses that would be suitable for my background.\\\"\n",
      "   Characters: 89\n",
      "   Simple count (chars/4): 22 tokens\n",
      "   Accurate count: 17 tokens\n",
      "   Difference: 5 tokens\n",
      "\n",
      "üí° Why This Matters:\n",
      "   ‚Ä¢ Accurate counting helps predict costs\n",
      "   ‚Ä¢ Simple counting is fast for approximations\n",
      "   ‚Ä¢ Production systems need accurate counting\n"
     ]
    }
   ],
   "source": [
    "# Simple setup - no classes, just functions\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Simple token counting (approximation)\n",
    "def count_tokens_simple(text: str) -> int:\n",
    "    \"\"\"Simple token counting - roughly 4 characters per token\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def count_tokens_accurate(text: str) -> int:\n",
    "    \"\"\"More accurate token counting using tiktoken\"\"\"\n",
    "    try:\n",
    "        import tiktoken\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        return len(encoding.encode(text))\n",
    "    except ImportError:\n",
    "        # Fallback to simple counting\n",
    "        return count_tokens_simple(text)\n",
    "\n",
    "# Test our token counting\n",
    "sample_text = \"Hello, I'm looking for machine learning courses that would be suitable for my background.\"\n",
    "\n",
    "simple_count = count_tokens_simple(sample_text)\n",
    "accurate_count = count_tokens_accurate(sample_text)\n",
    "\n",
    "print(\"üî¢ Token Counting Comparison:\")\n",
    "print(f\"   Text: '{sample_text}'\")\n",
    "print(f\"   Characters: {len(sample_text)}\")\n",
    "print(f\"   Simple count (chars/4): {simple_count} tokens\")\n",
    "print(f\"   Accurate count: {accurate_count} tokens\")\n",
    "print(f\"   Difference: {abs(simple_count - accurate_count)} tokens\")\n",
    "\n",
    "print(\"\\nüí° Why This Matters:\")\n",
    "print(\"   ‚Ä¢ Accurate counting helps predict costs\")\n",
    "print(\"   ‚Ä¢ Simple counting is fast for approximations\")\n",
    "print(\"   ‚Ä¢ Production systems need accurate counting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept 1: Context Size Analysis\n",
    "\n",
    "Let's analyze how context grows in a typical conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Context Growth Analysis:\n",
      "==================================================\n",
      "Base context: 89 tokens\n",
      "Turn 1: +25 tokens ‚Üí 114 total\n",
      "Turn 2: +22 tokens ‚Üí 136 total\n",
      "Turn 3: +28 tokens ‚Üí 164 total\n",
      "Turn 4: +35 tokens ‚Üí 199 total\n",
      "Turn 5: +32 tokens ‚Üí 231 total\n",
      "\n",
      "üí∞ Cost Impact:\n",
      "   GPT-3.5: $0.0003 per query\n",
      "   GPT-4: $0.0069 per query\n",
      "   At 1000 queries/day: GPT-4 = $6.93/day\n"
     ]
    }
   ],
   "source": [
    "# Simulate a growing conversation context\n",
    "def simulate_conversation_growth():\n",
    "    \"\"\"Show how context grows over time\"\"\"\n",
    "    \n",
    "    # Simulate conversation turns\n",
    "    conversation = []\n",
    "    \n",
    "    # Base context (student profile, course info, etc.)\n",
    "    base_context = \"\"\"\n",
    "STUDENT PROFILE:\n",
    "Name: Sarah Chen\n",
    "Major: Computer Science, Year 3\n",
    "Completed: RU101, RU201, CS101, CS201\n",
    "Interests: machine learning, data science, python\n",
    "Preferred Format: online\n",
    "\n",
    "AVAILABLE COURSES:\n",
    "1. RU301: Vector Search - Advanced Redis vector operations\n",
    "2. CS301: Machine Learning - Introduction to ML algorithms\n",
    "3. CS302: Deep Learning - Neural networks and deep learning\n",
    "4. CS401: Advanced ML - Advanced machine learning techniques\n",
    "\"\"\"\n",
    "    \n",
    "    # Conversation turns\n",
    "    turns = [\n",
    "        (\"What machine learning courses are available?\", \"I found several ML courses: CS301, CS302, and CS401. CS301 is perfect for beginners...\"),\n",
    "        (\"What are the prerequisites for CS301?\", \"CS301 requires CS101 and CS201, which you've completed. You're eligible to enroll!\"),\n",
    "        (\"How about CS302?\", \"CS302 (Deep Learning) requires CS301 as a prerequisite. You'd need to take CS301 first.\"),\n",
    "        (\"Can you recommend a learning path?\", \"I recommend: 1) CS301 (Machine Learning) this semester, 2) CS302 (Deep Learning) next semester...\"),\n",
    "        (\"What about RU301?\", \"RU301 (Vector Search) is excellent for ML applications. It teaches vector databases used in AI systems...\")\n",
    "    ]\n",
    "    \n",
    "    print(\"üìà Context Growth Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Start with base context\n",
    "    current_context = base_context\n",
    "    base_tokens = count_tokens_accurate(current_context)\n",
    "    print(f\"Base context: {base_tokens} tokens\")\n",
    "    \n",
    "    # Add each conversation turn\n",
    "    for i, (user_msg, assistant_msg) in enumerate(turns, 1):\n",
    "        # Add to conversation history\n",
    "        current_context += f\"\\nUser: {user_msg}\\nAssistant: {assistant_msg}\"\n",
    "        \n",
    "        # Count tokens\n",
    "        total_tokens = count_tokens_accurate(current_context)\n",
    "        turn_tokens = count_tokens_accurate(f\"User: {user_msg}\\nAssistant: {assistant_msg}\")\n",
    "        \n",
    "        print(f\"Turn {i}: +{turn_tokens} tokens ‚Üí {total_tokens} total\")\n",
    "        \n",
    "        # Show cost implications\n",
    "        cost_gpt35 = total_tokens * 0.0015 / 1000  # $0.0015 per 1K tokens\n",
    "        cost_gpt4 = total_tokens * 0.03 / 1000     # $0.03 per 1K tokens\n",
    "        \n",
    "        if i == len(turns):\n",
    "            print(f\"\\nüí∞ Cost Impact:\")\n",
    "            print(f\"   GPT-3.5: ${cost_gpt35:.4f} per query\")\n",
    "            print(f\"   GPT-4: ${cost_gpt4:.4f} per query\")\n",
    "            print(f\"   At 1000 queries/day: GPT-4 = ${cost_gpt4 * 1000:.2f}/day\")\n",
    "\n",
    "simulate_conversation_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept 2: Simple Context Compression\n",
    "\n",
    "Now let's implement simple compression techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Compression Techniques Comparison:\n",
      "Original context: 231 tokens\n",
      "==================================================\n",
      "\n",
      "1. Truncation (200 token limit):\n",
      "   Result: 180 tokens (77.9% of original)\n",
      "   Preview: STUDENT PROFILE: Name: Sarah Chen Major: Computer Science, Year 3 Completed: RU101, RU201, CS101...\n",
      "\n",
      "2. Summarization (keep important lines):\n",
      "   Result: 156 tokens (67.5% of original)\n",
      "   Preview: STUDENT PROFILE: Name: Sarah Chen Major: Computer Science, Year 3 What machine learning courses...\n",
      "\n",
      "üí° Key Insights:\n",
      "   ‚Ä¢ Truncation is fast but loses recent context\n",
      "   ‚Ä¢ Summarization preserves key information\n",
      "   ‚Ä¢ Priority-based keeps most important parts\n",
      "   ‚Ä¢ Choose technique based on your use case\n"
     ]
    }
   ],
   "source": [
    "# Simple compression techniques\n",
    "def compress_by_truncation(text: str, max_tokens: int) -> str:\n",
    "    \"\"\"Simplest compression: just cut off the end\"\"\"\n",
    "    current_tokens = count_tokens_accurate(text)\n",
    "    \n",
    "    if current_tokens <= max_tokens:\n",
    "        return text\n",
    "    \n",
    "    # Rough truncation - cut to approximate token limit\n",
    "    chars_per_token = len(text) / current_tokens\n",
    "    target_chars = int(max_tokens * chars_per_token)\n",
    "    \n",
    "    return text[:target_chars] + \"...[truncated]\"\n",
    "\n",
    "def compress_by_summarization(conversation_history: str) -> str:\n",
    "    \"\"\"Simple summarization - keep key points\"\"\"\n",
    "    # Simple rule-based summarization\n",
    "    lines = conversation_history.split('\\n')\n",
    "    \n",
    "    # Keep important lines (questions, course codes, recommendations)\n",
    "    important_lines = []\n",
    "    for line in lines:\n",
    "        if any(keyword in line.lower() for keyword in \n",
    "               ['?', 'recommend', 'cs301', 'cs302', 'ru301', 'prerequisite']):\n",
    "            important_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(important_lines)\n",
    "\n",
    "def compress_by_priority(context_parts: dict, max_tokens: int) -> str:\n",
    "    \"\"\"Compress by keeping most important parts first\"\"\"\n",
    "    # Priority order (most important first)\n",
    "    priority_order = ['student_profile', 'current_query', 'recent_conversation', 'course_info', 'old_conversation']\n",
    "    \n",
    "    compressed_context = \"\"\n",
    "    used_tokens = 0\n",
    "    \n",
    "    for part_name in priority_order:\n",
    "        if part_name in context_parts:\n",
    "            part_text = context_parts[part_name]\n",
    "            part_tokens = count_tokens_accurate(part_text)\n",
    "            \n",
    "            if used_tokens + part_tokens <= max_tokens:\n",
    "                compressed_context += part_text + \"\\n\\n\"\n",
    "                used_tokens += part_tokens\n",
    "            else:\n",
    "                # Partial inclusion if space allows\n",
    "                remaining_tokens = max_tokens - used_tokens\n",
    "                if remaining_tokens > 50:  # Only if meaningful space left\n",
    "                    partial_text = compress_by_truncation(part_text, remaining_tokens)\n",
    "                    compressed_context += partial_text\n",
    "                break\n",
    "    \n",
    "    return compressed_context.strip()\n",
    "\n",
    "# Test compression techniques\n",
    "sample_context = \"\"\"\n",
    "STUDENT PROFILE:\n",
    "Name: Sarah Chen, Major: Computer Science, Year 3\n",
    "Completed: RU101, RU201, CS101, CS201\n",
    "Interests: machine learning, data science, python\n",
    "\n",
    "CONVERSATION:\n",
    "User: What machine learning courses are available?\n",
    "Assistant: I found several ML courses: CS301 (Machine Learning), CS302 (Deep Learning), and CS401 (Advanced ML). CS301 is perfect for beginners and covers supervised learning, unsupervised learning, and basic neural networks. It requires CS101 and CS201 as prerequisites.\n",
    "\n",
    "User: What are the prerequisites for CS301?\n",
    "Assistant: CS301 requires CS101 (Introduction to Programming) and CS201 (Data Structures), which you've already completed. You're eligible to enroll!\n",
    "\n",
    "User: How about CS302?\n",
    "Assistant: CS302 (Deep Learning) is more advanced and requires CS301 as a prerequisite. It covers neural networks, CNNs, RNNs, and modern architectures like transformers.\n",
    "\"\"\"\n",
    "\n",
    "original_tokens = count_tokens_accurate(sample_context)\n",
    "print(f\"üîç Compression Techniques Comparison:\")\n",
    "print(f\"Original context: {original_tokens} tokens\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test truncation\n",
    "truncated = compress_by_truncation(sample_context, 200)\n",
    "truncated_tokens = count_tokens_accurate(truncated)\n",
    "print(f\"1. Truncation (200 token limit):\")\n",
    "print(f\"   Result: {truncated_tokens} tokens ({truncated_tokens/original_tokens:.1%} of original)\")\n",
    "print(f\"   Preview: {truncated[:100]}...\")\n",
    "\n",
    "# Test summarization\n",
    "summarized = compress_by_summarization(sample_context)\n",
    "summarized_tokens = count_tokens_accurate(summarized)\n",
    "print(f\"\\n2. Summarization (keep important lines):\")\n",
    "print(f\"   Result: {summarized_tokens} tokens ({summarized_tokens/original_tokens:.1%} of original)\")\n",
    "print(f\"   Preview: {summarized[:100]}...\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Truncation is fast but loses recent context\")\n",
    "print(\"   ‚Ä¢ Summarization preserves key information\")\n",
    "print(\"   ‚Ä¢ Priority-based keeps most important parts\")\n",
    "print(\"   ‚Ä¢ Choose technique based on your use case\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Optimizing for Production: Context Engineering at Scale\n",
    "\n",
    "## Welcome to Section 5: Context Optimization\n",
    "\n",
    "In Section 4, you built a sophisticated multi-tool agent with semantic routing. Now you'll optimize it for production use with:\n",
    "- Context compression and pruning strategies\n",
    "- Token usage optimization and cost management\n",
    "- Performance monitoring and analytics\n",
    "- Scalable architecture patterns\n",
    "\n",
    "This is where your educational project becomes a production-ready system.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Implement context compression and relevance-based pruning\n",
    "2. Add token usage tracking and cost optimization\n",
    "3. Build performance monitoring and analytics\n",
    "4. Create scalable caching and batching strategies\n",
    "5. Deploy optimization techniques for production workloads\n",
    "\n",
    "## The Production Challenge\n",
    "\n",
    "Your multi-tool agent works great in development, but production brings new challenges:\n",
    "\n",
    "### Scale Challenges:\n",
    "- **Cost**: Token usage can become expensive at scale\n",
    "- **Latency**: Large contexts slow down responses\n",
    "- **Memory**: Long conversations consume increasing memory\n",
    "- **Concurrency**: Multiple users require efficient resource sharing\n",
    "\n",
    "### Cross-Reference: Optimization Concepts\n",
    "\n",
    "This builds on optimization patterns from existing notebooks and production systems:\n",
    "- Context window management and token budgeting\n",
    "- Memory compression and summarization strategies\n",
    "- Performance monitoring and cost tracking\n",
    "\n",
    "**Development vs Production:**\n",
    "```\n",
    "Development: \"Does it work?\"\n",
    "Production: \"Does it work efficiently at scale with acceptable cost?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Your Multi-Tool Agent\n",
    "\n",
    "First, let's load the multi-tool agent you built in Section 4 as our optimization target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required environment variables are set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not found. Please create a .env file with your OpenAI API key. \"\n",
    "        \"Get your key from: https://platform.openai.com/api-keys\"\n",
    "    )\n",
    "\n",
    "print(\"✅ Environment variables loaded\")\n",
    "print(f\"   REDIS_URL: {os.getenv('REDIS_URL', 'redis://localhost:6379')}\")\n",
    "print(f\"   OPENAI_API_KEY: {'✓ Set' if os.getenv('OPENAI_API_KEY') else '✗ Not set'}\")\n",
    "\n",
    "# Import components from previous sections\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add reference agent to path\n",
    "sys.path.append('../../../reference-agent')\n",
    "\n",
    "from redis_context_course.models import (\n",
    "    Course, StudentProfile, DifficultyLevel, \n",
    "    CourseFormat, Semester\n",
    ")\n",
    "from redis_context_course.course_manager import CourseManager\n",
    "\n",
    "print(\"Foundation components loaded for optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Context Optimizer\n",
    "\n",
    "Let's create a context optimizer that can compress and prune context intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionContextOptimizer:\n",
    "    \"\"\"Context optimizer for production workloads\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 4000, compression_ratio: float = 0.7):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.token_usage_stats = defaultdict(int)\n",
    "        self.optimization_stats = defaultdict(int)\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count (simplified - real implementation would use tiktoken)\"\"\"\n",
    "        # Rough estimation: ~4 characters per token\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def compress_conversation_history(self, conversation: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Compress conversation history by summarizing older messages\"\"\"\n",
    "        if len(conversation) <= 6:  # Keep recent messages as-is\n",
    "            return conversation\n",
    "        \n",
    "        # Keep last 4 messages, summarize the rest\n",
    "        recent_messages = conversation[-4:]\n",
    "        older_messages = conversation[:-4]\n",
    "        \n",
    "        # Create summary of older messages\n",
    "        summary_content = self._summarize_messages(older_messages)\n",
    "        \n",
    "        summary_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"[Conversation Summary: {summary_content}]\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": \"summary\"\n",
    "        }\n",
    "        \n",
    "        self.optimization_stats[\"conversations_compressed\"] += 1\n",
    "        return [summary_message] + recent_messages\n",
    "    \n",
    "    def _summarize_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Create a summary of conversation messages\"\"\"\n",
    "        topics = set()\n",
    "        user_intents = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            content = msg.get(\"content\", \"\").lower()\n",
    "            \n",
    "            # Extract topics\n",
    "            if \"machine learning\" in content or \"ml\" in content:\n",
    "                topics.add(\"machine learning\")\n",
    "            if \"course\" in content:\n",
    "                topics.add(\"courses\")\n",
    "            if \"recommend\" in content or \"suggest\" in content:\n",
    "                topics.add(\"recommendations\")\n",
    "            \n",
    "            # Extract user intents\n",
    "            if msg.get(\"role\") == \"user\":\n",
    "                if \"what\" in content and \"course\" in content:\n",
    "                    user_intents.append(\"course inquiry\")\n",
    "                elif \"can i\" in content or \"eligible\" in content:\n",
    "                    user_intents.append(\"eligibility check\")\n",
    "        \n",
    "        summary_parts = []\n",
    "        if topics:\n",
    "            summary_parts.append(f\"Topics: {', '.join(topics)}\")\n",
    "        if user_intents:\n",
    "            summary_parts.append(f\"User asked about: {', '.join(set(user_intents))}\")\n",
    "        \n",
    "        return \"; \".join(summary_parts) if summary_parts else \"General conversation about courses\"\n",
    "    \n",
    "    def prune_context_by_relevance(self, context_parts: List[Tuple[str, str]], query: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Prune context parts based on relevance to current query\"\"\"\n",
    "        if len(context_parts) <= 3:  # Don't prune if already small\n",
    "            return context_parts\n",
    "        \n",
    "        # Score relevance of each context part\n",
    "        scored_parts = []\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        for part_type, content in context_parts:\n",
    "            content_words = set(content.lower().split())\n",
    "            overlap = len(query_words.intersection(content_words))\n",
    "            \n",
    "            # Boost score for certain context types\n",
    "            relevance_score = overlap\n",
    "            if part_type in [\"student_profile\", \"current_query\"]:\n",
    "                relevance_score += 10  # Always keep these\n",
    "            elif part_type == \"conversation_history\":\n",
    "                relevance_score += 5   # High priority\n",
    "            \n",
    "            scored_parts.append((relevance_score, part_type, content))\n",
    "        \n",
    "        # Sort by relevance and keep top parts\n",
    "        scored_parts.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Keep parts that fit within token budget\n",
    "        selected_parts = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for score, part_type, content in scored_parts:\n",
    "            part_tokens = self.estimate_tokens(content)\n",
    "            if total_tokens + part_tokens <= self.max_tokens * self.compression_ratio:\n",
    "                selected_parts.append((part_type, content))\n",
    "                total_tokens += part_tokens\n",
    "            else:\n",
    "                self.optimization_stats[\"context_parts_pruned\"] += 1\n",
    "        \n",
    "        return selected_parts\n",
    "    \n",
    "    def optimize_context(self, context_data: Dict[str, Any], query: str) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"Main optimization method that combines all strategies\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract context parts\n",
    "        context_parts = []\n",
    "        \n",
    "        # Student profile (always include)\n",
    "        if \"student_profile\" in context_data:\n",
    "            profile_text = self._format_student_profile(context_data[\"student_profile\"])\n",
    "            context_parts.append((\"student_profile\", profile_text))\n",
    "        \n",
    "        # Conversation history (compress if needed)\n",
    "        if \"conversation_history\" in context_data:\n",
    "            compressed_history = self.compress_conversation_history(context_data[\"conversation_history\"])\n",
    "            history_text = self._format_conversation_history(compressed_history)\n",
    "            context_parts.append((\"conversation_history\", history_text))\n",
    "        \n",
    "        # Retrieved courses (limit to most relevant)\n",
    "        if \"retrieved_courses\" in context_data:\n",
    "            courses_text = self._format_courses(context_data[\"retrieved_courses\"][:3])  # Limit to top 3\n",
    "            context_parts.append((\"retrieved_courses\", courses_text))\n",
    "        \n",
    "        # Memory context (summarize if long)\n",
    "        if \"loaded_memories\" in context_data:\n",
    "            memory_text = self._format_memories(context_data[\"loaded_memories\"][:5])  # Limit to top 5\n",
    "            context_parts.append((\"loaded_memories\", memory_text))\n",
    "        \n",
    "        # Current query (always include)\n",
    "        context_parts.append((\"current_query\", f\"CURRENT QUERY: {query}\"))\n",
    "        \n",
    "        # Prune by relevance\n",
    "        optimized_parts = self.prune_context_by_relevance(context_parts, query)\n",
    "        \n",
    "        # Assemble final context\n",
    "        final_context = \"\\n\\n\".join([content for _, content in optimized_parts])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        optimization_time = time.time() - start_time\n",
    "        final_tokens = self.estimate_tokens(final_context)\n",
    "        \n",
    "        metrics = {\n",
    "            \"original_parts\": len(context_parts),\n",
    "            \"optimized_parts\": len(optimized_parts),\n",
    "            \"final_tokens\": final_tokens,\n",
    "            \"optimization_time_ms\": int(optimization_time * 1000),\n",
    "            \"compression_achieved\": len(context_parts) > len(optimized_parts)\n",
    "        }\n",
    "        \n",
    "        # Update stats\n",
    "        self.token_usage_stats[\"total_tokens\"] += final_tokens\n",
    "        self.optimization_stats[\"contexts_optimized\"] += 1\n",
    "        \n",
    "        return final_context, metrics\n",
    "    \n",
    "    def _format_student_profile(self, profile: Dict) -> str:\n",
    "        \"\"\"Format student profile concisely\"\"\"\n",
    "        return f\"\"\"STUDENT: {profile.get('name', 'Unknown')}\n",
    "Major: {profile.get('major', 'Unknown')}, Year: {profile.get('year', 'Unknown')}\n",
    "Completed: {', '.join(profile.get('completed_courses', []))}\n",
    "Interests: {', '.join(profile.get('interests', []))}\n",
    "Preferences: {profile.get('preferred_format', 'Unknown')}, {profile.get('preferred_difficulty', 'Unknown')} level\"\"\"\n",
    "    \n",
    "    def _format_conversation_history(self, history: List[Dict]) -> str:\n",
    "        \"\"\"Format conversation history concisely\"\"\"\n",
    "        if not history:\n",
    "            return \"\"\n",
    "        \n",
    "        formatted = \"CONVERSATION:\\n\"\n",
    "        for msg in history[-4:]:  # Last 4 messages\n",
    "            role = msg[\"role\"].title()\n",
    "            content = msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"]\n",
    "            formatted += f\"{role}: {content}\\n\"\n",
    "        \n",
    "        return formatted.strip()\n",
    "    \n",
    "    def _format_courses(self, courses: List[Dict]) -> str:\n",
    "        \"\"\"Format course information concisely\"\"\"\n",
    "        if not courses:\n",
    "            return \"\"\n",
    "        \n",
    "        formatted = \"RELEVANT COURSES:\\n\"\n",
    "        for i, course in enumerate(courses, 1):\n",
    "            formatted += f\"{i}. {course.get('course_code', 'Unknown')}: {course.get('title', 'Unknown')}\\n\"\n",
    "            formatted += f\"   Level: {course.get('level', 'Unknown')}, Credits: {course.get('credits', 'Unknown')}\\n\"\n",
    "        \n",
    "        return formatted.strip()\n",
    "    \n",
    "    def _format_memories(self, memories: List[Dict]) -> str:\n",
    "        \"\"\"Format memory information concisely\"\"\"\n",
    "        if not memories:\n",
    "            return \"\"\n",
    "        \n",
    "        formatted = \"RELEVANT MEMORIES:\\n\"\n",
    "        for memory in memories:\n",
    "            if isinstance(memory, dict) and \"content\" in memory:\n",
    "                content = memory[\"content\"][:80] + \"...\" if len(memory[\"content\"]) > 80 else memory[\"content\"]\n",
    "                formatted += f\"- {content}\\n\"\n",
    "            else:\n",
    "                formatted += f\"- {str(memory)[:80]}...\\n\"\n",
    "        \n",
    "        return formatted.strip()\n",
    "    \n",
    "    def get_optimization_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get optimization performance statistics\"\"\"\n",
    "        return {\n",
    "            \"token_usage\": dict(self.token_usage_stats),\n",
    "            \"optimization_stats\": dict(self.optimization_stats),\n",
    "            \"average_tokens_per_context\": (\n",
    "                self.token_usage_stats[\"total_tokens\"] / max(1, self.optimization_stats[\"contexts_optimized\"])\n",
    "            )\n",
    "        }\n",
    "\n",
    "# Initialize the context optimizer\n",
    "context_optimizer = ProductionContextOptimizer(max_tokens=4000, compression_ratio=0.7)\n",
    "\n",
    "print(\"Production context optimizer initialized\")\n",
    "print(f\"Max tokens: {context_optimizer.max_tokens}\")\n",
    "print(f\"Compression ratio: {context_optimizer.compression_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Production-Ready Agent\n",
    "\n",
    "Let's create an optimized version of your multi-tool agent that uses the context optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedProductionAgent:\n",
    "    \"\"\"Production-optimized agent with context compression and monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, context_optimizer: ProductionContextOptimizer):\n",
    "        self.context_optimizer = context_optimizer\n",
    "        self.course_manager = CourseManager()\n",
    "        \n",
    "        # Performance monitoring\n",
    "        self.performance_metrics = defaultdict(list)\n",
    "        self.cost_tracking = defaultdict(float)\n",
    "        \n",
    "        # Caching for efficiency\n",
    "        self.query_cache = {}  # Simple in-memory cache\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "        # Session management\n",
    "        self.active_sessions = {}\n",
    "        self.session_stats = defaultdict(int)\n",
    "    \n",
    "    def start_optimized_session(self, student: StudentProfile) -> str:\n",
    "        \"\"\"Start an optimized session with efficient memory management\"\"\"\n",
    "        session_id = f\"{student.email}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Create lightweight session context\n",
    "        session_context = {\n",
    "            \"student_profile\": {\n",
    "                \"name\": student.name,\n",
    "                \"email\": student.email,\n",
    "                \"major\": student.major,\n",
    "                \"year\": student.year,\n",
    "                \"completed_courses\": student.completed_courses,\n",
    "                \"interests\": student.interests[:3],  # Limit to top 3 interests\n",
    "                \"preferred_format\": student.preferred_format.value,\n",
    "                \"preferred_difficulty\": student.preferred_difficulty.value\n",
    "            },\n",
    "            \"conversation_history\": [],\n",
    "            \"loaded_memories\": [],  # Would load from Redis in real system\n",
    "            \"session_start_time\": time.time(),\n",
    "            \"query_count\": 0\n",
    "        }\n",
    "        \n",
    "        self.active_sessions[session_id] = session_context\n",
    "        self.session_stats[\"sessions_started\"] += 1\n",
    "        \n",
    "        print(f\"Started optimized session {session_id} for {student.name}\")\n",
    "        return session_id\n",
    "    \n",
    "    def _check_cache(self, query: str, student_email: str) -> Optional[str]:\n",
    "        \"\"\"Check if we have a cached response for this query\"\"\"\n",
    "        cache_key = f\"{student_email}:{query.lower().strip()}\"\n",
    "        \n",
    "        if cache_key in self.query_cache:\n",
    "            cache_entry = self.query_cache[cache_key]\n",
    "            # Check if cache entry is still fresh (within 1 hour)\n",
    "            if time.time() - cache_entry[\"timestamp\"] < 3600:\n",
    "                self.cache_hits += 1\n",
    "                return cache_entry[\"response\"]\n",
    "            else:\n",
    "                # Remove stale cache entry\n",
    "                del self.query_cache[cache_key]\n",
    "        \n",
    "        self.cache_misses += 1\n",
    "        return None\n",
    "    \n",
    "    def _cache_response(self, query: str, student_email: str, response: str):\n",
    "        \"\"\"Cache a response for future use\"\"\"\n",
    "        cache_key = f\"{student_email}:{query.lower().strip()}\"\n",
    "        self.query_cache[cache_key] = {\n",
    "            \"response\": response,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        \n",
    "        # Limit cache size to prevent memory bloat\n",
    "        if len(self.query_cache) > 1000:\n",
    "            # Remove oldest entries\n",
    "            oldest_keys = sorted(self.query_cache.keys(), \n",
    "                               key=lambda k: self.query_cache[k][\"timestamp\"])[:100]\n",
    "            for key in oldest_keys:\n",
    "                del self.query_cache[key]\n",
    "    \n",
    "    def optimized_chat(self, session_id: str, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Optimized chat method with performance monitoring\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if session_id not in self.active_sessions:\n",
    "            return {\"error\": \"Invalid session ID\", \"response\": \"Please start a session first.\"}\n",
    "        \n",
    "        session_context = self.active_sessions[session_id]\n",
    "        student_email = session_context[\"student_profile\"][\"email\"]\n",
    "        \n",
    "        # Check cache first\n",
    "        cached_response = self._check_cache(query, student_email)\n",
    "        if cached_response:\n",
    "            return {\n",
    "                \"response\": cached_response,\n",
    "                \"cached\": True,\n",
    "                \"processing_time_ms\": int((time.time() - start_time) * 1000)\n",
    "            }\n",
    "        \n",
    "        # Add query to conversation history\n",
    "        session_context[\"conversation_history\"].append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        session_context[\"query_count\"] += 1\n",
    "        \n",
    "        # Simulate course retrieval (would use real search in production)\n",
    "        retrieved_courses = self._simulate_course_search(query)\n",
    "        \n",
    "        # Prepare context data for optimization\n",
    "        context_data = {\n",
    "            \"student_profile\": session_context[\"student_profile\"],\n",
    "            \"conversation_history\": session_context[\"conversation_history\"],\n",
    "            \"retrieved_courses\": retrieved_courses,\n",
    "            \"loaded_memories\": session_context[\"loaded_memories\"]\n",
    "        }\n",
    "        \n",
    "        # Optimize context\n",
    "        optimized_context, optimization_metrics = self.context_optimizer.optimize_context(context_data, query)\n",
    "        \n",
    "        # Generate response (simplified - would use LLM in production)\n",
    "        response = self._generate_optimized_response(query, retrieved_courses, session_context)\n",
    "        \n",
    "        # Add response to conversation history\n",
    "        session_context[\"conversation_history\"].append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Cache the response\n",
    "        self._cache_response(query, student_email, response)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Track costs (simplified calculation)\n",
    "        estimated_cost = optimization_metrics[\"final_tokens\"] * 0.00002  # $0.02 per 1K tokens\n",
    "        self.cost_tracking[\"total_cost\"] += estimated_cost\n",
    "        self.cost_tracking[\"total_tokens\"] += optimization_metrics[\"final_tokens\"]\n",
    "        \n",
    "        # Record performance metrics\n",
    "        self.performance_metrics[\"response_times\"].append(total_time)\n",
    "        self.performance_metrics[\"token_counts\"].append(optimization_metrics[\"final_tokens\"])\n",
    "        self.performance_metrics[\"optimization_times\"].append(optimization_metrics[\"optimization_time_ms\"])\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"cached\": False,\n",
    "            \"processing_time_ms\": int(total_time * 1000),\n",
    "            \"optimization_metrics\": optimization_metrics,\n",
    "            \"estimated_cost\": estimated_cost,\n",
    "            \"session_query_count\": session_context[\"query_count\"]\n",
    "        }\n",
    "    \n",
    "    def _simulate_course_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Simulate course search (would use real CourseManager in production)\"\"\"\n",
    "        # Simplified course data for demonstration\n",
    "        all_courses = [\n",
    "            {\"course_code\": \"RU101\", \"title\": \"Introduction to Redis\", \"level\": \"beginner\", \"credits\": 3},\n",
    "            {\"course_code\": \"RU201\", \"title\": \"Redis for Python\", \"level\": \"intermediate\", \"credits\": 4},\n",
    "            {\"course_code\": \"RU301\", \"title\": \"Vector Similarity Search\", \"level\": \"advanced\", \"credits\": 4},\n",
    "            {\"course_code\": \"RU302\", \"title\": \"Redis for Machine Learning\", \"level\": \"advanced\", \"credits\": 4}\n",
    "        ]\n",
    "        \n",
    "        # Simple keyword matching\n",
    "        query_lower = query.lower()\n",
    "        relevant_courses = []\n",
    "        \n",
    "        for course in all_courses:\n",
    "            if any(keyword in query_lower for keyword in [\"machine learning\", \"ml\", \"vector\"]):\n",
    "                if \"machine learning\" in course[\"title\"].lower() or \"vector\" in course[\"title\"].lower():\n",
    "                    relevant_courses.append(course)\n",
    "            elif \"python\" in query_lower:\n",
    "                if \"python\" in course[\"title\"].lower():\n",
    "                    relevant_courses.append(course)\n",
    "            elif \"beginner\" in query_lower or \"introduction\" in query_lower:\n",
    "                if course[\"level\"] == \"beginner\":\n",
    "                    relevant_courses.append(course)\n",
    "        \n",
    "        return relevant_courses[:3]  # Return top 3 matches\n",
    "    \n",
    "    def _generate_optimized_response(self, query: str, courses: List[Dict], session_context: Dict) -> str:\n",
    "        \"\"\"Generate optimized response (simplified - would use LLM in production)\"\"\"\n",
    "        if not courses:\n",
    "            return \"I couldn't find specific courses matching your query. Could you provide more details about what you're looking for?\"\n",
    "        \n",
    "        student_name = session_context[\"student_profile\"][\"name\"]\n",
    "        interests = session_context[\"student_profile\"][\"interests\"]\n",
    "        \n",
    "        response = f\"Hi {student_name}! Based on your interests in {', '.join(interests)}, I found these relevant courses:\\n\\n\"\n",
    "        \n",
    "        for course in courses:\n",
    "            response += f\"• **{course['course_code']}: {course['title']}**\\n\"\n",
    "            response += f\"  Level: {course['level'].title()}, Credits: {course['credits']}\\n\\n\"\n",
    "        \n",
    "        response += \"Would you like more details about any of these courses?\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_performance_analytics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive performance analytics\"\"\"\n",
    "        response_times = self.performance_metrics[\"response_times\"]\n",
    "        token_counts = self.performance_metrics[\"token_counts\"]\n",
    "        \n",
    "        analytics = {\n",
    "            \"performance\": {\n",
    "                \"total_queries\": len(response_times),\n",
    "                \"avg_response_time_ms\": int(sum(response_times) / len(response_times) * 1000) if response_times else 0,\n",
    "                \"max_response_time_ms\": int(max(response_times) * 1000) if response_times else 0,\n",
    "                \"min_response_time_ms\": int(min(response_times) * 1000) if response_times else 0\n",
    "            },\n",
    "            \"token_usage\": {\n",
    "                \"total_tokens\": sum(token_counts),\n",
    "                \"avg_tokens_per_query\": int(sum(token_counts) / len(token_counts)) if token_counts else 0,\n",
    "                \"max_tokens_per_query\": max(token_counts) if token_counts else 0\n",
    "            },\n",
    "            \"caching\": {\n",
    "                \"cache_hits\": self.cache_hits,\n",
    "                \"cache_misses\": self.cache_misses,\n",
    "                \"cache_hit_rate\": self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0,\n",
    "                \"cache_size\": len(self.query_cache)\n",
    "            },\n",
    "            \"costs\": {\n",
    "                \"total_estimated_cost\": round(self.cost_tracking[\"total_cost\"], 4),\n",
    "                \"total_tokens_processed\": int(self.cost_tracking[\"total_tokens\"]),\n",
    "                \"avg_cost_per_query\": round(self.cost_tracking[\"total_cost\"] / len(response_times), 4) if response_times else 0\n",
    "            },\n",
    "            \"sessions\": dict(self.session_stats),\n",
    "            \"optimization\": self.context_optimizer.get_optimization_stats()\n",
    "        }\n",
    "        \n",
    "        return analytics\n",
    "\n",
    "# Initialize the optimized production agent\n",
    "production_agent = OptimizedProductionAgent(context_optimizer)\n",
    "\n",
    "print(\"Optimized production agent initialized\")\n",
    "print(\"Features: Context optimization, caching, performance monitoring, cost tracking\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

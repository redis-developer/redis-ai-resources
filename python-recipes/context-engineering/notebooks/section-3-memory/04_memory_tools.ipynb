{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory Tools: Giving the LLM Control Over Memory\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this advanced notebook, you'll learn how to give your agent control over its own memory using tools. Instead of automatically extracting memories, you can let the LLM decide what to remember and when to search for memories. The Agent Memory Server SDK provides built-in memory tools for this.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "- Why give the LLM control over memory\n",
        "- Agent Memory Server's built-in memory tools\n",
        "- How to configure memory tools for your agent\n",
        "- When the LLM decides to store vs. search memories\n",
        "- Best practices for memory-aware agents\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Completed all Section 3 notebooks\n",
        "- Redis 8 running locally\n",
        "- Agent Memory Server running\n",
        "- OpenAI API key set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concepts: Tool-Based Memory Management\n",
        "\n",
        "### Two Approaches to Memory\n",
        "\n",
        "#### 1. Automatic Memory (What We've Been Doing)\n",
        "\n",
        "```python\n",
        "# Agent has conversation\n",
        "# → Save working memory\n",
        "# → Agent Memory Server automatically extracts important facts\n",
        "# → Facts stored in long-term memory\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- ✅ Fully automatic\n",
        "- ✅ No LLM overhead in your application\n",
        "- ✅ Consistent extraction\n",
        "\n",
        "**Cons:**\n",
        "- ⚠️ Your application's LLM can't directly control what gets extracted\n",
        "- ⚠️ May extract too much or too little\n",
        "- ⚠️ Can't dynamically decide what's important based on conversation context\n",
        "\n",
        "**Note:** You can configure custom extraction prompts on the memory server to guide what gets extracted, but your client application's LLM doesn't have direct control over the extraction process.\n",
        "\n",
        "#### 2. Tool-Based Memory (This Notebook)\n",
        "\n",
        "```python\n",
        "# Agent has conversation\n",
        "# → LLM decides: \"This is important, I should remember it\"\n",
        "# → LLM calls store_memory tool\n",
        "# → Fact stored in long-term memory\n",
        "\n",
        "# Later...\n",
        "# → LLM decides: \"I need to know about the user's preferences\"\n",
        "# → LLM calls search_memories tool\n",
        "# → Retrieves relevant memories\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- ✅ Your application's LLM has full control\n",
        "- ✅ Can decide what's important in real-time\n",
        "- ✅ Can search when needed\n",
        "- ✅ More intelligent, context-aware behavior\n",
        "\n",
        "**Cons:**\n",
        "- ⚠️ Requires tool calls (more tokens)\n",
        "- ⚠️ LLM might forget to store/search\n",
        "- ⚠️ Less consistent\n",
        "\n",
        "### When to Use Tool-Based Memory\n",
        "\n",
        "**Use tool-based memory when:**\n",
        "- ✅ Agent needs fine-grained control\n",
        "- ✅ Importance is context-dependent\n",
        "- ✅ Agent should decide when to search\n",
        "- ✅ Building advanced, autonomous agents\n",
        "\n",
        "**Use automatic memory when:**\n",
        "- ✅ Simple, consistent extraction is fine\n",
        "- ✅ Want to minimize token usage\n",
        "- ✅ Building straightforward agents\n",
        "\n",
        "**Best: Use both!**\n",
        "- Automatic extraction for baseline\n",
        "- Tools for explicit control\n",
        "\n",
        "### Agent Memory Server's Built-in Tools\n",
        "\n",
        "The Agent Memory Server SDK provides:\n",
        "\n",
        "1. **`store_memory`** - Store important information\n",
        "2. **`search_memories`** - Search for relevant memories\n",
        "3. **`update_memory`** - Update existing memories\n",
        "4. **`delete_memory`** - Remove memories\n",
        "\n",
        "These are pre-built, tested, and optimized!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "from agent_memory_client import MemoryAPIClient as MemoryClient, MemoryClientConfig\n",
        "\n",
        "# Initialize\n",
        "student_id = \"student_memory_tools\"\n",
        "session_id = \"tool_demo\"\n",
        "\n",
        "# Initialize memory client with proper config\n",
        "import os\n",
        "config = MemoryClientConfig(\n",
        "    base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8000\"),\n",
        "    default_namespace=\"redis_university\"\n",
        ")\n",
        "memory_client = MemoryClient(config=config)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "print(f\"✅ Setup complete for {student_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring Agent Memory Server's Memory Tools\n",
        "\n",
        "Let's create tools that wrap the Agent Memory Server's memory operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tool 1: Store Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StoreMemoryInput(BaseModel):\n",
        "    text: str = Field(description=\"The information to remember\")\n",
        "    memory_type: str = Field(\n",
        "        default=\"semantic\",\n",
        "        description=\"Type of memory: 'semantic' for facts, 'episodic' for events\"\n",
        "    )\n",
        "    topics: List[str] = Field(\n",
        "        default=[],\n",
        "        description=\"Topics/tags for this memory (e.g., ['preferences', 'courses'])\"\n",
        "    )\n",
        "\n",
        "@tool(args_schema=StoreMemoryInput)\n",
        "async def store_memory(text: str, memory_type: str = \"semantic\", topics: List[str] = []) -> str:\n",
        "    \"\"\"\n",
        "    Store important information in long-term memory.\n",
        "    \n",
        "    Use this tool when:\n",
        "    - Student shares preferences (e.g., \"I prefer online courses\")\n",
        "    - Student states goals (e.g., \"I want to graduate in 2026\")\n",
        "    - Student provides important facts (e.g., \"My major is Computer Science\")\n",
        "    - You learn something that should be remembered for future sessions\n",
        "    \n",
        "    Do NOT use for:\n",
        "    - Temporary conversation context (working memory handles this)\n",
        "    - Trivial details\n",
        "    - Information that changes frequently\n",
        "    \n",
        "    Examples:\n",
        "    - text=\"Student prefers morning classes\", memory_type=\"semantic\", topics=[\"preferences\", \"schedule\"]\n",
        "    - text=\"Student completed CS101 with grade A\", memory_type=\"episodic\", topics=[\"courses\", \"grades\"]\n",
        "    \"\"\"\n",
        "    try:\n",
        "        await memory_client.create_long_term_memory([ClientMemoryRecord(\n",
        "            text=text,\n",
        "            memory_type=memory_type,\n",
        "            topics=topics if topics else [\"general\"]\n",
        "        )])\n",
        "        return f\"✅ Stored memory: {text}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Failed to store memory: {str(e)}\"\n",
        "\n",
        "print(\"✅ store_memory tool defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tool 2: Search Memories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SearchMemoriesInput(BaseModel):\n",
        "    query: str = Field(description=\"What to search for in memories\")\n",
        "    limit: int = Field(default=5, description=\"Maximum number of memories to retrieve\")\n",
        "\n",
        "@tool(args_schema=SearchMemoriesInput)\n",
        "async def search_memories(query: str, limit: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Search for relevant memories using semantic search.\n",
        "    \n",
        "    Use this tool when:\n",
        "    - You need to recall information about the student\n",
        "    - Student asks \"What do you know about me?\"\n",
        "    - You need context from previous sessions\n",
        "    - Making personalized recommendations\n",
        "    \n",
        "    The search uses semantic matching, so natural language queries work well.\n",
        "    \n",
        "    Examples:\n",
        "    - query=\"student preferences\" → finds preference-related memories\n",
        "    - query=\"completed courses\" → finds course completion records\n",
        "    - query=\"goals\" → finds student's stated goals\n",
        "    \"\"\"\n",
        "    try:\n",
        "        memories = await memory_client.search_long_term_memory(\n",
        "            text=query,\n",
        "            limit=limit\n",
        "        )\n",
        "        \n",
        "        if not memories:\n",
        "            return \"No relevant memories found.\"\n",
        "        \n",
        "        result = f\"Found {len(memories)} relevant memories:\\n\\n\"\n",
        "        for i, memory in enumerate(memories, 1):\n",
        "            result += f\"{i}. {memory.text}\\n\"\n",
        "            result += f\"   Type: {memory.memory_type} | Topics: {', '.join(memory.topics)}\\n\\n\"\n",
        "        \n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"❌ Failed to search memories: {str(e)}\"\n",
        "\n",
        "print(\"✅ search_memories tool defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Memory Tools with an Agent\n",
        "\n",
        "Let's create an agent that uses these memory tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure agent with memory tools\n",
        "memory_tools = [store_memory, search_memories]\n",
        "llm_with_tools = llm.bind_tools(memory_tools)\n",
        "\n",
        "system_prompt = \"\"\"You are a class scheduling agent for Redis University.\n",
        "\n",
        "You have access to memory tools:\n",
        "- store_memory: Store important information about the student\n",
        "- search_memories: Search for information you've stored before\n",
        "\n",
        "Use these tools intelligently:\n",
        "- When students share preferences, goals, or important facts → store them\n",
        "- When you need to recall information → search for it\n",
        "- When making recommendations → search for preferences first\n",
        "\n",
        "Be proactive about using memory to provide personalized service.\n",
        "\"\"\"\n",
        "\n",
        "print(\"✅ Agent configured with memory tools\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Agent Stores a Preference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 1: Agent Stores a Preference\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "user_message = \"I prefer online courses because I work part-time.\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_message)\n",
        "]\n",
        "\n",
        "print(f\"\\n👤 User: {user_message}\")\n",
        "\n",
        "# First response - should call store_memory\n",
        "response = llm_with_tools.invoke(messages)\n",
        "\n",
        "if response.tool_calls:\n",
        "    print(\"\\n🤖 Agent decision: Store this preference\")\n",
        "    for tool_call in response.tool_calls:\n",
        "        print(f\"   Tool: {tool_call['name']}\")\n",
        "        print(f\"   Args: {tool_call['args']}\")\n",
        "        \n",
        "        # Execute the tool\n",
        "        if tool_call['name'] == 'store_memory':\n",
        "            result = await store_memory.ainvoke(tool_call['args'])\n",
        "            print(f\"   Result: {result}\")\n",
        "            \n",
        "            # Add tool result to messages\n",
        "            messages.append(response)\n",
        "            messages.append(ToolMessage(\n",
        "                content=result,\n",
        "                tool_call_id=tool_call['id']\n",
        "            ))\n",
        "    \n",
        "    # Get final response\n",
        "    final_response = llm_with_tools.invoke(messages)\n",
        "    print(f\"\\n🤖 Agent: {final_response.content}\")\n",
        "else:\n",
        "    print(f\"\\n🤖 Agent: {response.content}\")\n",
        "    print(\"\\n⚠️  Agent didn't use store_memory tool\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Agent Searches for Memories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXAMPLE 2: Agent Searches for Memories\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Wait a moment for memory to be stored\n",
        "await asyncio.sleep(1)\n",
        "\n",
        "user_message = \"What courses would you recommend for me?\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_message)\n",
        "]\n",
        "\n",
        "print(f\"\\n👤 User: {user_message}\")\n",
        "\n",
        "# First response - should call search_memories\n",
        "response = llm_with_tools.invoke(messages)\n",
        "\n",
        "if response.tool_calls:\n",
        "    print(\"\\n🤖 Agent decision: Search for preferences first\")\n",
        "    for tool_call in response.tool_calls:\n",
        "        print(f\"   Tool: {tool_call['name']}\")\n",
        "        print(f\"   Args: {tool_call['args']}\")\n",
        "        \n",
        "        # Execute the tool\n",
        "        if tool_call['name'] == 'search_memories':\n",
        "            result = await search_memories.ainvoke(tool_call['args'])\n",
        "            print(f\"\\n   Retrieved memories:\")\n",
        "            print(f\"   {result}\")\n",
        "            \n",
        "            # Add tool result to messages\n",
        "            messages.append(response)\n",
        "            messages.append(ToolMessage(\n",
        "                content=result,\n",
        "                tool_call_id=tool_call['id']\n",
        "            ))\n",
        "    \n",
        "    # Get final response\n",
        "    final_response = llm_with_tools.invoke(messages)\n",
        "    print(f\"\\n🤖 Agent: {final_response.content}\")\n",
        "    print(\"\\n✅ Agent used memories to personalize recommendation!\")\n",
        "else:\n",
        "    print(f\"\\n🤖 Agent: {response.content}\")\n",
        "    print(\"\\n⚠️  Agent didn't search memories\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Multi-Turn Conversation with Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXAMPLE 3: Multi-Turn Conversation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "async def chat_with_memory(user_message, conversation_history):\n",
        "    \"\"\"Helper function for conversation with memory tools.\"\"\"\n",
        "    messages = [SystemMessage(content=system_prompt)]\n",
        "    messages.extend(conversation_history)\n",
        "    messages.append(HumanMessage(content=user_message))\n",
        "    \n",
        "    # Get response\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    \n",
        "    # Handle tool calls\n",
        "    if response.tool_calls:\n",
        "        messages.append(response)\n",
        "        \n",
        "        for tool_call in response.tool_calls:\n",
        "            # Execute tool\n",
        "            if tool_call['name'] == 'store_memory':\n",
        "                result = await store_memory.ainvoke(tool_call['args'])\n",
        "            elif tool_call['name'] == 'search_memories':\n",
        "                result = await search_memories.ainvoke(tool_call['args'])\n",
        "            else:\n",
        "                result = \"Unknown tool\"\n",
        "            \n",
        "            messages.append(ToolMessage(\n",
        "                content=result,\n",
        "                tool_call_id=tool_call['id']\n",
        "            ))\n",
        "        \n",
        "        # Get final response after tool execution\n",
        "        response = llm_with_tools.invoke(messages)\n",
        "    \n",
        "    # Update conversation history\n",
        "    conversation_history.append(HumanMessage(content=user_message))\n",
        "    conversation_history.append(AIMessage(content=response.content))\n",
        "    \n",
        "    return response.content, conversation_history\n",
        "\n",
        "# Have a conversation\n",
        "conversation = []\n",
        "\n",
        "queries = [\n",
        "    \"I'm a junior majoring in Computer Science.\",\n",
        "    \"I want to focus on machine learning and AI.\",\n",
        "    \"What do you know about me so far?\",\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"\\n👤 User: {query}\")\n",
        "    response, conversation = await chat_with_memory(query, conversation)\n",
        "    print(f\"🤖 Agent: {response}\")\n",
        "    await asyncio.sleep(1)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ Agent proactively stored and retrieved memories!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Benefits of Memory Tools\n",
        "\n",
        "✅ **LLM Control:**\n",
        "- Agent decides what's important\n",
        "- Agent decides when to search\n",
        "- More intelligent behavior\n",
        "\n",
        "✅ **Flexibility:**\n",
        "- Can store context-dependent information\n",
        "- Can search on-demand\n",
        "- Can update/delete memories\n",
        "\n",
        "✅ **Transparency:**\n",
        "- You can see when agent stores/searches\n",
        "- Easier to debug\n",
        "- More explainable\n",
        "\n",
        "### When to Use Memory Tools\n",
        "\n",
        "**Use memory tools when:**\n",
        "- ✅ Building advanced, autonomous agents\n",
        "- ✅ Agent needs fine-grained control\n",
        "- ✅ Importance is context-dependent\n",
        "- ✅ Want explicit memory operations\n",
        "\n",
        "**Use automatic extraction when:**\n",
        "- ✅ Simple, consistent extraction is fine\n",
        "- ✅ Want to minimize token usage\n",
        "- ✅ Building straightforward agents\n",
        "\n",
        "**Best practice: Combine both!**\n",
        "- Automatic extraction as baseline\n",
        "- Tools for explicit control\n",
        "\n",
        "### Tool Design Best Practices\n",
        "\n",
        "1. **Clear descriptions** - Explain when to use each tool\n",
        "2. **Good examples** - Show typical usage\n",
        "3. **Error handling** - Handle failures gracefully\n",
        "4. **Feedback** - Return clear success/failure messages\n",
        "\n",
        "### Common Patterns\n",
        "\n",
        "**Store after learning:**\n",
        "```\n",
        "User: \"I prefer online courses\"\n",
        "Agent: [stores memory] \"Got it, I'll remember that!\"\n",
        "```\n",
        "\n",
        "**Search before recommending:**\n",
        "```\n",
        "User: \"What courses should I take?\"\n",
        "Agent: [searches memories] \"Based on your preferences...\"\n",
        "```\n",
        "\n",
        "**Proactive recall:**\n",
        "```\n",
        "User: \"Tell me about CS401\"\n",
        "Agent: [searches memories] \"I remember you're interested in ML...\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Test memory decisions**: Have a 10-turn conversation. Does the agent store and search appropriately?\n",
        "\n",
        "2. **Add update tool**: Create an `update_memory` tool that lets the agent modify existing memories.\n",
        "\n",
        "3. **Compare approaches**: Build two agents - one with automatic extraction, one with tools. Which performs better?\n",
        "\n",
        "4. **Memory strategy**: Design a system prompt that guides the agent on when to use memory tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "- ✅ Memory tools give the LLM control over memory operations\n",
        "- ✅ Agent Memory Server provides built-in memory tools\n",
        "- ✅ Tools enable intelligent, context-aware memory management\n",
        "- ✅ Combine automatic extraction with tools for best results\n",
        "- ✅ Clear tool descriptions guide proper usage\n",
        "\n",
        "**Key insight:** Tool-based memory management enables more sophisticated agents that can decide what to remember and when to recall information. This is especially powerful for autonomous agents that need fine-grained control over their memory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

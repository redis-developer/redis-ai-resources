{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory Tools: Giving the LLM Control Over Memory\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this advanced notebook, you'll learn how to give your agent control over its own memory using tools. Instead of automatically extracting memories, you can let the LLM decide what to remember and when to search for memories. The Agent Memory Server SDK provides built-in memory tools for this.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "- Why give the LLM control over memory\n",
        "- Agent Memory Server's built-in memory tools\n",
        "- How to configure memory tools for your agent\n",
        "- When the LLM decides to store vs. search memories\n",
        "- Best practices for memory-aware agents\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Completed all Section 3 notebooks\n",
        "- Redis 8 running locally\n",
        "- Agent Memory Server running\n",
        "- OpenAI API key set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concepts: Tool-Based Memory Management\n",
        "\n",
        "### Two Approaches to Memory\n",
        "\n",
        "#### 1. Automatic Memory (What We've Been Doing)\n",
        "\n",
        "```python\n",
        "# Agent has conversation\n",
        "# ‚Üí Save working memory\n",
        "# ‚Üí Agent Memory Server automatically extracts important facts\n",
        "# ‚Üí Facts stored in long-term memory\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- ‚úÖ Fully automatic\n",
        "- ‚úÖ No LLM overhead in your application\n",
        "- ‚úÖ Consistent extraction\n",
        "- ‚úÖ Faster - extraction happens in the background after response is sent\n",
        "\n",
        "**Cons:**\n",
        "- ‚ö†Ô∏è Your application's LLM can't directly control what gets extracted\n",
        "- ‚ö†Ô∏è May extract too much or too little\n",
        "- ‚ö†Ô∏è Can't dynamically decide what's important based on conversation context\n",
        "\n",
        "**Note:** You can configure custom extraction prompts on the memory server to guide what gets extracted, but your client application's LLM doesn't have direct control over the extraction process.\n",
        "\n",
        "#### 2. Tool-Based Memory (This Notebook)\n",
        "\n",
        "```python\n",
        "# Agent has conversation\n",
        "# ‚Üí LLM decides: \"This is important, I should remember it\"\n",
        "# ‚Üí LLM calls store_memory tool\n",
        "# ‚Üí Fact stored in long-term memory\n",
        "\n",
        "# Later...\n",
        "# ‚Üí LLM decides: \"I need to know about the user's preferences\"\n",
        "# ‚Üí LLM calls search_memories tool\n",
        "# ‚Üí Retrieves relevant memories\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- ‚úÖ Your application's LLM has full control\n",
        "- ‚úÖ Can decide what's important in real-time\n",
        "- ‚úÖ Can search when needed\n",
        "- ‚úÖ More intelligent, context-aware behavior\n",
        "\n",
        "**Cons:**\n",
        "- ‚ö†Ô∏è Requires tool calls (more tokens)\n",
        "- ‚ö†Ô∏è Slower - tool calls add latency to every response\n",
        "- ‚ö†Ô∏è LLM might forget to store/search\n",
        "- ‚ö†Ô∏è Less consistent\n",
        "\n",
        "### When to Use Tool-Based Memory\n",
        "\n",
        "**Use tool-based memory when:**\n",
        "- ‚úÖ Agent needs fine-grained control\n",
        "- ‚úÖ Importance is context-dependent\n",
        "- ‚úÖ Agent should decide when to search\n",
        "- ‚úÖ Building advanced, autonomous agents\n",
        "\n",
        "**Use automatic memory when:**\n",
        "- ‚úÖ Simple, consistent extraction is fine\n",
        "- ‚úÖ Want to minimize token usage\n",
        "- ‚úÖ Building straightforward agents\n",
        "\n",
        "**Best: Use both!**\n",
        "- Automatic extraction for baseline\n",
        "- Tools for explicit control\n",
        "\n",
        "### Agent Memory Server's Built-in Tools\n",
        "\n",
        "The Agent Memory Server SDK provides:\n",
        "\n",
        "1. **`store_memory`** - Store important information\n",
        "2. **`search_memories`** - Search for relevant memories\n",
        "3. **`update_memory`** - Update existing memories\n",
        "4. **`delete_memory`** - Remove memories\n",
        "\n",
        "These are pre-built, tested, and optimized!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from agent_memory_client import create_memory_client\n",
        "from agent_memory_client.integrations.langchain import get_memory_tools\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "# Initialize\n",
        "student_id = \"student_memory_tools\"\n",
        "session_id = \"tool_demo\"\n",
        "\n",
        "# Initialize memory client using the new async factory\n",
        "base_url = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8000\")\n",
        "memory_client = await create_memory_client(base_url)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "print(f\"‚úÖ Setup complete for {student_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring Agent Memory Server's Memory Tools\n",
        "\n",
        "Let's create tools that wrap the Agent Memory Server's memory operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Getting Memory Tools with LangChain Integration\n",
        "\n",
        "The memory client now has built-in LangChain/LangGraph integration! Just call `get_memory_tools()` and you get ready-to-use LangChain tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get LangChain-compatible memory tools from the client\n",
        "# This returns a list of StructuredTool objects ready to use with LangChain/LangGraph\n",
        "memory_tools = get_memory_tools(\n",
        "    memory_client=memory_client,\n",
        "    session_id=session_id,\n",
        "    user_id=student_id\n",
        ")\n",
        "\n",
        "print(\"Available memory tools:\")\n",
        "for tool in memory_tools:\n",
        "    print(f\"  - {tool.name}: {tool.description[:80]}...\")\n",
        "\n",
        "print(f\"\\n‚úÖ Got {len(memory_tools)} LangChain tools from memory client\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Insight: Built-in LangChain Integration\n",
        "\n",
        "The `get_memory_tools()` function returns LangChain `StructuredTool` objects that:\n",
        "- Work seamlessly with LangChain's `llm.bind_tools()` and LangGraph agents\n",
        "- Handle all the memory client API calls internally\n",
        "- Are pre-configured with your session_id and user_id\n",
        "\n",
        "No manual wrapping needed - just use them like any other LangChain tool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Memory Tools with an Agent\n",
        "\n",
        "Let's create an agent that uses these memory tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure agent with memory tools\n",
        "llm_with_tools = llm.bind_tools(memory_tools)\n",
        "\n",
        "system_prompt = \"\"\"You are a class scheduling agent for Redis University.\n",
        "\n",
        "You have access to memory tools:\n",
        "- create_long_term_memory: Store important information about the student\n",
        "- search_long_term_memory: Search for information you've stored before\n",
        "\n",
        "Use these tools intelligently:\n",
        "- When students share preferences, goals, or important facts ‚Üí store them\n",
        "- When you need to recall information ‚Üí search for it\n",
        "- When making recommendations ‚Üí search for preferences first\n",
        "\n",
        "Be proactive about using memory to provide personalized service.\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚úÖ Agent configured with LangChain memory tools\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Agent Stores a Preference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 1: Agent Stores a Preference\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "user_message = \"I prefer online courses because I work part-time.\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_message)\n",
        "]\n",
        "\n",
        "print(f\"\\nüë§ User: {user_message}\")\n",
        "\n",
        "# First response - should call create_long_term_memory\n",
        "response = llm_with_tools.invoke(messages)\n",
        "\n",
        "if response.tool_calls:\n",
        "    print(\"\\nü§ñ Agent decision: Store this preference\")\n",
        "    for tool_call in response.tool_calls:\n",
        "        print(f\"   Tool: {tool_call['name']}\")\n",
        "        print(f\"   Args: {tool_call['args']}\")\n",
        "        \n",
        "        # Find and execute the tool\n",
        "        tool = next((t for t in memory_tools if t.name == tool_call['name']), None)\n",
        "        if tool:\n",
        "            result = await tool.ainvoke(tool_call['args'])\n",
        "            print(f\"   Result: {result}\")\n",
        "            \n",
        "            # Add tool result to messages\n",
        "            messages.append(response)\n",
        "            messages.append(ToolMessage(\n",
        "                content=str(result),\n",
        "                tool_call_id=tool_call['id']\n",
        "            ))\n",
        "    \n",
        "    # Get final response\n",
        "    final_response = llm_with_tools.invoke(messages)\n",
        "    print(f\"\\nü§ñ Agent: {final_response.content}\")\n",
        "else:\n",
        "    print(f\"\\nü§ñ Agent: {response.content}\")\n",
        "    print(\"\\n‚ö†Ô∏è  Agent didn't use memory tool\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Agent Searches for Memories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXAMPLE 2: Agent Searches for Memories\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Wait a moment for memory to be stored\n",
        "await asyncio.sleep(1)\n",
        "\n",
        "user_message = \"What courses would you recommend for me?\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_message)\n",
        "]\n",
        "\n",
        "print(f\"\\nüë§ User: {user_message}\")\n",
        "\n",
        "# First response - should call search_long_term_memory\n",
        "response = llm_with_tools.invoke(messages)\n",
        "\n",
        "if response.tool_calls:\n",
        "    print(\"\\nü§ñ Agent decision: Search for preferences first\")\n",
        "    for tool_call in response.tool_calls:\n",
        "        print(f\"   Tool: {tool_call['name']}\")\n",
        "        print(f\"   Args: {tool_call['args']}\")\n",
        "        \n",
        "        # Find and execute the tool\n",
        "        tool = next((t for t in memory_tools if t.name == tool_call['name']), None)\n",
        "        if tool:\n",
        "            result = await tool.ainvoke(tool_call['args'])\n",
        "            print(f\"\\n   Retrieved memories:\")\n",
        "            print(f\"   {result}\")\n",
        "            \n",
        "            # Add tool result to messages\n",
        "            messages.append(response)\n",
        "            messages.append(ToolMessage(\n",
        "                content=str(result),\n",
        "                tool_call_id=tool_call['id']\n",
        "            ))\n",
        "    \n",
        "    # Get final response\n",
        "    final_response = llm_with_tools.invoke(messages)\n",
        "    print(f\"\\nü§ñ Agent: {final_response.content}\")\n",
        "    print(\"\\n‚úÖ Agent used memories to personalize recommendation!\")\n",
        "else:\n",
        "    print(f\"\\nü§ñ Agent: {response.content}\")\n",
        "    print(\"\\n‚ö†Ô∏è  Agent didn't search memories\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Multi-Turn Conversation with Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXAMPLE 3: Multi-Turn Conversation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "async def chat_with_memory(user_message, conversation_history):\n",
        "    \"\"\"Helper function for conversation with memory tools.\"\"\"\n",
        "    messages = [SystemMessage(content=system_prompt)]\n",
        "    messages.extend(conversation_history)\n",
        "    messages.append(HumanMessage(content=user_message))\n",
        "    \n",
        "    # Get response\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    \n",
        "    # Handle tool calls\n",
        "    if response.tool_calls:\n",
        "        messages.append(response)\n",
        "        \n",
        "        for tool_call in response.tool_calls:\n",
        "            # Execute tool\n",
        "            if tool_call['name'] == 'store_memory':\n",
        "                result = await store_memory.ainvoke(tool_call['args'])\n",
        "            elif tool_call['name'] == 'search_memories':\n",
        "                result = await search_memories.ainvoke(tool_call['args'])\n",
        "            else:\n",
        "                result = \"Unknown tool\"\n",
        "            \n",
        "            messages.append(ToolMessage(\n",
        "                content=result,\n",
        "                tool_call_id=tool_call['id']\n",
        "            ))\n",
        "        \n",
        "        # Get final response after tool execution\n",
        "        response = llm_with_tools.invoke(messages)\n",
        "    \n",
        "    # Update conversation history\n",
        "    conversation_history.append(HumanMessage(content=user_message))\n",
        "    conversation_history.append(AIMessage(content=response.content))\n",
        "    \n",
        "    return response.content, conversation_history\n",
        "\n",
        "# Have a conversation\n",
        "conversation = []\n",
        "\n",
        "queries = [\n",
        "    \"I'm a junior majoring in Computer Science.\",\n",
        "    \"I want to focus on machine learning and AI.\",\n",
        "    \"What do you know about me so far?\",\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"\\nüë§ User: {query}\")\n",
        "    response, conversation = await chat_with_memory(query, conversation)\n",
        "    print(f\"ü§ñ Agent: {response}\")\n",
        "    await asyncio.sleep(1)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ Agent proactively stored and retrieved memories!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Benefits of Memory Tools\n",
        "\n",
        "‚úÖ **LLM Control:**\n",
        "- Agent decides what's important\n",
        "- Agent decides when to search\n",
        "- More intelligent behavior\n",
        "\n",
        "‚úÖ **Flexibility:**\n",
        "- Can store context-dependent information\n",
        "- Can search on-demand\n",
        "- Can update/delete memories\n",
        "\n",
        "‚úÖ **Transparency:**\n",
        "- You can see when agent stores/searches\n",
        "- Easier to debug\n",
        "- More explainable\n",
        "\n",
        "### When to Use Memory Tools\n",
        "\n",
        "**Use memory tools when:**\n",
        "- ‚úÖ Building advanced, autonomous agents\n",
        "- ‚úÖ Agent needs fine-grained control\n",
        "- ‚úÖ Importance is context-dependent\n",
        "- ‚úÖ Want explicit memory operations\n",
        "\n",
        "**Use automatic extraction when:**\n",
        "- ‚úÖ Simple, consistent extraction is fine\n",
        "- ‚úÖ Want to minimize token usage\n",
        "- ‚úÖ Building straightforward agents\n",
        "\n",
        "**Best practice: Combine both!**\n",
        "- Automatic extraction as baseline\n",
        "- Tools for explicit control\n",
        "\n",
        "### Tool Design Best Practices\n",
        "\n",
        "1. **Clear descriptions** - Explain when to use each tool\n",
        "2. **Good examples** - Show typical usage\n",
        "3. **Error handling** - Handle failures gracefully\n",
        "4. **Feedback** - Return clear success/failure messages\n",
        "\n",
        "### Common Patterns\n",
        "\n",
        "**Store after learning:**\n",
        "```\n",
        "User: \"I prefer online courses\"\n",
        "Agent: [stores memory] \"Got it, I'll remember that!\"\n",
        "```\n",
        "\n",
        "**Search before recommending:**\n",
        "```\n",
        "User: \"What courses should I take?\"\n",
        "Agent: [searches memories] \"Based on your preferences...\"\n",
        "```\n",
        "\n",
        "**Proactive recall:**\n",
        "```\n",
        "User: \"Tell me about CS401\"\n",
        "Agent: [searches memories] \"I remember you're interested in ML...\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Test memory decisions**: Have a 10-turn conversation. Does the agent store and search appropriately?\n",
        "\n",
        "2. **Add update tool**: Create an `update_memory` tool that lets the agent modify existing memories.\n",
        "\n",
        "3. **Compare approaches**: Build two agents - one with automatic extraction, one with tools. Which performs better?\n",
        "\n",
        "4. **Memory strategy**: Design a system prompt that guides the agent on when to use memory tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "- ‚úÖ Memory tools give the LLM control over memory operations\n",
        "- ‚úÖ Agent Memory Server provides built-in memory tools\n",
        "- ‚úÖ Tools enable intelligent, context-aware memory management\n",
        "- ‚úÖ Combine automatic extraction with tools for best results\n",
        "- ‚úÖ Clear tool descriptions guide proper usage\n",
        "\n",
        "**Key insight:** Tool-based memory management enables more sophisticated agents that can decide what to remember and when to recall information. This is especially powerful for autonomous agents that need fine-grained control over their memory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

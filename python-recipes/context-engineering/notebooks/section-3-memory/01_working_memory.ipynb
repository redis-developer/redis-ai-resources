{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Working Memory\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to implement working memory, which is session-scoped data that persists across multiple turns of a conversation. Working memory stores conversation messages and task-related context, giving LLMs the knowledge they need to maintain coherent, context-aware conversations.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Working Memory**: Persistent storage for current conversation messages and task-specific context\n",
    "- **Long-term Memory**: Cross-session knowledge (user preferences, important facts learned over time)\n",
    "- **Session Scope**: Working memory is tied to a specific conversation session\n",
    "- **Message History**: The sequence of user and assistant messages that form the conversation\n",
    "\n",
    "### The Problem We're Solving\n",
    "\n",
    "LLMs are stateless - they don't inherently remember previous messages in a conversation. Working memory solves this by:\n",
    "- Storing conversation messages so the LLM can reference earlier parts of the conversation\n",
    "- Maintaining task-specific context (like current goals, preferences mentioned in this session)\n",
    "- Persisting this information across multiple turns of the conversation\n",
    "- Providing a foundation for extracting important information to long-term storage\n",
    "\n",
    "Because working memory stores messages, we can extract long-term data from it. When using the Agent Memory Server, extraction happens automatically in the background based on a configured strategy that controls what kind of information gets extracted."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:01:24.609615Z",
     "start_time": "2025-10-02T22:01:21.200949Z"
    }
   },
   "source": [
    "# Install the Redis Context Course package\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Install the package in development mode\n",
    "package_path = \"../../reference-agent\"\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", package_path], \n",
    "                      capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"âœ… Package installed successfully\")\n",
    "else:\n",
    "    print(f\"âŒ Package installation failed: {result.stderr}\")\n",
    "    raise RuntimeError(f\"Failed to install package: {result.stderr}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Package installed successfully\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:01:28.046925Z",
     "start_time": "2025-10-02T22:01:28.044504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required environment variables are set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not found. Please create a .env file with your OpenAI API key. \"\n",
    "        \"See SETUP.md for instructions.\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… Environment variables loaded\")\n",
    "print(f\"   REDIS_URL: {os.getenv('REDIS_URL', 'redis://localhost:6379')}\")\n",
    "print(f\"   AGENT_MEMORY_URL: {os.getenv('AGENT_MEMORY_URL', 'http://localhost:8000')}\")\n",
    "print(f\"   OPENAI_API_KEY: {'âœ“ Set' if os.getenv('OPENAI_API_KEY') else 'âœ— Not set'}\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Working Memory Structure\n",
    "\n",
    "Working memory contains the essential context for the current conversation:\n",
    "\n",
    "- **Messages**: The conversation history (user and assistant messages)\n",
    "- **Session ID**: Identifies this specific conversation\n",
    "- **User ID**: Identifies the user across sessions\n",
    "- **Task Data**: Optional task-specific context (current goals, temporary state)\n",
    "\n",
    "This structure gives the LLM everything it needs to understand the current conversation context.\n",
    "\n",
    "Let's import the memory client to work with working memory:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:01:32.779633Z",
     "start_time": "2025-10-02T22:01:32.776671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from redis_context_course import MemoryClient\n",
    "\n",
    "print(\"âœ… Memory server client imported successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory server client imported successfully\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Storing and Retrieving Conversation Context\n",
    "\n",
    "Let's see how working memory stores and retrieves conversation context:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:01:39.218627Z",
     "start_time": "2025-10-02T22:01:39.167246Z"
    }
   },
   "source": [
    "import os\n",
    "from agent_memory_client import MemoryClientConfig\n",
    "\n",
    "# Initialize memory client for working memory\n",
    "student_id = \"demo_student_working_memory\"\n",
    "session_id = \"session_001\"\n",
    "config = MemoryClientConfig(\n",
    "    base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8000\"),\n",
    "    default_namespace=\"redis_university\"\n",
    ")\n",
    "memory_client = MemoryClient(config=config)\n",
    "\n",
    "print(\"âœ… Memory client initialized successfully\")\n",
    "print(f\"ðŸ“Š User ID: {student_id}\")\n",
    "print(f\"ðŸ“Š Session ID: {session_id}\")\n",
    "print(\"\\nWorking memory will store conversation messages for this session.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory client initialized successfully\n",
      "ðŸ“Š User ID: demo_student_working_memory\n",
      "ðŸ“Š Session ID: session_001\n",
      "\n",
      "Working memory will store conversation messages for this session.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:01:47.863402Z",
     "start_time": "2025-10-02T22:01:47.590762Z"
    }
   },
   "source": [
    "# Simulate a conversation using working memory\n",
    "\n",
    "print(\"ðŸ’¬ Simulating Conversation with Working Memory\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create messages for the conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I prefer online courses because I work part-time\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I understand you prefer online courses due to your work schedule.\"},\n",
    "    {\"role\": \"user\", \"content\": \"My goal is to specialize in machine learning\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Machine learning is an excellent specialization!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What courses do you recommend?\"},\n",
    "]\n",
    "\n",
    "# Save to working memory\n",
    "from agent_memory_client.models import WorkingMemory, MemoryMessage\n",
    "\n",
    "# Convert messages to MemoryMessage format\n",
    "memory_messages = [MemoryMessage(**msg) for msg in messages]\n",
    "\n",
    "# Create WorkingMemory object\n",
    "working_memory = WorkingMemory(\n",
    "    session_id=session_id,\n",
    "    user_id=student_id,\n",
    "    messages=memory_messages,\n",
    "    memories=[],\n",
    "    data={}\n",
    ")\n",
    "\n",
    "await memory_client.put_working_memory(\n",
    "    session_id=session_id,\n",
    "    memory=working_memory,\n",
    "    user_id=student_id,\n",
    "    model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Conversation saved to working memory\")\n",
    "print(f\"ðŸ“Š Messages: {len(messages)}\")\n",
    "print(\"\\nThese messages are now available as context for the LLM.\")\n",
    "print(\"The LLM can reference earlier parts of the conversation.\")\n",
    "\n",
    "# Retrieve working memory\n",
    "_, working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id=session_id,\n",
    "    model_name=\"gpt-4o\",\n",
    "    user_id=student_id,\n",
    ")\n",
    "\n",
    "if working_memory:\n",
    "    print(f\"\\nðŸ“‹ Retrieved {len(working_memory.messages)} messages from working memory\")\n",
    "    print(\"This is the conversation context that would be provided to the LLM.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Simulating Conversation with Working Memory\n",
      "==================================================\n",
      "15:01:47 httpx INFO   HTTP Request: PUT http://localhost:8000/v1/working-memory/session_001?user_id=demo_student_working_memory&model_name=gpt-4o \"HTTP/1.1 500 Internal Server Error\"\n"
     ]
    },
    {
     "ename": "MemoryServerError",
     "evalue": "HTTP 500: dial tcp [::1]:8000: connect: connection refused\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/src/redis-ai-resources/env/lib/python3.11/site-packages/agent_memory_client/client.py:457\u001B[0m, in \u001B[0;36mMemoryAPIClient.put_working_memory\u001B[0;34m(self, session_id, memory, user_id, model_name, context_window_max)\u001B[0m\n\u001B[1;32m    452\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mput(\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/v1/working-memory/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msession_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    454\u001B[0m     json\u001B[38;5;241m=\u001B[39mmemory\u001B[38;5;241m.\u001B[39mmodel_dump(exclude_none\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    455\u001B[0m     params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[1;32m    456\u001B[0m )\n\u001B[0;32m--> 457\u001B[0m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m WorkingMemoryResponse(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse\u001B[38;5;241m.\u001B[39mjson())\n",
      "File \u001B[0;32m~/src/redis-ai-resources/env/lib/python3.11/site-packages/httpx/_models.py:829\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    828\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[0;32m--> 829\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m: Server error '500 Internal Server Error' for url 'http://localhost:8000/v1/working-memory/session_001?user_id=demo_student_working_memory&model_name=gpt-4o'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mMemoryServerError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 30\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Create WorkingMemory object\u001B[39;00m\n\u001B[1;32m     22\u001B[0m working_memory \u001B[38;5;241m=\u001B[39m WorkingMemory(\n\u001B[1;32m     23\u001B[0m     session_id\u001B[38;5;241m=\u001B[39msession_id,\n\u001B[1;32m     24\u001B[0m     user_id\u001B[38;5;241m=\u001B[39mstudent_id,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m     data\u001B[38;5;241m=\u001B[39m{}\n\u001B[1;32m     28\u001B[0m )\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m memory_client\u001B[38;5;241m.\u001B[39mput_working_memory(\n\u001B[1;32m     31\u001B[0m     session_id\u001B[38;5;241m=\u001B[39msession_id,\n\u001B[1;32m     32\u001B[0m     memory\u001B[38;5;241m=\u001B[39mworking_memory,\n\u001B[1;32m     33\u001B[0m     user_id\u001B[38;5;241m=\u001B[39mstudent_id,\n\u001B[1;32m     34\u001B[0m     model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-4o\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     35\u001B[0m )\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mâœ… Conversation saved to working memory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mðŸ“Š Messages: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(messages)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/src/redis-ai-resources/env/lib/python3.11/site-packages/agent_memory_client/client.py:460\u001B[0m, in \u001B[0;36mMemoryAPIClient.put_working_memory\u001B[0;34m(self, session_id, memory, user_id, model_name, context_window_max)\u001B[0m\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m WorkingMemoryResponse(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse\u001B[38;5;241m.\u001B[39mjson())\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_http_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/src/redis-ai-resources/env/lib/python3.11/site-packages/agent_memory_client/client.py:167\u001B[0m, in \u001B[0;36mMemoryAPIClient._handle_http_error\u001B[0;34m(self, response)\u001B[0m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    166\u001B[0m         message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHTTP \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MemoryServerError(message, response\u001B[38;5;241m.\u001B[39mstatus_code)\n\u001B[1;32m    168\u001B[0m \u001B[38;5;66;03m# This should never be reached, but mypy needs to know this never returns\u001B[39;00m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m MemoryServerError(\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected status code: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, response\u001B[38;5;241m.\u001B[39mstatus_code\n\u001B[1;32m    171\u001B[0m )\n",
      "\u001B[0;31mMemoryServerError\u001B[0m: HTTP 500: dial tcp [::1]:8000: connect: connection refused\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Automatic Extraction to Long-Term Memory\n",
    "\n",
    "Because working memory stores messages, we can extract important long-term information from it. When using the Agent Memory Server, this extraction happens automatically in the background.\n",
    "\n",
    "The extraction strategy controls what kind of information gets extracted:\n",
    "- User preferences (e.g., \"I prefer online courses\")\n",
    "- Goals (e.g., \"I want to specialize in machine learning\")\n",
    "- Important facts (e.g., \"I work part-time\")\n",
    "- Key decisions or outcomes from the conversation\n",
    "\n",
    "This extracted information becomes long-term memory that persists across sessions.\n",
    "\n",
    "Let's check what information was automatically extracted from our working memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what was extracted to long-term memory\n",
    "import asyncio\n",
    "from agent_memory_client import MemoryAPIClient as MemoryClient, MemoryClientConfig\n",
    "\n",
    "# Ensure memory_client is defined (in case cells are run out of order)\n",
    "if 'memory_client' not in globals():\n",
    "    # Initialize memory client with proper config\n",
    "    import os\n",
    "    config = MemoryClientConfig(\n",
    "        base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8000\"),\n",
    "        default_namespace=\"redis_university\"\n",
    "    )\n",
    "    memory_client = MemoryClient(config=config)\n",
    "\n",
    "await asyncio.sleep(2)  # Give the extraction process time to complete\n",
    "\n",
    "# Search for extracted memories\n",
    "extracted_memories = await memory_client.search_long_term_memory(\n",
    "    text=\"preferences goals\",\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  Extracted to Long-term Memory\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if extracted_memories.memories:\n",
    "    for i, memory in enumerate(extracted_memories.memories, 1):\n",
    "        print(f\"{i}. {memory.text}\")\n",
    "        print(f\"   Type: {memory.memory_type} | Topics: {', '.join(memory.topics)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No memories extracted yet (extraction may take a moment)\")\n",
    "    print(\"\\nThe Agent Memory Server automatically extracts:\")\n",
    "    print(\"- User preferences (e.g., 'prefers online courses')\")\n",
    "    print(\"- Goals (e.g., 'wants to specialize in machine learning')\")\n",
    "    print(\"- Important facts (e.g., 'works part-time')\")\n",
    "    print(\"\\nThis happens in the background based on the configured extraction strategy.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "- âœ… **The Core Problem**: LLMs are stateless and need working memory to maintain conversation context\n",
    "- âœ… **Working Memory Solution**: Stores messages and task-specific context for the current session\n",
    "- âœ… **Message Storage**: Conversation history gives the LLM knowledge of what was said earlier\n",
    "- âœ… **Automatic Extraction**: Important information is extracted to long-term memory in the background\n",
    "- âœ… **Extraction Strategy**: Controls what kind of information gets extracted from working memory\n",
    "\n",
    "**Key API Methods:**\n",
    "```python\n",
    "# Save working memory (stores messages for this session)\n",
    "await memory_client.put_working_memory(session_id, memory, user_id, model_name)\n",
    "\n",
    "# Retrieve working memory (gets conversation context)\n",
    "_, working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id, model_name, user_id\n",
    ")\n",
    "\n",
    "# Search long-term memories (extracted from working memory)\n",
    "memories = await memory_client.search_long_term_memory(text, limit)\n",
    "```\n",
    "\n",
    "**The Key Insight:**\n",
    "Working memory solves the fundamental problem of giving LLMs knowledge of the current conversation. Because it stores messages, we can also extract long-term data from it. The extraction strategy controls what gets extracted, and this happens automatically in the background when using the Agent Memory Server.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "See the next notebooks to learn about:\n",
    "- Long-term memory and how it persists across sessions\n",
    "- Memory tools that give LLMs explicit control over what gets remembered\n",
    "- Integrating working and long-term memory in your applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

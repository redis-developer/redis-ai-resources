{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# ðŸ¤– Building a Course Advisor Agent\n",
    "\n",
    "**â±ï¸ Estimated Time:** 60-75 minutes\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Build** a complete LangGraph agent with tools and memory\n",
    "2. **Implement** exactly 3 tools: memory storage, memory search, and course search\n",
    "3. **Integrate** Redis Agent Memory Server for dual-memory architecture\n",
    "4. **Visualize** the agent's decision-making graph\n",
    "5. **Demonstrate** the progression from RAG (Section 3) to full agent\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Bridge from Previous Sections\n",
    "\n",
    "### **Your Learning Journey:**\n",
    "\n",
    "**Section 1:** Context Types\n",
    "- System, User, Conversation, Retrieved context\n",
    "- How context shapes LLM responses\n",
    "\n",
    "**Section 2:** RAG Foundations\n",
    "- Semantic search with vector embeddings\n",
    "- Retrieving and presenting information\n",
    "- Single-step retrieval â†’ generation\n",
    "\n",
    "**Section 3:** Memory Architecture\n",
    "- Working memory (conversation continuity)\n",
    "- Long-term memory (persistent knowledge)\n",
    "- Memory-enhanced RAG systems\n",
    "\n",
    "**Section 4 (Notebook 1):** Tool-Calling Basics\n",
    "- What tools are and how LLMs use them\n",
    "- LangGraph fundamentals (nodes, edges, state)\n",
    "- Simple tool-calling examples\n",
    "- Agents vs RAG comparison\n",
    "\n",
    "### **What We're Building Now:**\n",
    "\n",
    "**A Full Agent** that combines everything:\n",
    "- âœ… **Tools** for actions (search courses, manage memory)\n",
    "- âœ… **Memory** for personalization (working + long-term)\n",
    "- âœ… **RAG** for course information (semantic search)\n",
    "- âœ… **LangGraph** for orchestration (state management)\n",
    "\n",
    "**ðŸ’¡ Key Insight:** This agent is RAG + Memory + Tools + Decision-Making\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Agent Architecture\n",
    "\n",
    "### **The Complete Flow:**\n",
    "\n",
    "```\n",
    "User Query\n",
    "    â†“\n",
    "[Load Working Memory] â† Conversation history\n",
    "    â†“\n",
    "[Agent Node] â† Decides what to do\n",
    "    â†“\n",
    "    â”œâ”€â†’ [search_courses] â† Find relevant courses\n",
    "    â”œâ”€â†’ [search_memories] â† Recall user preferences\n",
    "    â”œâ”€â†’ [store_memory] â† Save important facts\n",
    "    â†“\n",
    "[Agent Node] â† Processes tool results\n",
    "    â†“\n",
    "[Generate Response] â† Final answer\n",
    "    â†“\n",
    "[Save Working Memory] â† Update conversation\n",
    "```\n",
    "\n",
    "### **Our 3 Tools:**\n",
    "\n",
    "1. **`search_courses`** - Semantic search over course catalog\n",
    "   - When: Student asks about courses, topics, or recommendations\n",
    "   - Example: \"What machine learning courses are available?\"\n",
    "\n",
    "2. **`search_memories`** - Search long-term memory for user facts\n",
    "   - When: Need to recall preferences, goals, or past interactions\n",
    "   - Example: \"What courses did I say I was interested in?\"\n",
    "\n",
    "3. **`store_memory`** - Save important information to long-term memory\n",
    "   - When: User shares preferences, goals, or important facts\n",
    "   - Example: \"I'm interested in AI and want to work at a startup\"\n",
    "\n",
    "### **Memory Architecture:**\n",
    "\n",
    "| Memory Type | Purpose | Managed By | Lifespan |\n",
    "|------------|---------|------------|----------|\n",
    "| **Working Memory** | Conversation history | Agent Memory Server | Session |\n",
    "| **Long-term Memory** | User preferences, facts | Agent Memory Server | Persistent |\n",
    "| **Graph State** | Current execution state | LangGraph | Single turn |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Setup and Environment\n",
    "\n",
    "### âš ï¸ **CRITICAL: Prerequisites Required**\n",
    "\n",
    "**This notebook requires ALL services to be running. If any service is down, the agent will not work.**\n",
    "\n",
    "**Required Services:**\n",
    "1. **Redis** - Vector storage and caching (port 6379)\n",
    "2. **Agent Memory Server** - Memory management (port 8088)\n",
    "3. **OpenAI API** - LLM functionality\n",
    "\n",
    "**ðŸš€ Quick Setup (Run this first!):**\n",
    "```bash\n",
    "# Navigate to notebooks_v2 directory\n",
    "cd ../../\n",
    "\n",
    "# Check if services are running\n",
    "./check_setup.sh\n",
    "\n",
    "# If services are down, run setup\n",
    "./setup_memory_server.sh\n",
    "```\n",
    "\n",
    "**ðŸ“– Need help?** See `../SETUP_GUIDE.md` for detailed setup instructions.\n",
    "\n",
    "**ðŸ” Manual Check:**\n",
    "- Redis: `redis-cli ping` should return `PONG`\n",
    "- Memory Server: `curl http://localhost:8088/v1/health` should return `{\"status\":\"ok\"}`\n",
    "- Environment: Create `.env` file in `reference-agent/` with your `OPENAI_API_KEY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-packages",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "### Automated Setup Check\n",
    "\n",
    "Let's run the setup script to ensure all services are running properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libraries",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:51.825255Z",
     "iopub.status.busy": "2025-10-31T23:57:51.825073Z",
     "iopub.status.idle": "2025-10-31T23:57:52.103012Z",
     "shell.execute_reply": "2025-10-31T23:57:52.102484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running automated setup check...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Agent Memory Server Setup\n",
      "===========================\n",
      "ðŸ“Š Checking Redis...\n",
      "âœ… Redis is running\n",
      "ðŸ“Š Checking Agent Memory Server...\n",
      "ðŸ” Agent Memory Server container exists. Checking health...\n",
      "âœ… Agent Memory Server is running and healthy\n",
      "âœ… No Redis connection issues detected\n",
      "\n",
      "âœ… Setup Complete!\n",
      "=================\n",
      "ðŸ“Š Services Status:\n",
      "   â€¢ Redis: Running on port 6379\n",
      "   â€¢ Agent Memory Server: Running on port 8088\n",
      "\n",
      "ðŸŽ¯ You can now run the notebooks!\n",
      "\n",
      "\n",
      "âœ… All services are ready!\n"
     ]
    }
   ],
   "source": [
    "# Run the setup script to ensure Redis and Agent Memory Server are running\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to setup script\n",
    "setup_script = Path(\"../../reference-agent/setup_agent_memory_server.py\")\n",
    "\n",
    "if setup_script.exists():\n",
    "    print(\"Running automated setup check...\\n\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(setup_script)], capture_output=True, text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"âš ï¸  Setup check failed. Please review the output above.\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"\\nâœ… All services are ready!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Setup script not found. Please ensure services are running manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-env",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already installed the reference-agent package, uncomment and run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:52.104763Z",
     "iopub.status.busy": "2025-10-31T23:57:52.104657Z",
     "iopub.status.idle": "2025-10-31T23:57:52.106517Z",
     "shell.execute_reply": "2025-10-31T23:57:52.106037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install reference-agent package\n",
    "# %pip install -q -e ../../reference-agent\n",
    "\n",
    "# Uncomment to install agent-memory-client\n",
    "# %pip install -q agent-memory-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-services",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "service-check",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:52.107702Z",
     "iopub.status.busy": "2025-10-31T23:57:52.107645Z",
     "iopub.status.idle": "2025-10-31T23:57:53.822487Z",
     "shell.execute_reply": "2025-10-31T23:57:53.821994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Annotated, Any, Dict, List, Optional\n",
    "\n",
    "# Redis and Agent Memory\n",
    "from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "from agent_memory_client.models import MemoryMessage, WorkingMemory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain and LangGraph\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Add reference-agent to path for course utilities\n",
    "sys.path.insert(0, os.path.abspath(\"../../reference-agent\"))\n",
    "from redis_context_course.course_manager import CourseManager\n",
    "from redis_context_course.models import CourseFormat, DifficultyLevel, StudentProfile\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-components",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "init-course-manager",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.823677Z",
     "iopub.status.busy": "2025-10-31T23:57:53.823553Z",
     "iopub.status.idle": "2025-10-31T23:57:53.826253Z",
     "shell.execute_reply": "2025-10-31T23:57:53.825901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured successfully!\n",
      "   OpenAI API Key: ********************wTMA\n",
      "   Redis URL: redis://localhost:6379\n",
      "   Agent Memory URL: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(\"../../reference-agent/.env\")\n",
    "\n",
    "# Get configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "\n",
    "# Verify OpenAI API key\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"\"\"\n",
    "    âš ï¸  OPENAI_API_KEY not found!\n",
    "\n",
    "    Please create a .env file in the reference-agent directory:\n",
    "    1. cd ../../reference-agent\n",
    "    2. cp .env.example .env\n",
    "    3. Edit .env and add your OpenAI API key\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… Environment configured successfully!\")\n",
    "print(f\"   OpenAI API Key: {'*' * 20}{OPENAI_API_KEY[-4:]}\")\n",
    "print(f\"   Redis URL: {REDIS_URL}\")\n",
    "print(f\"   Agent Memory URL: {AGENT_MEMORY_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "course-manager",
   "metadata": {},
   "source": [
    "### Check Required Services\n",
    "\n",
    "Let's verify that Redis and the Agent Memory Server are running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "init-llm",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.827385Z",
     "iopub.status.busy": "2025-10-31T23:57:53.827318Z",
     "iopub.status.idle": "2025-10-31T23:57:53.839615Z",
     "shell.execute_reply": "2025-10-31T23:57:53.839213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Redis is running\n",
      "âœ… Agent Memory Server is running\n",
      "\n",
      "âœ… All services are ready!\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import requests\n",
    "\n",
    "# Check Redis\n",
    "try:\n",
    "    redis_client = redis.from_url(REDIS_URL)\n",
    "    redis_client.ping()\n",
    "    print(\"âœ… Redis is running\")\n",
    "    REDIS_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Redis is not available: {e}\")\n",
    "    print(\"   Please start Redis using Docker:\")\n",
    "    print(\"   docker run -d -p 6379:6379 redis/redis-stack:latest\")\n",
    "    REDIS_AVAILABLE = False\n",
    "\n",
    "# Check Agent Memory Server\n",
    "try:\n",
    "    response = requests.get(f\"{AGENT_MEMORY_URL}/v1/health\", timeout=2)\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ… Agent Memory Server is running\")\n",
    "        MEMORY_SERVER_AVAILABLE = True\n",
    "    else:\n",
    "        print(f\"âš ï¸  Agent Memory Server returned status {response.status_code}\")\n",
    "        MEMORY_SERVER_AVAILABLE = False\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Agent Memory Server is not available: {e}\")\n",
    "    print(\"   Please start the Agent Memory Server:\")\n",
    "    print(\"   cd ../../reference-agent && python setup_agent_memory_server.py\")\n",
    "    MEMORY_SERVER_AVAILABLE = False\n",
    "\n",
    "if not (REDIS_AVAILABLE and MEMORY_SERVER_AVAILABLE):\n",
    "    print(\"\\nâš ï¸  Some services are not available. Please start them before continuing.\")\n",
    "else:\n",
    "    print(\"\\nâœ… All services are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-init",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Initialize Components\n",
    "\n",
    "Now let's initialize the components we'll use to build our agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-memory",
   "metadata": {},
   "source": [
    "### Initialize Course Manager\n",
    "\n",
    "The `CourseManager` handles course storage and semantic search, just like in Section 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "memory-init",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.840793Z",
     "iopub.status.busy": "2025-10-31T23:57:53.840727Z",
     "iopub.status.idle": "2025-10-31T23:57:53.933415Z",
     "shell.execute_reply": "2025-10-31T23:57:53.933012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:57:53 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Course Manager initialized\n",
      "   Ready to search and retrieve courses\n"
     ]
    }
   ],
   "source": [
    "# Initialize Course Manager\n",
    "course_manager = CourseManager()\n",
    "\n",
    "print(\"âœ… Course Manager initialized\")\n",
    "print(\"   Ready to search and retrieve courses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-profile",
   "metadata": {},
   "source": [
    "### Initialize LLM\n",
    "\n",
    "We'll use GPT-4o with temperature=0.0 for consistent, deterministic responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "create-student",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.934684Z",
     "iopub.status.busy": "2025-10-31T23:57:53.934605Z",
     "iopub.status.idle": "2025-10-31T23:57:53.943986Z",
     "shell.execute_reply": "2025-10-31T23:57:53.943698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM initialized\n",
      "   Model: gpt-4o\n",
      "   Temperature: 0.0 (deterministic)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "print(\"âœ… LLM initialized\")\n",
    "print(\"   Model: gpt-4o\")\n",
    "print(\"   Temperature: 0.0 (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-section",
   "metadata": {},
   "source": [
    "### Initialize Memory Client\n",
    "\n",
    "The memory client handles both working memory (conversation history) and long-term memory (persistent facts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tool-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.945184Z",
     "iopub.status.busy": "2025-10-31T23:57:53.945115Z",
     "iopub.status.idle": "2025-10-31T23:57:53.950020Z",
     "shell.execute_reply": "2025-10-31T23:57:53.949643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory Client initialized\n",
      "   Base URL: http://localhost:8088\n",
      "   Namespace: redis_university\n",
      "   Ready for working memory and long-term memory operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Memory Client\n",
    "config = MemoryClientConfig(\n",
    "    base_url=AGENT_MEMORY_URL, default_namespace=\"redis_university\"\n",
    ")\n",
    "memory_client = MemoryAPIClient(config=config)\n",
    "\n",
    "print(\"âœ… Memory Client initialized\")\n",
    "print(f\"   Base URL: {config.base_url}\")\n",
    "print(f\"   Namespace: {config.default_namespace}\")\n",
    "print(\"   Ready for working memory and long-term memory operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-courses-tool",
   "metadata": {},
   "source": [
    "### Create Sample Student Profile\n",
    "\n",
    "We'll create a sample student to use throughout our demos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tool-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.951077Z",
     "iopub.status.busy": "2025-10-31T23:57:53.951016Z",
     "iopub.status.idle": "2025-10-31T23:57:53.953293Z",
     "shell.execute_reply": "2025-10-31T23:57:53.952950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Student profile created\n",
      "   Name: Sarah Chen\n",
      "   Student ID: student_sarah_001\n",
      "   Session ID: session_student_sarah_001_20251031_195753\n",
      "   Major: Computer Science\n",
      "   Interests: machine learning, data science, algorithms\n"
     ]
    }
   ],
   "source": [
    "# Create sample student profile\n",
    "STUDENT_ID = \"student_sarah_001\"\n",
    "SESSION_ID = f\"session_{STUDENT_ID}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "sarah = StudentProfile(\n",
    "    name=\"Sarah Chen\",\n",
    "    email=\"sarah.chen@university.edu\",\n",
    "    major=\"Computer Science\",\n",
    "    year=2,\n",
    "    interests=[\"machine learning\", \"data science\", \"algorithms\"],\n",
    "    completed_courses=[\"Introduction to Programming\", \"Data Structures\"],\n",
    "    current_courses=[\"Linear Algebra\"],\n",
    "    preferred_format=CourseFormat.ONLINE,\n",
    "    preferred_difficulty=DifficultyLevel.INTERMEDIATE,\n",
    ")\n",
    "\n",
    "print(\"âœ… Student profile created\")\n",
    "print(f\"   Name: {sarah.name}\")\n",
    "print(f\"   Student ID: {STUDENT_ID}\")\n",
    "print(f\"   Session ID: {SESSION_ID}\")\n",
    "print(f\"   Major: {sarah.major}\")\n",
    "print(f\"   Interests: {', '.join(sarah.interests)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-memories-tool",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ› ï¸ Part 1: Define the Agent's Tools\n",
    "\n",
    "Let's build our 3 tools step by step. Each tool will have:\n",
    "- Clear input schema (what parameters it accepts)\n",
    "- Descriptive docstring (tells the LLM when to use it)\n",
    "- Implementation (the actual logic)\n",
    "\n",
    "**Remember:** The LLM only sees the tool name, description, and parametersâ€”not the implementation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool-3",
   "metadata": {},
   "source": [
    "### Tool 1: `search_courses`\n",
    "\n",
    "This tool searches the course catalog using semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "store-memory-tool",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.954314Z",
     "iopub.status.busy": "2025-10-31T23:57:53.954256Z",
     "iopub.status.idle": "2025-10-31T23:57:53.957045Z",
     "shell.execute_reply": "2025-10-31T23:57:53.956679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool 1 defined: search_courses\n",
      "   Purpose: Search course catalog with semantic search\n",
      "   Parameters: query (str), limit (int)\n"
     ]
    }
   ],
   "source": [
    "# Define input schema\n",
    "\n",
    "\n",
    "class SearchCoursesInput(BaseModel):\n",
    "    \"\"\"Input schema for searching courses.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        description=\"Natural language search query. Can be topics (e.g., 'machine learning'), \"\n",
    "        \"characteristics (e.g., 'online courses'), or general questions \"\n",
    "        \"(e.g., 'beginner programming courses')\"\n",
    "    )\n",
    "    limit: int = Field(\n",
    "        default=5,\n",
    "        description=\"Maximum number of results to return. Default is 5. \"\n",
    "        \"Use 3 for quick answers, 10 for comprehensive results.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the tool\n",
    "\n",
    "\n",
    "@tool(\"search_courses\", args_schema=SearchCoursesInput)\n",
    "async def search_courses(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search for courses using semantic search based on topics, descriptions, or characteristics.\n",
    "\n",
    "    Use this tool when students ask about:\n",
    "    - Topics or subjects: \"machine learning courses\", \"database courses\"\n",
    "    - Course characteristics: \"online courses\", \"beginner courses\", \"3-credit courses\"\n",
    "    - General exploration: \"what courses are available in AI?\"\n",
    "\n",
    "    The search uses semantic matching, so natural language queries work well.\n",
    "\n",
    "    Returns: Formatted list of matching courses with details.\n",
    "    \"\"\"\n",
    "    results = await course_manager.search_courses(query, limit=limit)\n",
    "\n",
    "    if not results:\n",
    "        return \"No courses found matching your query.\"\n",
    "\n",
    "    output = []\n",
    "    for course in results:\n",
    "        output.append(\n",
    "            f\"{course.course_code}: {course.title}\\n\"\n",
    "            f\"  Credits: {course.credits} | {course.format.value} | {course.difficulty_level.value}\\n\"\n",
    "            f\"  {course.description[:150]}...\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n",
    "\n",
    "print(\"âœ… Tool 1 defined: search_courses\")\n",
    "print(\"   Purpose: Search course catalog with semantic search\")\n",
    "print(\"   Parameters: query (str), limit (int)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-summary",
   "metadata": {},
   "source": [
    "### Tool 2: `search_memories`\n",
    "\n",
    "This tool searches long-term memory for user preferences and facts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "list-tools",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.958090Z",
     "iopub.status.busy": "2025-10-31T23:57:53.958029Z",
     "iopub.status.idle": "2025-10-31T23:57:53.960900Z",
     "shell.execute_reply": "2025-10-31T23:57:53.960462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool 2 defined: search_memories\n",
      "   Purpose: Search long-term memory for user facts\n",
      "   Parameters: query (str), limit (int)\n"
     ]
    }
   ],
   "source": [
    "# Define input schema\n",
    "\n",
    "\n",
    "class SearchMemoriesInput(BaseModel):\n",
    "    \"\"\"Input schema for searching memories.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        description=\"Natural language query to search for in user's long-term memory. \"\n",
    "        \"Examples: 'career goals', 'course preferences', 'learning style'\"\n",
    "    )\n",
    "    limit: int = Field(\n",
    "        default=5, description=\"Maximum number of memories to return. Default is 5.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the tool\n",
    "\n",
    "\n",
    "@tool(\"search_memories\", args_schema=SearchMemoriesInput)\n",
    "async def search_memories(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the user's long-term memory for relevant facts, preferences, and past interactions.\n",
    "\n",
    "    Use this tool when you need to:\n",
    "    - Recall user preferences: \"What format does the user prefer?\"\n",
    "    - Remember past goals: \"What career path is the user interested in?\"\n",
    "    - Find previous interactions: \"What courses did we discuss before?\"\n",
    "    - Personalize recommendations: \"What are the user's interests?\"\n",
    "\n",
    "    The search uses semantic matching to find relevant memories.\n",
    "\n",
    "    Returns: List of relevant memories with content and metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from agent_memory_client.filters import UserId\n",
    "\n",
    "        # Search long-term memory\n",
    "        results = await memory_client.search_long_term_memory(\n",
    "            text=query, user_id=UserId(eq=STUDENT_ID), limit=limit\n",
    "        )\n",
    "\n",
    "        if not results.memories or len(results.memories) == 0:\n",
    "            return \"No relevant memories found.\"\n",
    "\n",
    "        output = []\n",
    "        for i, memory in enumerate(results.memories, 1):\n",
    "            output.append(f\"{i}. {memory.text}\")\n",
    "            if memory.topics:\n",
    "                output.append(f\"   Topics: {', '.join(memory.topics)}\")\n",
    "\n",
    "        return \"\\n\".join(output)\n",
    "    except Exception as e:\n",
    "        return f\"Error searching memories: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"âœ… Tool 2 defined: search_memories\")\n",
    "print(\"   Purpose: Search long-term memory for user facts\")\n",
    "print(\"   Parameters: query (str), limit (int)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-state",
   "metadata": {},
   "source": [
    "### Tool 3: `store_memory`\n",
    "\n",
    "This tool saves important information to long-term memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "define-state",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.962062Z",
     "iopub.status.busy": "2025-10-31T23:57:53.961995Z",
     "iopub.status.idle": "2025-10-31T23:57:53.964832Z",
     "shell.execute_reply": "2025-10-31T23:57:53.964534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool 3 defined: store_memory\n",
      "   Purpose: Save important facts to long-term memory\n",
      "   Parameters: text (str), memory_type (str), topics (List[str])\n"
     ]
    }
   ],
   "source": [
    "# Define input schema\n",
    "\n",
    "\n",
    "class StoreMemoryInput(BaseModel):\n",
    "    \"\"\"Input schema for storing memories.\"\"\"\n",
    "\n",
    "    text: str = Field(\n",
    "        description=\"The information to store. Should be a clear, factual statement. \"\n",
    "        \"Examples: 'User prefers online courses', 'User's career goal is AI research'\"\n",
    "    )\n",
    "    memory_type: str = Field(\n",
    "        default=\"semantic\",\n",
    "        description=\"Type of memory: 'semantic' (facts/preferences), 'episodic' (events/interactions). \"\n",
    "        \"Default is 'semantic'.\",\n",
    "    )\n",
    "    topics: List[str] = Field(\n",
    "        default=[],\n",
    "        description=\"Optional tags to categorize the memory, such as ['preferences', 'courses']\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the tool\n",
    "\n",
    "\n",
    "@tool(\"store_memory\", args_schema=StoreMemoryInput)\n",
    "async def store_memory(\n",
    "    text: str, memory_type: str = \"semantic\", topics: List[str] = []\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Store important information to the user's long-term memory.\n",
    "\n",
    "    Use this tool when the user shares:\n",
    "    - Preferences: \"I prefer online courses\", \"I like hands-on projects\"\n",
    "    - Goals: \"I want to work in AI\", \"I'm preparing for grad school\"\n",
    "    - Important facts: \"I have a part-time job\", \"I'm interested in startups\"\n",
    "    - Constraints: \"I can only take 2 courses per semester\"\n",
    "\n",
    "    Do NOT store:\n",
    "    - Temporary information (use conversation context instead)\n",
    "    - Course details (already in course catalog)\n",
    "    - General questions\n",
    "\n",
    "    Returns: Confirmation message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from agent_memory_client.models import ClientMemoryRecord\n",
    "\n",
    "        # Create memory record\n",
    "        memory = ClientMemoryRecord(\n",
    "            text=text, user_id=STUDENT_ID, memory_type=memory_type, topics=topics or []\n",
    "        )\n",
    "\n",
    "        # Store in long-term memory\n",
    "        await memory_client.create_long_term_memory([memory])\n",
    "        return f\"âœ… Stored to long-term memory: {text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error storing memory: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"âœ… Tool 3 defined: store_memory\")\n",
    "print(\"   Purpose: Save important facts to long-term memory\")\n",
    "print(\"   Parameters: text (str), memory_type (str), topics (List[str])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-nodes",
   "metadata": {},
   "source": [
    "### Tools Summary\n",
    "\n",
    "Let's review our 3 tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "load-memory-node",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.966158Z",
     "iopub.status.busy": "2025-10-31T23:57:53.966078Z",
     "iopub.status.idle": "2025-10-31T23:57:53.968399Z",
     "shell.execute_reply": "2025-10-31T23:57:53.968046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ› ï¸  AGENT TOOLS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. search_courses\n",
      "   Description: Search for courses using semantic search based on topics, descriptions, or characteristics\n",
      "   Parameters: query, limit\n",
      "\n",
      "2. search_memories\n",
      "   Description: Search the user's long-term memory for relevant facts, preferences, and past interactions\n",
      "   Parameters: query, limit\n",
      "\n",
      "3. store_memory\n",
      "   Description: Store important information to the user's long-term memory\n",
      "   Parameters: text, memory_type, topics\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Collect all tools\n",
    "tools = [search_courses, search_memories, store_memory]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ› ï¸  AGENT TOOLS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "for i, tool in enumerate(tools, 1):\n",
    "    print(f\"\\n{i}. {tool.name}\")\n",
    "    print(f\"   Description: {tool.description.split('.')[0]}\")\n",
    "    print(f\"   Parameters: {', '.join(tool.args_schema.model_fields.keys())}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-node",
   "metadata": {},
   "source": "\n"
  },
  {
   "cell_type": "markdown",
   "id": "save-memory-node",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.969443Z",
     "iopub.status.busy": "2025-10-31T23:57:53.969382Z",
     "iopub.status.idle": "2025-10-31T23:57:53.971457Z",
     "shell.execute_reply": "2025-10-31T23:57:53.971109Z"
    }
   },
   "source": [
    "## ðŸ§  Memory Extraction in This Agent\n",
    "\n",
    "Understanding how this agent creates and manages long-term memories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "routing-logic",
   "metadata": {},
   "source": [
    "### How This Agent Uses Memory\n",
    "\n",
    "Our agent has 3 tools, and 2 of them interact with memory:\n",
    "\n",
    "1. **`store_memory`** - Saves facts to long-term memory\n",
    "2. **`search_memories`** - Retrieves facts from long-term memory\n",
    "3. **`search_courses`** - Searches course catalog (not memory-related)\n",
    "\n",
    "**Question:** When the agent calls `store_memory`, how does the Agent Memory Server decide what to extract and how to structure it?\n",
    "\n",
    "**Answer:** Memory Extraction Strategies (covered in Section 3, Notebook 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "should-continue",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.972503Z",
     "iopub.status.busy": "2025-10-31T23:57:53.972440Z",
     "iopub.status.idle": "2025-10-31T23:57:53.974986Z",
     "shell.execute_reply": "2025-10-31T23:57:53.974616Z"
    }
   },
   "source": [
    "### Current Configuration: Discrete Strategy (Default)\n",
    "\n",
    "**This agent uses the DISCRETE strategy** (default) because:\n",
    "\n",
    "âœ… **Individual facts are searchable**\n",
    "- \"User's major is Computer Science\"\n",
    "- \"User interested in machine learning\"\n",
    "- \"User completed RU101\"\n",
    "\n",
    "âœ… **Facts are independently useful**\n",
    "- Agent can search for specific facts\n",
    "- Each fact has its own relevance score\n",
    "- No need to parse summaries\n",
    "\n",
    "âœ… **Good for Q&A interactions**\n",
    "- Student: \"What courses did I say I was interested in?\"\n",
    "- Agent searches discrete facts: \"User interested in ML\", \"User interested in AI\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-graph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.975927Z",
     "iopub.status.busy": "2025-10-31T23:57:53.975854Z",
     "iopub.status.idle": "2025-10-31T23:57:53.977825Z",
     "shell.execute_reply": "2025-10-31T23:57:53.977580Z"
    }
   },
   "source": [
    "### Example: Discrete Strategy in Action\n",
    "\n",
    "**Conversation:**\n",
    "```\n",
    "User: \"I'm a CS major interested in ML. I prefer online courses.\"\n",
    "Agent: [Calls store_memory tool]\n",
    "```\n",
    "\n",
    "**What Gets Stored (Discrete Strategy):**\n",
    "```json\n",
    "[\n",
    "  {\"text\": \"User's major is Computer Science\", \"type\": \"semantic\"},\n",
    "  {\"text\": \"User interested in machine learning\", \"type\": \"semantic\"},\n",
    "  {\"text\": \"User prefers online courses\", \"type\": \"semantic\"}\n",
    "]\n",
    "```\n",
    "\n",
    "**Later:**\n",
    "```\n",
    "User: \"What courses match my interests?\"\n",
    "Agent: [Calls search_memories tool]\n",
    "       â†’ Finds: \"User interested in machine learning\"\n",
    "       â†’ Finds: \"User prefers online courses\"\n",
    "       [Calls search_courses with these preferences]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "construct-graph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.978903Z",
     "iopub.status.busy": "2025-10-31T23:57:53.978835Z",
     "iopub.status.idle": "2025-10-31T23:57:53.981202Z",
     "shell.execute_reply": "2025-10-31T23:57:53.980864Z"
    }
   },
   "source": [
    "### When Would Summary Strategy Be Better?\n",
    "\n",
    "**Summary strategy** would be beneficial for:\n",
    "\n",
    "**Scenario 1: Long Advising Sessions**\n",
    "```\n",
    "User has 30-minute conversation discussing:\n",
    "- Academic goals\n",
    "- Career aspirations\n",
    "- Course preferences\n",
    "- Schedule constraints\n",
    "- Graduation timeline\n",
    "```\n",
    "\n",
    "**Discrete Strategy:** Extracts 20+ individual facts\n",
    "**Summary Strategy:** Creates 1-2 comprehensive summaries preserving context\n",
    "\n",
    "**Scenario 2: Session Notes**\n",
    "```\n",
    "Agent: \"Let me summarize our conversation today...\"\n",
    "[Retrieves summary memory instead of reconstructing from discrete facts]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-graph",
   "metadata": {},
   "source": [
    "### Configuration Example (Not Used in This Notebook)\n",
    "\n",
    "If you wanted to use summary strategy instead:\n",
    "\n",
    "```python\n",
    "from agent_memory_client.models import MemoryStrategyConfig\n",
    "\n",
    "# Configure summary strategy\n",
    "summary_strategy = MemoryStrategyConfig(\n",
    "    strategy=\"summary\",\n",
    "    config={\"max_summary_length\": 500}\n",
    ")\n",
    "\n",
    "# Apply when creating working memory\n",
    "await memory_client.set_working_memory(\n",
    "    session_id=session_id,\n",
    "    messages=messages,\n",
    "    long_term_memory_strategy=summary_strategy  # â† Use summary instead of discrete\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "show-graph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.982174Z",
     "iopub.status.busy": "2025-10-31T23:57:53.982118Z",
     "iopub.status.idle": "2025-10-31T23:57:53.983908Z",
     "shell.execute_reply": "2025-10-31T23:57:53.983535Z"
    }
   },
   "source": [
    "### Why We Stick with Discrete (Default)\n",
    "\n",
    "For this course advisor agent:\n",
    "- âœ… Questions are specific (\"What are prerequisites for RU301?\")\n",
    "- âœ… Facts are independently useful\n",
    "- âœ… Search works better with discrete facts\n",
    "- âœ… No configuration needed (default behavior)\n",
    "\n",
    "**In production**, you might:\n",
    "- Use **discrete** for most interactions (default)\n",
    "- Use **summary** for end-of-session notes\n",
    "- Use **preferences** during student onboarding\n",
    "- Use **custom** for specialized academic domains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-section",
   "metadata": {},
   "source": [
    "### ðŸ”— Connection to Section 3\n",
    "\n",
    "In **Section 3, Notebook 1**, we introduced memory extraction strategies conceptually.\n",
    "\n",
    "In **Section 3, Notebook 2**, we demonstrated the difference between discrete and summary strategies with hands-on examples.\n",
    "\n",
    "**Now in Section 4**, we see how a production agent uses the discrete strategy (default) for course advising.\n",
    "\n",
    "**Key Takeaway:** The Agent Memory Server's memory extraction strategies give you flexibility in HOW memories are created, but for most agent interactions (like this course advisor), the default discrete strategy works best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-agent-helper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.984807Z",
     "iopub.status.busy": "2025-10-31T23:57:53.984751Z",
     "iopub.status.idle": "2025-10-31T23:57:53.990038Z",
     "shell.execute_reply": "2025-10-31T23:57:53.989670Z"
    }
   },
   "source": [
    "### ðŸ“š Learn More\n",
    "\n",
    "- [Memory Extraction Strategies Documentation](https://redis.github.io/agent-memory-server/memory-extraction-strategies/)\n",
    "- [Section 3, Notebook 1](../section-3-memory-systems-for-context-engineering/01_working_and_longterm_memory.ipynb) - Theory foundation\n",
    "- [Section 3, Notebook 2](../section-3-memory-systems-for-context-engineering/02_combining_memory_with_retrieved_context.ipynb) - Hands-on comparison demo\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¨ Part 2: Define the Agent State\n",
    "\n",
    "In LangGraph, **state** is the shared data structure that flows through the graph. Each node can read from and write to the state.\n",
    "\n",
    "### What Goes in State?\n",
    "\n",
    "- **messages**: Conversation history (automatically managed by LangGraph)\n",
    "- **student_id**: Who we're helping\n",
    "- **session_id**: Current conversation session\n",
    "- **context**: Additional context (memories, preferences, etc.)\n",
    "\n",
    "**Note:** We use `Annotated[List[BaseMessage], add_messages]` for messages. The `add_messages` reducer automatically handles message deduplication and ordering.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-1",
   "metadata": {},
   "source": [
    "# Define the agent state\n",
    "\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"State for the course advisor agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    student_id: str\n",
    "    session_id: str\n",
    "    context: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "print(\"âœ… Agent state defined\")\n",
    "print(\"   Fields: messages, student_id, session_id, context\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "demo-search",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.991081Z",
     "iopub.status.busy": "2025-10-31T23:57:53.991018Z",
     "iopub.status.idle": "2025-10-31T23:57:54.095976Z",
     "shell.execute_reply": "2025-10-31T23:57:54.095530Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— Part 3: Build the Agent Graph\n",
    "\n",
    "Now we'll build the LangGraph workflow. Our graph will have:\n",
    "\n",
    "1. **load_memory** - Load working memory (conversation history)\n",
    "2. **agent** - LLM decides what to do (call tools or respond)\n",
    "3. **tools** - Execute tool calls\n",
    "4. **save_memory** - Save updated conversation to working memory\n",
    "\n",
    "### Step 1: Define Node Functions\n",
    "\n",
    "Each node is a function that takes state and returns updated state.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-2",
   "metadata": {},
   "source": [
    "# Node 1: Load working memory\n",
    "\n",
    "\n",
    "async def load_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Load conversation history from working memory.\n",
    "\n",
    "    This gives the agent context about previous interactions in this session.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get or create working memory for this session\n",
    "        _, working_memory = await memory_client.get_or_create_working_memory(\n",
    "            session_id=state.session_id, user_id=state.student_id, model_name=\"gpt-4o\"\n",
    "        )\n",
    "\n",
    "        if working_memory and working_memory.messages:\n",
    "            # Convert stored messages to LangChain message objects\n",
    "            loaded_messages = []\n",
    "            for msg in working_memory.messages:\n",
    "                if msg.role == \"user\":\n",
    "                    loaded_messages.append(HumanMessage(content=msg.content))\n",
    "                elif msg.role == \"assistant\":\n",
    "                    loaded_messages.append(AIMessage(content=msg.content))\n",
    "\n",
    "            # Add loaded messages to state (prepend to current messages)\n",
    "            state.messages = loaded_messages + state.messages\n",
    "            state.context[\"memory_loaded\"] = True\n",
    "            print(f\"   Loaded {len(loaded_messages)} messages from working memory\")\n",
    "        else:\n",
    "            state.context[\"memory_loaded\"] = False\n",
    "            print(\"   No previous conversation found (new session)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Could not load memory: {e}\")\n",
    "        state.context[\"memory_loaded\"] = False\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"âœ… Node 1 defined: load_memory\")\n",
    "print(\"   Purpose: Load conversation history from working memory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "demo-store",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:54.097563Z",
     "iopub.status.busy": "2025-10-31T23:57:54.097461Z",
     "iopub.status.idle": "2025-10-31T23:57:54.100763Z",
     "shell.execute_reply": "2025-10-31T23:57:54.100208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper function defined: run_agent\n"
     ]
    }
   ],
   "source": [
    "# Node 2: Agent (LLM with tools)\n",
    "\n",
    "\n",
    "async def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    The agent decides what to do: call tools or respond to the user.\n",
    "\n",
    "    This is where the LLM reasoning happens.\n",
    "    \"\"\"\n",
    "    # Create system message with instructions\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"\n",
    "You are a helpful Redis University course advisor assistant.\n",
    "\n",
    "Your role:\n",
    "- Help students find courses that match their interests and goals\n",
    "- Remember student preferences and use them for personalized recommendations\n",
    "- Store important information about students for future conversations\n",
    "\n",
    "Guidelines:\n",
    "- Use search_courses to find relevant courses\n",
    "- Use search_memories to recall student preferences and past interactions\n",
    "- Use store_memory when students share important preferences, goals, or constraints\n",
    "- Be conversational and helpful\n",
    "- Provide specific course recommendations with details\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    # Bind tools to LLM\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # Call LLM with system message + conversation history\n",
    "    messages = [system_message] + state.messages\n",
    "    response = await llm_with_tools.ainvoke(messages)\n",
    "\n",
    "    # Add response to state\n",
    "    state.messages.append(response)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"âœ… Node 2 defined: agent_node\")\n",
    "print(\"   Purpose: LLM decides whether to call tools or respond\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-3",
   "metadata": {},
   "source": [
    "# Node 3: Save working memory\n",
    "\n",
    "\n",
    "async def save_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Save the updated conversation to working memory.\n",
    "\n",
    "    This ensures continuity across conversation turns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get or create working memory\n",
    "        _, working_memory = await memory_client.get_or_create_working_memory(\n",
    "            session_id=state.session_id, user_id=state.student_id, model_name=\"gpt-4o\"\n",
    "        )\n",
    "\n",
    "        # Clear existing messages and add current conversation\n",
    "        working_memory.messages = []\n",
    "        for msg in state.messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                working_memory.messages.append(\n",
    "                    MemoryMessage(role=\"user\", content=msg.content)\n",
    "                )\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                # Only store text content, not tool calls\n",
    "                if msg.content:\n",
    "                    working_memory.messages.append(\n",
    "                        MemoryMessage(role=\"assistant\", content=msg.content)\n",
    "                    )\n",
    "\n",
    "        # Save to working memory\n",
    "        await memory_client.put_working_memory(\n",
    "            session_id=state.session_id,\n",
    "            memory=working_memory,\n",
    "            user_id=state.student_id,\n",
    "            model_name=\"gpt-4o\",\n",
    "        )\n",
    "\n",
    "        print(f\"   Saved {len(working_memory.messages)} messages to working memory\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Could not save memory: {e}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"âœ… Node 3 defined: save_memory\")\n",
    "print(\"   Purpose: Save conversation to working memory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "demo-recall",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:54.102049Z",
     "iopub.status.busy": "2025-10-31T23:57:54.101962Z",
     "iopub.status.idle": "2025-10-31T23:57:58.356458Z",
     "shell.execute_reply": "2025-10-31T23:57:58.355667Z"
    }
   },
   "source": [
    "### Step 2: Define Routing Logic\n",
    "\n",
    "We need a function to decide: should we call tools or end the conversation?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-4",
   "metadata": {},
   "source": [
    "# Routing function\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determine if we should continue to tools or end.\n",
    "\n",
    "    If the last message has tool calls, route to tools.\n",
    "    Otherwise, we're done.\n",
    "    \"\"\"\n",
    "    last_message = state.messages[-1]\n",
    "\n",
    "    # Check if there are tool calls\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"save_memory\"\n",
    "\n",
    "\n",
    "print(\"âœ… Routing logic defined: should_continue\")\n",
    "print(\"   Routes to 'tools' if LLM wants to call tools, otherwise to 'save_memory'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "demo-personalized",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:58.358447Z",
     "iopub.status.busy": "2025-10-31T23:57:58.358312Z",
     "iopub.status.idle": "2025-10-31T23:58:04.410189Z",
     "shell.execute_reply": "2025-10-31T23:58:04.409512Z"
    }
   },
   "source": [
    "### Step 3: Build the Graph\n",
    "\n",
    "Now we assemble all the pieces into a LangGraph workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect-memory",
   "metadata": {},
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"load_memory\", load_memory)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "workflow.add_node(\"save_memory\", save_memory)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"load_memory\")\n",
    "workflow.add_edge(\"load_memory\", \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", should_continue, {\"tools\": \"tools\", \"save_memory\": \"save_memory\"}\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")  # After tools, go back to agent\n",
    "workflow.add_edge(\"save_memory\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent_graph = workflow.compile()\n",
    "\n",
    "print(\"âœ… Agent graph built and compiled!\")\n",
    "print(\"\\nðŸ“Š Graph structure:\")\n",
    "print(\"   START â†’ load_memory â†’ agent â†’ [tools â†’ agent]* â†’ save_memory â†’ END\")\n",
    "print(\"\\n   * The agent can call tools multiple times before responding\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "check-memories",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:58:04.411898Z",
     "iopub.status.busy": "2025-10-31T23:58:04.411768Z",
     "iopub.status.idle": "2025-10-31T23:58:06.565467Z",
     "shell.execute_reply": "2025-10-31T23:58:06.564738Z"
    }
   },
   "source": [
    "### Step 4: Visualize the Graph\n",
    "\n",
    "Let's see what our agent workflow looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "comparison",
   "metadata": {},
   "source": [
    "# Try to visualize the graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    # Generate graph visualization\n",
    "    graph_image = agent_graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(graph_image))\n",
    "    print(\"\\nâœ… Graph visualization displayed above\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not display graph visualization: {e}\")\n",
    "    print(\"\\nGraph structure (text):\")\n",
    "    print(\n",
    "        \"\"\"\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   START     â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ load_memory â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚    agent    â”‚ â—„â”€â”€â”€â”€â”€â”\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "           â”‚              â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”‚\n",
    "      â”‚         â”‚         â”‚\n",
    "      â–¼         â–¼         â”‚\n",
    "   [tools]  [respond]     â”‚\n",
    "      â”‚                   â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚ save_memory â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚     END     â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \"\"\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "architecture-recap",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:58:06.567416Z",
     "iopub.status.busy": "2025-10-31T23:58:06.567279Z",
     "iopub.status.idle": "2025-10-31T23:58:11.047325Z",
     "shell.execute_reply": "2025-10-31T23:58:11.046775Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¬ Part 4: Demo the Agent\n",
    "\n",
    "Now let's see our agent in action! We'll have a conversation with the agent and watch it:\n",
    "- Search for courses\n",
    "- Store memories about preferences\n",
    "- Recall information from previous interactions\n",
    "\n",
    "### Helper Function: Run Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "async def run_agent(user_message: str, verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Run the agent with a user message.\n",
    "\n",
    "    Args:\n",
    "        user_message: The user's input\n",
    "        verbose: Whether to print detailed execution info\n",
    "\n",
    "    Returns:\n",
    "        The agent's response\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ðŸ‘¤ USER: {user_message}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Create initial state\n",
    "    initial_state = AgentState(\n",
    "        messages=[HumanMessage(content=user_message)],\n",
    "        student_id=STUDENT_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        context={},\n",
    "    )\n",
    "\n",
    "    # Run the graph\n",
    "    if verbose:\n",
    "        print(\"\\nðŸ¤– AGENT EXECUTION:\")\n",
    "\n",
    "    final_state = await agent_graph.ainvoke(initial_state)\n",
    "\n",
    "    # Extract the final response\n",
    "    final_message = final_state[\"messages\"][-1]\n",
    "    response = (\n",
    "        final_message.content\n",
    "        if hasattr(final_message, \"content\")\n",
    "        else str(final_message)\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"ðŸ¤– ASSISTANT: {response}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"âœ… Helper function defined: run_agent\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:58:11.049386Z",
     "iopub.status.busy": "2025-10-31T23:58:11.049237Z",
     "iopub.status.idle": "2025-10-31T23:58:11.464715Z",
     "shell.execute_reply": "2025-10-31T23:58:11.464089Z"
    }
   },
   "source": [
    "### Demo 1: Search Courses\n",
    "\n",
    "Let's ask the agent to find machine learning courses.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "# Demo 1: Search for courses\n",
    "response1 = await run_agent(\n",
    "    \"What machine learning courses are available? I'm interested in intermediate level courses.\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8c8b43a1a04fff3",
   "metadata": {},
   "source": [
    "### Demo 2: Store Preferences\n",
    "\n",
    "Now let's share some preferences and watch the agent store them.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "97d4b563a3a30240",
   "metadata": {},
   "source": [
    "# Demo 2: Store preferences\n",
    "response2 = await run_agent(\n",
    "    \"I prefer online courses because I have a part-time job. \"\n",
    "    \"Also, I'm really interested in AI and want to work at a startup after graduation.\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2fc05bfee7ece66",
   "metadata": {},
   "source": [
    "### Demo 3: Recall Memories\n",
    "\n",
    "Let's ask the agent to recall what it knows about us.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "437746891b606882",
   "metadata": {},
   "source": [
    "# Demo 3: Recall memories\n",
    "response3 = await run_agent(\"What do you remember about my preferences and goals?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d495052317c67bb",
   "metadata": {},
   "source": [
    "### Demo 4: Personalized Recommendations\n",
    "\n",
    "Now let's ask for recommendations and see if the agent uses our stored preferences.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Demo 4: Personalized recommendations\n",
    "response4 = await run_agent(\n",
    "    \"Can you recommend some courses for next semester based on what you know about me?\"\n",
    ")"
   ],
   "id": "3eb0f6ddeb45a9f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Inspect Stored Memories\n",
    "\n",
    "Let's look at what's actually stored in long-term memory.\n"
   ],
   "id": "17dd61ca397db6be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check what's in long-term memory\n",
    "try:\n",
    "    from agent_memory_client.filters import UserId\n",
    "\n",
    "    results = await memory_client.search_long_term_memory(\n",
    "        text=\"preferences goals interests\", user_id=UserId(eq=STUDENT_ID), limit=10\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸ’¾ LONG-TERM MEMORY CONTENTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if results.memories and len(results.memories) > 0:\n",
    "        for i, memory in enumerate(results.memories, 1):\n",
    "            print(f\"\\n{i}. [{memory.memory_type}] {memory.text}\")\n",
    "            if memory.topics:\n",
    "                print(f\"   Topics: {', '.join(memory.topics)}\")\n",
    "            if memory.created_at:\n",
    "                print(f\"   Created: {memory.created_at}\")\n",
    "    else:\n",
    "        print(\"\\nNo memories found.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving memories: {e}\")"
   ],
   "id": "19a91887b957f48c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š Part 5: RAG vs Agent Comparison\n",
    "\n",
    "Let's compare what we've built across the sections:\n",
    "\n",
    "### **Section 2: Basic RAG**\n",
    "```python\n",
    "# Simple flow\n",
    "query â†’ search_courses() â†’ generate_response()\n",
    "```\n",
    "- âœ… Can retrieve course information\n",
    "- âŒ No memory of previous interactions\n",
    "- âŒ Can't store user preferences\n",
    "- âŒ Single-step only\n",
    "\n",
    "### **Section 3: Memory-Enhanced RAG**\n",
    "```python\n",
    "# With memory\n",
    "load_memory() â†’ search_courses() â†’ generate_response() â†’ save_memory()\n",
    "```\n",
    "- âœ… Remembers conversation history\n",
    "- âœ… Can reference previous messages\n",
    "- âš ï¸  Limited to predefined flow\n",
    "- âŒ Can't decide when to store memories\n",
    "\n",
    "### **Section 4: Full Agent (This Notebook)**\n",
    "```python\n",
    "# Agent with tools and decision-making\n",
    "load_memory() â†’ agent_decides() â†’ [search_courses | search_memories | store_memory]* â†’ save_memory()\n",
    "```\n",
    "- âœ… Remembers conversation history\n",
    "- âœ… Decides when to search courses\n",
    "- âœ… Decides when to store memories\n",
    "- âœ… Decides when to recall memories\n",
    "- âœ… Can chain multiple operations\n",
    "- âœ… Adaptive to user needs\n",
    "\n",
    "### **Key Differences:**\n",
    "\n",
    "| Feature | RAG | Memory-RAG | Agent |\n",
    "|---------|-----|------------|-------|\n",
    "| **Retrieval** | âœ… | âœ… | âœ… |\n",
    "| **Conversation Memory** | âŒ | âœ… | âœ… |\n",
    "| **Long-term Memory** | âŒ | âš ï¸ (manual) | âœ… (automatic) |\n",
    "| **Decision Making** | âŒ | âŒ | âœ… |\n",
    "| **Multi-step Reasoning** | âŒ | âŒ | âœ… |\n",
    "| **Tool Selection** | âŒ | âŒ | âœ… |\n",
    "| **Complexity** | Low | Medium | High |\n",
    "| **Latency** | Low | Medium | Higher |\n",
    "| **Cost** | Low | Medium | Higher |\n",
    "\n",
    "**ðŸ’¡ Key Insight:** Agents add decision-making and multi-step reasoning to RAG systems.\n"
   ],
   "id": "fd45b11038775302"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ Architecture Recap\n",
    "\n",
    "### **What We Built:**\n",
    "\n",
    "A complete course advisor agent with:\n",
    "\n",
    "**1. Tools (3 total)**\n",
    "- `search_courses` - Semantic search over course catalog\n",
    "- `search_memories` - Recall user preferences and facts\n",
    "- `store_memory` - Save important information\n",
    "\n",
    "**2. Memory Architecture**\n",
    "- **Working Memory** - Conversation history (session-scoped)\n",
    "- **Long-term Memory** - User preferences and facts (persistent)\n",
    "- **Graph State** - Current execution state (turn-scoped)\n",
    "\n",
    "**3. LangGraph Workflow**\n",
    "- **Nodes**: load_memory, agent, tools, save_memory\n",
    "- **Edges**: Conditional routing based on LLM decisions\n",
    "- **State**: Shared data structure flowing through the graph\n",
    "\n",
    "**4. Integration Points**\n",
    "- **Redis** - Course catalog storage and vector search\n",
    "- **Agent Memory Server** - Working and long-term memory\n",
    "- **OpenAI** - LLM for reasoning and tool selection\n",
    "- **LangGraph** - Workflow orchestration\n",
    "\n",
    "### **The Complete Context Engineering Stack:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    AGENT LAYER                          â”‚\n",
    "â”‚  (LangGraph orchestration + tool selection)             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚            â”‚            â”‚\n",
    "        â–¼            â–¼            â–¼\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Tools  â”‚  â”‚ Memory  â”‚  â”‚   RAG   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚            â”‚            â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â–¼\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚  Redis Stack    â”‚\n",
    "            â”‚  (Storage +     â”‚\n",
    "            â”‚   Vector Search)â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n"
   ],
   "id": "d4a533d945ca605e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "### **1. Agents = RAG + Tools + Decision-Making**\n",
    "- RAG retrieves information\n",
    "- Tools enable actions\n",
    "- Agents decide when to use each\n",
    "\n",
    "### **2. Memory is Critical for Personalization**\n",
    "- Working memory enables conversation continuity\n",
    "- Long-term memory enables personalization\n",
    "- Agents can decide when to store/recall memories\n",
    "\n",
    "### **3. LangGraph Simplifies Complex Workflows**\n",
    "- State management is automatic\n",
    "- Conditional routing is declarative\n",
    "- Visualization helps debugging\n",
    "\n",
    "### **4. Tool Design Matters**\n",
    "- Clear descriptions guide LLM selection\n",
    "- Well-defined schemas prevent errors\n",
    "- Focused tools are better than Swiss Army knives\n",
    "\n",
    "### **5. Trade-offs to Consider**\n",
    "- **Complexity**: Agents are more complex than RAG\n",
    "- **Latency**: Multiple tool calls add latency\n",
    "- **Cost**: More LLM calls = higher cost\n",
    "- **Value**: Worth it for complex, multi-step tasks\n",
    "\n",
    "### **6. When to Use Agents vs RAG**\n",
    "\n",
    "**Use RAG when:**\n",
    "- Simple question answering\n",
    "- Single-step retrieval\n",
    "- Low latency required\n",
    "- Predictable workflows\n",
    "\n",
    "**Use Agents when:**\n",
    "- Multi-step reasoning needed\n",
    "- Actions beyond retrieval\n",
    "- Personalization required\n",
    "- Complex decision-making\n"
   ],
   "id": "c4654c5a2c4e5323"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸš€ Next Steps and Extensions\n",
    "\n",
    "### **Ideas to Extend This Agent:**\n",
    "\n",
    "1. **Add More Tools**\n",
    "   - `check_prerequisites` - Verify if student meets course requirements\n",
    "   - `get_course_details` - Get detailed info about a specific course\n",
    "   - `create_schedule` - Build a semester schedule\n",
    "   - `check_conflicts` - Detect time conflicts\n",
    "\n",
    "2. **Enhance Memory**\n",
    "   - Automatic memory extraction from conversations\n",
    "   - Memory summarization for long conversations\n",
    "   - Memory importance scoring\n",
    "   - Memory expiration policies\n",
    "\n",
    "3. **Improve Personalization**\n",
    "   - Learning style detection\n",
    "   - Career path recommendations\n",
    "   - Skill gap analysis\n",
    "   - Progress tracking\n",
    "\n",
    "4. **Add Guardrails**\n",
    "   - Input validation\n",
    "   - Output filtering\n",
    "   - Rate limiting\n",
    "   - Error handling\n",
    "\n",
    "5. **Production Considerations**\n",
    "   - Authentication and authorization\n",
    "   - Logging and monitoring\n",
    "   - Caching for performance\n",
    "   - Fallback strategies\n",
    "\n",
    "### **Reference Implementation:**\n",
    "\n",
    "Check out `reference-agent/` for a full production implementation with:\n",
    "- 7 tools (vs our 3)\n",
    "- Advanced memory management\n",
    "- Semantic tool selection\n",
    "- Comprehensive error handling\n",
    "- CLI interface\n",
    "- Full test suite\n"
   ],
   "id": "346d2737598bfd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the Context Engineering course! You've learned:\n",
    "\n",
    "**Section 1:** Context Types\n",
    "- System, User, Conversation, Retrieved context\n",
    "- How context shapes LLM behavior\n",
    "\n",
    "**Section 2:** RAG Foundations\n",
    "- Semantic search with vector embeddings\n",
    "- Context assembly and generation\n",
    "- Building a course search system\n",
    "\n",
    "**Section 3:** Memory Architecture\n",
    "- Working memory for conversation continuity\n",
    "- Long-term memory for persistent knowledge\n",
    "- Memory-enhanced RAG systems\n",
    "\n",
    "**ðŸ”¬ Research Foundation:** Throughout this course, you've learned techniques validated by Context Rot research - prioritizing relevance over quantity, filtering distractors, and structuring context for optimal LLM performance. ([Context Rot paper](https://research.trychroma.com/context-rot))\n",
    "\n",
    "**Section 4:** Agents and Tools\n",
    "- Tool calling fundamentals\n",
    "- LangGraph workflow orchestration\n",
    "- Building a complete course advisor agent\n",
    "- Agents vs RAG trade-offs\n",
    "\n",
    "### **You Can Now:**\n",
    "- âœ… Design effective context strategies\n",
    "- âœ… Build RAG systems with Redis\n",
    "- âœ… Implement dual-memory architectures\n",
    "- âœ… Create agents with tools and decision-making\n",
    "- âœ… Choose the right approach for your use case\n",
    "\n",
    "### **Keep Learning:**\n",
    "- Explore the reference-agent implementation\n",
    "- Experiment with different tools\n",
    "- Try different LLMs and embeddings\n",
    "- Build your own agents!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "\n",
    "- [Agent Memory Server Documentation](https://github.com/redis/agent-memory-server) - Production-ready memory management\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client for Agent Memory Server\n",
    "- [RedisVL Documentation](https://redisvl.com/) - Redis Vector Library\n",
    "- [Retrieval-Augmented Generation Paper](https://arxiv.org/abs/2005.11401) - Original RAG research\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/) - Building RAG systems\n",
    "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/) - Building agents with LangGraph\n",
    "- [Agent Architectures](https://python.langchain.com/docs/modules/agents/) - Different agent patterns\n",
    "- [ReAct: Synergizing Reasoning and Acting](https://arxiv.org/abs/2210.03629) - Reasoning + acting in LLMs\n",
    "- [Anthropic's Guide to Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - Agent design patterns\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this course! ðŸ™**\n"
   ],
   "id": "6a1c7e21740d4240"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "439770b03604fe49"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

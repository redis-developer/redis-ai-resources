{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# ðŸ¤– Agent with Memory Compression\n",
    "\n",
    "**â±ï¸ Estimated Time:** 90-120 minutes\n",
    "\n",
    "**ðŸ“ Note:** This is an enhanced version of the course advisor agent that includes working memory compression demonstrations. For the standard version without compression, see `02_building_course_advisor_agent.ipynb`.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Build** a complete LangGraph agent with tools and memory\n",
    "2. **Implement** exactly 3 tools: memory storage, memory search, and course search\n",
    "3. **Integrate** Redis Agent Memory Server for dual-memory architecture\n",
    "4. **Visualize** the agent's decision-making graph\n",
    "5. **Demonstrate** the progression from RAG (Section 3) to full agent\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Bridge from Previous Sections\n",
    "\n",
    "### **Your Learning Journey:**\n",
    "\n",
    "**Section 1:** Context Types\n",
    "- System, User, Conversation, Retrieved context\n",
    "- How context shapes LLM responses\n",
    "\n",
    "**Section 2:** RAG Foundations\n",
    "- Semantic search with vector embeddings\n",
    "- Retrieving and presenting information\n",
    "- Single-step retrieval â†’ generation\n",
    "\n",
    "**Section 3:** Memory Architecture\n",
    "- Working memory (conversation continuity)\n",
    "- Long-term memory (persistent knowledge)\n",
    "- Memory-enhanced RAG systems\n",
    "\n",
    "**Section 4 (Notebook 1):** Tool-Calling Basics\n",
    "- What tools are and how LLMs use them\n",
    "- LangGraph fundamentals (nodes, edges, state)\n",
    "- Simple tool-calling examples\n",
    "- Agents vs RAG comparison\n",
    "\n",
    "### **What We're Building Now:**\n",
    "\n",
    "**A Full Agent** that combines everything:\n",
    "- âœ… **Tools** for actions (search courses, manage memory)\n",
    "- âœ… **Memory** for personalization (working + long-term)\n",
    "- âœ… **RAG** for course information (semantic search)\n",
    "- âœ… **LangGraph** for orchestration (state management)\n",
    "\n",
    "**ðŸ’¡ Key Insight:** This agent is RAG + Memory + Tools + Decision-Making\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Agent Architecture\n",
    "\n",
    "### **The Complete Flow:**\n",
    "\n",
    "```\n",
    "User Query\n",
    "    â†“\n",
    "[Load Working Memory] â† Conversation history\n",
    "    â†“\n",
    "[Agent Node] â† Decides what to do\n",
    "    â†“\n",
    "    â”œâ”€â†’ [search_courses] â† Find relevant courses\n",
    "    â”œâ”€â†’ [search_memories] â† Recall user preferences\n",
    "    â”œâ”€â†’ [store_memory] â† Save important facts\n",
    "    â†“\n",
    "[Agent Node] â† Processes tool results\n",
    "    â†“\n",
    "[Generate Response] â† Final answer\n",
    "    â†“\n",
    "[Save Working Memory] â† Update conversation\n",
    "```\n",
    "\n",
    "### **Our 3 Tools:**\n",
    "\n",
    "1. **`search_courses`** - Semantic search over course catalog\n",
    "   - When: Student asks about courses, topics, or recommendations\n",
    "   - Example: \"What machine learning courses are available?\"\n",
    "\n",
    "2. **`search_memories`** - Search long-term memory for user facts\n",
    "   - When: Need to recall preferences, goals, or past interactions\n",
    "   - Example: \"What courses did I say I was interested in?\"\n",
    "\n",
    "3. **`store_memory`** - Save important information to long-term memory\n",
    "   - When: User shares preferences, goals, or important facts\n",
    "   - Example: \"I'm interested in AI and want to work at a startup\"\n",
    "\n",
    "### **Memory Architecture:**\n",
    "\n",
    "| Memory Type | Purpose | Managed By | Lifespan |\n",
    "|------------|---------|------------|----------|\n",
    "| **Working Memory** | Conversation history | Agent Memory Server | Session |\n",
    "| **Long-term Memory** | User preferences, facts | Agent Memory Server | Persistent |\n",
    "| **Graph State** | Current execution state | LangGraph | Single turn |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Setup and Environment\n",
    "\n",
    "### âš ï¸ **CRITICAL: Prerequisites Required**\n",
    "\n",
    "**This notebook requires ALL services to be running. If any service is down, the agent will not work.**\n",
    "\n",
    "**Required Services:**\n",
    "1. **Redis** - Vector storage and caching (port 6379)\n",
    "2. **Agent Memory Server** - Memory management (port 8088)\n",
    "3. **OpenAI API** - LLM functionality\n",
    "\n",
    "**ðŸš€ Quick Setup (Run this first!):**\n",
    "```bash\n",
    "# Navigate to notebooks_v2 directory\n",
    "cd ../../\n",
    "\n",
    "# Check if services are running\n",
    "./check_setup.sh\n",
    "\n",
    "# If services are down, run setup\n",
    "./setup_memory_server.sh\n",
    "```\n",
    "\n",
    "**ðŸ“– Need help?** See `../SETUP_GUIDE.md` for detailed setup instructions.\n",
    "\n",
    "**ðŸ” Manual Check:**\n",
    "- Redis: `redis-cli ping` should return `PONG`\n",
    "- Memory Server: `curl http://localhost:8088/v1/health` should return `{\"status\":\"ok\"}`\n",
    "- Environment: Create `.env` file in `reference-agent/` with your `OPENAI_API_KEY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-packages",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "### Automated Setup Check\n",
    "\n",
    "Let's run the setup script to ensure all services are running properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libraries",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:51.825255Z",
     "iopub.status.busy": "2025-10-31T23:57:51.825073Z",
     "iopub.status.idle": "2025-10-31T23:57:52.103012Z",
     "shell.execute_reply": "2025-10-31T23:57:52.102484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running automated setup check...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Agent Memory Server Setup\n",
      "===========================\n",
      "ðŸ“Š Checking Redis...\n",
      "âœ… Redis is running\n",
      "ðŸ“Š Checking Agent Memory Server...\n",
      "ðŸ” Agent Memory Server container exists. Checking health...\n",
      "âœ… Agent Memory Server is running and healthy\n",
      "âœ… No Redis connection issues detected\n",
      "\n",
      "âœ… Setup Complete!\n",
      "=================\n",
      "ðŸ“Š Services Status:\n",
      "   â€¢ Redis: Running on port 6379\n",
      "   â€¢ Agent Memory Server: Running on port 8088\n",
      "\n",
      "ðŸŽ¯ You can now run the notebooks!\n",
      "\n",
      "\n",
      "âœ… All services are ready!\n"
     ]
    }
   ],
   "source": [
    "# Run the setup script to ensure Redis and Agent Memory Server are running\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to setup script\n",
    "setup_script = Path(\"../../reference-agent/setup_agent_memory_server.py\")\n",
    "\n",
    "if setup_script.exists():\n",
    "    print(\"Running automated setup check...\\n\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(setup_script)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"âš ï¸  Setup check failed. Please review the output above.\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"\\nâœ… All services are ready!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Setup script not found. Please ensure services are running manually.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-env",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already installed the reference-agent package, uncomment and run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:52.104763Z",
     "iopub.status.busy": "2025-10-31T23:57:52.104657Z",
     "iopub.status.idle": "2025-10-31T23:57:52.106517Z",
     "shell.execute_reply": "2025-10-31T23:57:52.106037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install reference-agent package\n",
    "# %pip install -q -e ../../reference-agent\n",
    "\n",
    "# Uncomment to install agent-memory-client\n",
    "# %pip install -q agent-memory-client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-services",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "service-check",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:52.107702Z",
     "iopub.status.busy": "2025-10-31T23:57:52.107645Z",
     "iopub.status.idle": "2025-10-31T23:57:53.822487Z",
     "shell.execute_reply": "2025-10-31T23:57:53.821994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Annotated\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain and LangGraph\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Redis and Agent Memory\n",
    "from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "from agent_memory_client.models import WorkingMemory, MemoryMessage\n",
    "\n",
    "# Add reference-agent to path for course utilities\n",
    "sys.path.insert(0, os.path.abspath(\"../../reference-agent\"))\n",
    "from redis_context_course.course_manager import CourseManager\n",
    "from redis_context_course.models import StudentProfile, DifficultyLevel, CourseFormat\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-components",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "init-course-manager",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.823677Z",
     "iopub.status.busy": "2025-10-31T23:57:53.823553Z",
     "iopub.status.idle": "2025-10-31T23:57:53.826253Z",
     "shell.execute_reply": "2025-10-31T23:57:53.825901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured successfully!\n",
      "   OpenAI API Key: ********************wTMA\n",
      "   Redis URL: redis://localhost:6379\n",
      "   Agent Memory URL: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(\"../../reference-agent/.env\")\n",
    "\n",
    "# Get configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "\n",
    "# Verify OpenAI API key\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"\"\"\n",
    "    âš ï¸  OPENAI_API_KEY not found!\n",
    "\n",
    "    Please create a .env file in the reference-agent directory:\n",
    "    1. cd ../../reference-agent\n",
    "    2. cp .env.example .env\n",
    "    3. Edit .env and add your OpenAI API key\n",
    "    \"\"\")\n",
    "\n",
    "print(\"âœ… Environment configured successfully!\")\n",
    "print(f\"   OpenAI API Key: {'*' * 20}{OPENAI_API_KEY[-4:]}\")\n",
    "print(f\"   Redis URL: {REDIS_URL}\")\n",
    "print(f\"   Agent Memory URL: {AGENT_MEMORY_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "course-manager",
   "metadata": {},
   "source": [
    "### Check Required Services\n",
    "\n",
    "Let's verify that Redis and the Agent Memory Server are running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "init-llm",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.827385Z",
     "iopub.status.busy": "2025-10-31T23:57:53.827318Z",
     "iopub.status.idle": "2025-10-31T23:57:53.839615Z",
     "shell.execute_reply": "2025-10-31T23:57:53.839213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Redis is running\n",
      "âœ… Agent Memory Server is running\n",
      "\n",
      "âœ… All services are ready!\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import requests\n",
    "\n",
    "# Check Redis\n",
    "try:\n",
    "    redis_client = redis.from_url(REDIS_URL)\n",
    "    redis_client.ping()\n",
    "    print(\"âœ… Redis is running\")\n",
    "    REDIS_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Redis is not available: {e}\")\n",
    "    print(\"   Please start Redis using Docker:\")\n",
    "    print(\"   docker run -d -p 6379:6379 redis/redis-stack:latest\")\n",
    "    REDIS_AVAILABLE = False\n",
    "\n",
    "# Check Agent Memory Server\n",
    "try:\n",
    "    response = requests.get(f\"{AGENT_MEMORY_URL}/v1/health\", timeout=2)\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ… Agent Memory Server is running\")\n",
    "        MEMORY_SERVER_AVAILABLE = True\n",
    "    else:\n",
    "        print(f\"âš ï¸  Agent Memory Server returned status {response.status_code}\")\n",
    "        MEMORY_SERVER_AVAILABLE = False\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Agent Memory Server is not available: {e}\")\n",
    "    print(\"   Please start the Agent Memory Server:\")\n",
    "    print(\"   cd ../../reference-agent && python setup_agent_memory_server.py\")\n",
    "    MEMORY_SERVER_AVAILABLE = False\n",
    "\n",
    "if not (REDIS_AVAILABLE and MEMORY_SERVER_AVAILABLE):\n",
    "    print(\"\\nâš ï¸  Some services are not available. Please start them before continuing.\")\n",
    "else:\n",
    "    print(\"\\nâœ… All services are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-init",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Initialize Components\n",
    "\n",
    "Now let's initialize the components we'll use to build our agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-memory",
   "metadata": {},
   "source": [
    "### Initialize Course Manager\n",
    "\n",
    "The `CourseManager` handles course storage and semantic search, just like in Section 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "memory-init",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.840793Z",
     "iopub.status.busy": "2025-10-31T23:57:53.840727Z",
     "iopub.status.idle": "2025-10-31T23:57:53.933415Z",
     "shell.execute_reply": "2025-10-31T23:57:53.933012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:57:53 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Course Manager initialized\n",
      "   Ready to search and retrieve courses\n"
     ]
    }
   ],
   "source": [
    "# Initialize Course Manager\n",
    "course_manager = CourseManager()\n",
    "\n",
    "print(\"âœ… Course Manager initialized\")\n",
    "print(\"   Ready to search and retrieve courses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-profile",
   "metadata": {},
   "source": [
    "### Initialize LLM\n",
    "\n",
    "We'll use GPT-4o with temperature=0.0 for consistent, deterministic responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "create-student",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.934684Z",
     "iopub.status.busy": "2025-10-31T23:57:53.934605Z",
     "iopub.status.idle": "2025-10-31T23:57:53.943986Z",
     "shell.execute_reply": "2025-10-31T23:57:53.943698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM initialized\n",
      "   Model: gpt-4o\n",
      "   Temperature: 0.0 (deterministic)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "print(\"âœ… LLM initialized\")\n",
    "print(\"   Model: gpt-4o\")\n",
    "print(\"   Temperature: 0.0 (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-section",
   "metadata": {},
   "source": [
    "### Initialize Memory Client\n",
    "\n",
    "The memory client handles both working memory (conversation history) and long-term memory (persistent facts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tool-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.945184Z",
     "iopub.status.busy": "2025-10-31T23:57:53.945115Z",
     "iopub.status.idle": "2025-10-31T23:57:53.950020Z",
     "shell.execute_reply": "2025-10-31T23:57:53.949643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory Client initialized\n",
      "   Base URL: http://localhost:8088\n",
      "   Namespace: redis_university\n",
      "   Ready for working memory and long-term memory operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Memory Client\n",
    "config = MemoryClientConfig(\n",
    "    base_url=AGENT_MEMORY_URL,\n",
    "    default_namespace=\"redis_university\"\n",
    ")\n",
    "memory_client = MemoryAPIClient(config=config)\n",
    "\n",
    "print(\"âœ… Memory Client initialized\")\n",
    "print(f\"   Base URL: {config.base_url}\")\n",
    "print(f\"   Namespace: {config.default_namespace}\")\n",
    "print(\"   Ready for working memory and long-term memory operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-courses-tool",
   "metadata": {},
   "source": [
    "### Create Sample Student Profile\n",
    "\n",
    "We'll create a sample student to use throughout our demos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tool-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.951077Z",
     "iopub.status.busy": "2025-10-31T23:57:53.951016Z",
     "iopub.status.idle": "2025-10-31T23:57:53.953293Z",
     "shell.execute_reply": "2025-10-31T23:57:53.952950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Student profile created\n",
      "   Name: Sarah Chen\n",
      "   Student ID: student_sarah_001\n",
      "   Session ID: session_student_sarah_001_20251031_195753\n",
      "   Major: Computer Science\n",
      "   Interests: machine learning, data science, algorithms\n"
     ]
    }
   ],
   "source": [
    "# Create sample student profile\n",
    "STUDENT_ID = \"student_sarah_001\"\n",
    "SESSION_ID = f\"session_{STUDENT_ID}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "sarah = StudentProfile(\n",
    "    name=\"Sarah Chen\",\n",
    "    email=\"sarah.chen@university.edu\",\n",
    "    major=\"Computer Science\",\n",
    "    year=2,\n",
    "    interests=[\"machine learning\", \"data science\", \"algorithms\"],\n",
    "    completed_courses=[\"Introduction to Programming\", \"Data Structures\"],\n",
    "    current_courses=[\"Linear Algebra\"],\n",
    "    preferred_format=CourseFormat.ONLINE,\n",
    "    preferred_difficulty=DifficultyLevel.INTERMEDIATE\n",
    ")\n",
    "\n",
    "print(\"âœ… Student profile created\")\n",
    "print(f\"   Name: {sarah.name}\")\n",
    "print(f\"   Student ID: {STUDENT_ID}\")\n",
    "print(f\"   Session ID: {SESSION_ID}\")\n",
    "print(f\"   Major: {sarah.major}\")\n",
    "print(f\"   Interests: {', '.join(sarah.interests)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-memories-tool",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ› ï¸ Part 1: Define the Agent's Tools\n",
    "\n",
    "Let's build our 3 tools step by step. Each tool will have:\n",
    "- Clear input schema (what parameters it accepts)\n",
    "- Descriptive docstring (tells the LLM when to use it)\n",
    "- Implementation (the actual logic)\n",
    "\n",
    "**Remember:** The LLM only sees the tool name, description, and parametersâ€”not the implementation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool-3",
   "metadata": {},
   "source": [
    "### Tool 1: `search_courses`\n",
    "\n",
    "This tool searches the course catalog using semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "store-memory-tool",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.954314Z",
     "iopub.status.busy": "2025-10-31T23:57:53.954256Z",
     "iopub.status.idle": "2025-10-31T23:57:53.957045Z",
     "shell.execute_reply": "2025-10-31T23:57:53.956679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool 1 defined: search_courses\n",
      "   Purpose: Search course catalog with semantic search\n",
      "   Parameters: query (str), limit (int)\n"
     ]
    }
   ],
   "source": [
    "# Define input schema\n",
    "class SearchCoursesInput(BaseModel):\n",
    "    \"\"\"Input schema for searching courses.\"\"\"\n",
    "    query: str = Field(\n",
    "        description=\"Natural language search query. Can be topics (e.g., 'machine learning'), \"\n",
    "                    \"characteristics (e.g., 'online courses'), or general questions \"\n",
    "                    \"(e.g., 'beginner programming courses')\"\n",
    "    )\n",
    "    limit: int = Field(\n",
    "        default=5,\n",
    "        description=\"Maximum number of results to return. Default is 5. \"\n",
    "                    \"Use 3 for quick answers, 10 for comprehensive results.\"\n",
    "    )\n",
    "\n",
    "# Define the tool\n",
    "@tool(\"search_courses\", args_schema=SearchCoursesInput)\n",
    "async def search_courses(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search for courses using semantic search based on topics, descriptions, or characteristics.\n",
    "\n",
    "    Use this tool when students ask about:\n",
    "    - Topics or subjects: \"machine learning courses\", \"database courses\"\n",
    "    - Course characteristics: \"online courses\", \"beginner courses\", \"3-credit courses\"\n",
    "    - General exploration: \"what courses are available in AI?\"\n",
    "\n",
    "    The search uses semantic matching, so natural language queries work well.\n",
    "\n",
    "    Returns: Formatted list of matching courses with details.\n",
    "    \"\"\"\n",
    "    results = await course_manager.search_courses(query, limit=limit)\n",
    "\n",
    "    if not results:\n",
    "        return \"No courses found matching your query.\"\n",
    "\n",
    "    output = []\n",
    "    for course in results:\n",
    "        output.append(\n",
    "            f\"{course.course_code}: {course.title}\\n\"\n",
    "            f\"  Credits: {course.credits} | {course.format.value} | {course.difficulty_level.value}\\n\"\n",
    "            f\"  {course.description[:150]}...\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n",
    "print(\"âœ… Tool 1 defined: search_courses\")\n",
    "print(\"   Purpose: Search course catalog with semantic search\")\n",
    "print(\"   Parameters: query (str), limit (int)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-summary",
   "metadata": {},
   "source": [
    "### Tool 2: `search_memories`\n",
    "\n",
    "This tool searches long-term memory for user preferences and facts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "list-tools",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.958090Z",
     "iopub.status.busy": "2025-10-31T23:57:53.958029Z",
     "iopub.status.idle": "2025-10-31T23:57:53.960900Z",
     "shell.execute_reply": "2025-10-31T23:57:53.960462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool 2 defined: search_memories\n",
      "   Purpose: Search long-term memory for user facts\n",
      "   Parameters: query (str), limit (int)\n"
     ]
    }
   ],
   "source": [
    "# Define input schema\n",
    "class SearchMemoriesInput(BaseModel):\n",
    "    \"\"\"Input schema for searching memories.\"\"\"\n",
    "    query: str = Field(\n",
    "        description=\"Natural language query to search for in user's long-term memory. \"\n",
    "                    \"Examples: 'career goals', 'course preferences', 'learning style'\"\n",
    "    )\n",
    "    limit: int = Field(\n",
    "        default=5,\n",
    "        description=\"Maximum number of memories to return. Default is 5.\"\n",
    "    )\n",
    "\n",
    "# Define the tool\n",
    "@tool(\"search_memories\", args_schema=SearchMemoriesInput)\n",
    "async def search_memories(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the user's long-term memory for relevant facts, preferences, and past interactions.\n",
    "\n",
    "    Use this tool when you need to:\n",
    "    - Recall user preferences: \"What format does the user prefer?\"\n",
    "    - Remember past goals: \"What career path is the user interested in?\"\n",
    "    - Find previous interactions: \"What courses did we discuss before?\"\n",
    "    - Personalize recommendations: \"What are the user's interests?\"\n",
    "\n",
    "    The search uses semantic matching to find relevant memories.\n",
    "\n",
    "    Returns: List of relevant memories with content and metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from agent_memory_client.filters import UserId\n",
    "\n",
    "        # Search long-term memory\n",
    "        results = await memory_client.search_long_term_memory(\n",
    "            text=query,\n",
    "            user_id=UserId(eq=STUDENT_ID),\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "        if not results.memories or len(results.memories) == 0:\n",
    "            return \"No relevant memories found.\"\n",
    "\n",
    "        output = []\n",
    "        for i, memory in enumerate(results.memories, 1):\n",
    "            output.append(f\"{i}. {memory.text}\")\n",
    "            if memory.topics:\n",
    "                output.append(f\"   Topics: {', '.join(memory.topics)}\")\n",
    "\n",
    "        return \"\\n\".join(output)\n",
    "    except Exception as e:\n",
    "        return f\"Error searching memories: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Tool 2 defined: search_memories\")\n",
    "print(\"   Purpose: Search long-term memory for user facts\")\n",
    "print(\"   Parameters: query (str), limit (int)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-state",
   "metadata": {},
   "source": [
    "### Tool 3: `store_memory`\n",
    "\n",
    "This tool saves important information to long-term memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "define-state",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.962062Z",
     "iopub.status.busy": "2025-10-31T23:57:53.961995Z",
     "iopub.status.idle": "2025-10-31T23:57:53.964832Z",
     "shell.execute_reply": "2025-10-31T23:57:53.964534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool 3 defined: store_memory\n",
      "   Purpose: Save important facts to long-term memory\n",
      "   Parameters: text (str), memory_type (str), topics (List[str])\n"
     ]
    }
   ],
   "source": [
    "# Define input schema\n",
    "class StoreMemoryInput(BaseModel):\n",
    "    \"\"\"Input schema for storing memories.\"\"\"\n",
    "    text: str = Field(\n",
    "        description=\"The information to store. Should be a clear, factual statement. \"\n",
    "                    \"Examples: 'User prefers online courses', 'User's career goal is AI research'\"\n",
    "    )\n",
    "    memory_type: str = Field(\n",
    "        default=\"semantic\",\n",
    "        description=\"Type of memory: 'semantic' (facts/preferences), 'episodic' (events/interactions). \"\n",
    "                    \"Default is 'semantic'.\"\n",
    "    )\n",
    "    topics: List[str] = Field(\n",
    "        default=[],\n",
    "        description=\"Optional tags to categorize the memory, such as ['preferences', 'courses']\"\n",
    "    )\n",
    "\n",
    "# Define the tool\n",
    "@tool(\"store_memory\", args_schema=StoreMemoryInput)\n",
    "async def store_memory(text: str, memory_type: str = \"semantic\", topics: List[str] = []) -> str:\n",
    "    \"\"\"\n",
    "    Store important information to the user's long-term memory.\n",
    "\n",
    "    Use this tool when the user shares:\n",
    "    - Preferences: \"I prefer online courses\", \"I like hands-on projects\"\n",
    "    - Goals: \"I want to work in AI\", \"I'm preparing for grad school\"\n",
    "    - Important facts: \"I have a part-time job\", \"I'm interested in startups\"\n",
    "    - Constraints: \"I can only take 2 courses per semester\"\n",
    "\n",
    "    Do NOT store:\n",
    "    - Temporary information (use conversation context instead)\n",
    "    - Course details (already in course catalog)\n",
    "    - General questions\n",
    "\n",
    "    Returns: Confirmation message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from agent_memory_client.models import ClientMemoryRecord\n",
    "\n",
    "        # Create memory record\n",
    "        memory = ClientMemoryRecord(\n",
    "            text=text,\n",
    "            user_id=STUDENT_ID,\n",
    "            memory_type=memory_type,\n",
    "            topics=topics or []\n",
    "        )\n",
    "\n",
    "        # Store in long-term memory\n",
    "        await memory_client.create_long_term_memory([memory])\n",
    "        return f\"âœ… Stored to long-term memory: {text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error storing memory: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Tool 3 defined: store_memory\")\n",
    "print(\"   Purpose: Save important facts to long-term memory\")\n",
    "print(\"   Parameters: text (str), memory_type (str), topics (List[str])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-nodes",
   "metadata": {},
   "source": [
    "### Tools Summary\n",
    "\n",
    "Let's review our 3 tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "load-memory-node",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.966158Z",
     "iopub.status.busy": "2025-10-31T23:57:53.966078Z",
     "iopub.status.idle": "2025-10-31T23:57:53.968399Z",
     "shell.execute_reply": "2025-10-31T23:57:53.968046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ› ï¸  AGENT TOOLS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. search_courses\n",
      "   Description: Search for courses using semantic search based on topics, descriptions, or characteristics\n",
      "   Parameters: query, limit\n",
      "\n",
      "2. search_memories\n",
      "   Description: Search the user's long-term memory for relevant facts, preferences, and past interactions\n",
      "   Parameters: query, limit\n",
      "\n",
      "3. store_memory\n",
      "   Description: Store important information to the user's long-term memory\n",
      "   Parameters: text, memory_type, topics\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Collect all tools\n",
    "tools = [search_courses, search_memories, store_memory]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ› ï¸  AGENT TOOLS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "for i, tool in enumerate(tools, 1):\n",
    "    print(f\"\\n{i}. {tool.name}\")\n",
    "    print(f\"   Description: {tool.description.split('.')[0]}\")\n",
    "    print(f\"   Parameters: {', '.join(tool.args_schema.model_fields.keys())}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-node",
   "metadata": {},
   "source": "\n"
  },
  {
   "cell_type": "markdown",
   "id": "save-memory-node",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.969443Z",
     "iopub.status.busy": "2025-10-31T23:57:53.969382Z",
     "iopub.status.idle": "2025-10-31T23:57:53.971457Z",
     "shell.execute_reply": "2025-10-31T23:57:53.971109Z"
    }
   },
   "source": [
    "## ðŸ§  Memory Extraction in This Agent\n",
    "\n",
    "Understanding how this agent creates and manages long-term memories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "routing-logic",
   "metadata": {},
   "source": [
    "### How This Agent Uses Memory\n",
    "\n",
    "Our agent has 3 tools, and 2 of them interact with memory:\n",
    "\n",
    "1. **`store_memory`** - Saves facts to long-term memory\n",
    "2. **`search_memories`** - Retrieves facts from long-term memory\n",
    "3. **`search_courses`** - Searches course catalog (not memory-related)\n",
    "\n",
    "**Question:** When the agent calls `store_memory`, how does the Agent Memory Server decide what to extract and how to structure it?\n",
    "\n",
    "**Answer:** Memory Extraction Strategies (covered in Section 3, Notebook 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "should-continue",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.972503Z",
     "iopub.status.busy": "2025-10-31T23:57:53.972440Z",
     "iopub.status.idle": "2025-10-31T23:57:53.974986Z",
     "shell.execute_reply": "2025-10-31T23:57:53.974616Z"
    }
   },
   "source": [
    "### Current Configuration: Discrete Strategy (Default)\n",
    "\n",
    "**This agent uses the DISCRETE strategy** (default) because:\n",
    "\n",
    "âœ… **Individual facts are searchable**\n",
    "- \"User's major is Computer Science\"\n",
    "- \"User interested in machine learning\"\n",
    "- \"User completed RU101\"\n",
    "\n",
    "âœ… **Facts are independently useful**\n",
    "- Agent can search for specific facts\n",
    "- Each fact has its own relevance score\n",
    "- No need to parse summaries\n",
    "\n",
    "âœ… **Good for Q&A interactions**\n",
    "- Student: \"What courses did I say I was interested in?\"\n",
    "- Agent searches discrete facts: \"User interested in ML\", \"User interested in AI\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-graph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.975927Z",
     "iopub.status.busy": "2025-10-31T23:57:53.975854Z",
     "iopub.status.idle": "2025-10-31T23:57:53.977825Z",
     "shell.execute_reply": "2025-10-31T23:57:53.977580Z"
    }
   },
   "source": [
    "### Example: Discrete Strategy in Action\n",
    "\n",
    "**Conversation:**\n",
    "```\n",
    "User: \"I'm a CS major interested in ML. I prefer online courses.\"\n",
    "Agent: [Calls store_memory tool]\n",
    "```\n",
    "\n",
    "**What Gets Stored (Discrete Strategy):**\n",
    "```json\n",
    "[\n",
    "  {\"text\": \"User's major is Computer Science\", \"type\": \"semantic\"},\n",
    "  {\"text\": \"User interested in machine learning\", \"type\": \"semantic\"},\n",
    "  {\"text\": \"User prefers online courses\", \"type\": \"semantic\"}\n",
    "]\n",
    "```\n",
    "\n",
    "**Later:**\n",
    "```\n",
    "User: \"What courses match my interests?\"\n",
    "Agent: [Calls search_memories tool]\n",
    "       â†’ Finds: \"User interested in machine learning\"\n",
    "       â†’ Finds: \"User prefers online courses\"\n",
    "       [Calls search_courses with these preferences]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "construct-graph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.978903Z",
     "iopub.status.busy": "2025-10-31T23:57:53.978835Z",
     "iopub.status.idle": "2025-10-31T23:57:53.981202Z",
     "shell.execute_reply": "2025-10-31T23:57:53.980864Z"
    }
   },
   "source": [
    "### When Would Summary Strategy Be Better?\n",
    "\n",
    "**Summary strategy** would be beneficial for:\n",
    "\n",
    "**Scenario 1: Long Advising Sessions**\n",
    "```\n",
    "User has 30-minute conversation discussing:\n",
    "- Academic goals\n",
    "- Career aspirations\n",
    "- Course preferences\n",
    "- Schedule constraints\n",
    "- Graduation timeline\n",
    "```\n",
    "\n",
    "**Discrete Strategy:** Extracts 20+ individual facts\n",
    "**Summary Strategy:** Creates 1-2 comprehensive summaries preserving context\n",
    "\n",
    "**Scenario 2: Session Notes**\n",
    "```\n",
    "Agent: \"Let me summarize our conversation today...\"\n",
    "[Retrieves summary memory instead of reconstructing from discrete facts]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-graph",
   "metadata": {},
   "source": [
    "### Configuration Example (Not Used in This Notebook)\n",
    "\n",
    "If you wanted to use summary strategy instead:\n",
    "\n",
    "```python\n",
    "from agent_memory_client.models import MemoryStrategyConfig\n",
    "\n",
    "# Configure summary strategy\n",
    "summary_strategy = MemoryStrategyConfig(\n",
    "    strategy=\"summary\",\n",
    "    config={\"max_summary_length\": 500}\n",
    ")\n",
    "\n",
    "# Apply when creating working memory\n",
    "await memory_client.set_working_memory(\n",
    "    session_id=session_id,\n",
    "    messages=messages,\n",
    "    long_term_memory_strategy=summary_strategy  # â† Use summary instead of discrete\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "show-graph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.982174Z",
     "iopub.status.busy": "2025-10-31T23:57:53.982118Z",
     "iopub.status.idle": "2025-10-31T23:57:53.983908Z",
     "shell.execute_reply": "2025-10-31T23:57:53.983535Z"
    }
   },
   "source": [
    "### Why We Stick with Discrete (Default)\n",
    "\n",
    "For this course advisor agent:\n",
    "- âœ… Questions are specific (\"What are prerequisites for RU301?\")\n",
    "- âœ… Facts are independently useful\n",
    "- âœ… Search works better with discrete facts\n",
    "- âœ… No configuration needed (default behavior)\n",
    "\n",
    "**In production**, you might:\n",
    "- Use **discrete** for most interactions (default)\n",
    "- Use **summary** for end-of-session notes\n",
    "- Use **preferences** during student onboarding\n",
    "- Use **custom** for specialized academic domains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-section",
   "metadata": {},
   "source": [
    "### ðŸ”— Connection to Section 3\n",
    "\n",
    "In **Section 3, Notebook 1**, we introduced memory extraction strategies conceptually.\n",
    "\n",
    "In **Section 3, Notebook 2**, we demonstrated the difference between discrete and summary strategies with hands-on examples.\n",
    "\n",
    "**Now in Section 4**, we see how a production agent uses the discrete strategy (default) for course advising.\n",
    "\n",
    "**Key Takeaway:** The Agent Memory Server's memory extraction strategies give you flexibility in HOW memories are created, but for most agent interactions (like this course advisor), the default discrete strategy works best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-agent-helper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.984807Z",
     "iopub.status.busy": "2025-10-31T23:57:53.984751Z",
     "iopub.status.idle": "2025-10-31T23:57:53.990038Z",
     "shell.execute_reply": "2025-10-31T23:57:53.989670Z"
    }
   },
   "source": [
    "### ðŸ“š Learn More\n",
    "\n",
    "- [Memory Extraction Strategies Documentation](https://redis.github.io/agent-memory-server/memory-extraction-strategies/)\n",
    "- [Section 3, Notebook 1](../section-3-memory-systems-for-context-engineering/01_working_and_longterm_memory.ipynb) - Theory foundation\n",
    "- [Section 3, Notebook 2](../section-3-memory-systems-for-context-engineering/02_combining_memory_with_retrieved_context.ipynb) - Hands-on comparison demo\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¨ Part 2: Define the Agent State\n",
    "\n",
    "In LangGraph, **state** is the shared data structure that flows through the graph. Each node can read from and write to the state.\n",
    "\n",
    "### What Goes in State?\n",
    "\n",
    "- **messages**: Conversation history (automatically managed by LangGraph)\n",
    "- **student_id**: Who we're helping\n",
    "- **session_id**: Current conversation session\n",
    "- **context**: Additional context (memories, preferences, etc.)\n",
    "\n",
    "**Note:** We use `Annotated[List[BaseMessage], add_messages]` for messages. The `add_messages` reducer automatically handles message deduplication and ordering.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-1",
   "metadata": {},
   "source": [
    "# Define the agent state\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"State for the course advisor agent.\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    student_id: str\n",
    "    session_id: str\n",
    "    context: Dict[str, Any] = {}\n",
    "\n",
    "print(\"âœ… Agent state defined\")\n",
    "print(\"   Fields: messages, student_id, session_id, context\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "demo-search",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:53.991081Z",
     "iopub.status.busy": "2025-10-31T23:57:53.991018Z",
     "iopub.status.idle": "2025-10-31T23:57:54.095976Z",
     "shell.execute_reply": "2025-10-31T23:57:54.095530Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— Part 3: Build the Agent Graph\n",
    "\n",
    "Now we'll build the LangGraph workflow. Our graph will have:\n",
    "\n",
    "1. **load_memory** - Load working memory (conversation history)\n",
    "2. **agent** - LLM decides what to do (call tools or respond)\n",
    "3. **tools** - Execute tool calls\n",
    "4. **save_memory** - Save updated conversation to working memory\n",
    "\n",
    "### Step 1: Define Node Functions\n",
    "\n",
    "Each node is a function that takes state and returns updated state.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-2",
   "metadata": {},
   "source": [
    "# Node 1: Load working memory\n",
    "async def load_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Load conversation history from working memory.\n",
    "\n",
    "    This gives the agent context about previous interactions in this session.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get or create working memory for this session\n",
    "        _, working_memory = await memory_client.get_or_create_working_memory(\n",
    "            session_id=state.session_id,\n",
    "            user_id=state.student_id,\n",
    "            model_name=\"gpt-4o\"\n",
    "        )\n",
    "\n",
    "        if working_memory and working_memory.messages:\n",
    "            # Convert stored messages to LangChain message objects\n",
    "            loaded_messages = []\n",
    "            for msg in working_memory.messages:\n",
    "                if msg.role == 'user':\n",
    "                    loaded_messages.append(HumanMessage(content=msg.content))\n",
    "                elif msg.role == 'assistant':\n",
    "                    loaded_messages.append(AIMessage(content=msg.content))\n",
    "\n",
    "            # Add loaded messages to state (prepend to current messages)\n",
    "            state.messages = loaded_messages + state.messages\n",
    "            state.context['memory_loaded'] = True\n",
    "            print(f\"   Loaded {len(loaded_messages)} messages from working memory\")\n",
    "        else:\n",
    "            state.context['memory_loaded'] = False\n",
    "            print(\"   No previous conversation found (new session)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Could not load memory: {e}\")\n",
    "        state.context['memory_loaded'] = False\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"âœ… Node 1 defined: load_memory\")\n",
    "print(\"   Purpose: Load conversation history from working memory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "demo-store",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:54.097563Z",
     "iopub.status.busy": "2025-10-31T23:57:54.097461Z",
     "iopub.status.idle": "2025-10-31T23:57:54.100763Z",
     "shell.execute_reply": "2025-10-31T23:57:54.100208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper function defined: run_agent\n"
     ]
    }
   ],
   "source": [
    "# Node 2: Agent (LLM with tools)\n",
    "async def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    The agent decides what to do: call tools or respond to the user.\n",
    "\n",
    "    This is where the LLM reasoning happens.\n",
    "    \"\"\"\n",
    "    # Create system message with instructions\n",
    "    system_message = SystemMessage(content=\"\"\"\n",
    "You are a helpful Redis University course advisor assistant.\n",
    "\n",
    "Your role:\n",
    "- Help students find courses that match their interests and goals\n",
    "- Remember student preferences and use them for personalized recommendations\n",
    "- Store important information about students for future conversations\n",
    "\n",
    "Guidelines:\n",
    "- Use search_courses to find relevant courses\n",
    "- Use search_memories to recall student preferences and past interactions\n",
    "- Use store_memory when students share important preferences, goals, or constraints\n",
    "- Be conversational and helpful\n",
    "- Provide specific course recommendations with details\n",
    "\"\"\")\n",
    "\n",
    "    # Bind tools to LLM\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # Call LLM with system message + conversation history\n",
    "    messages = [system_message] + state.messages\n",
    "    response = await llm_with_tools.ainvoke(messages)\n",
    "\n",
    "    # Add response to state\n",
    "    state.messages.append(response)\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"âœ… Node 2 defined: agent_node\")\n",
    "print(\"   Purpose: LLM decides whether to call tools or respond\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-3",
   "metadata": {},
   "source": [
    "# Node 3: Save working memory\n",
    "async def save_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Save the updated conversation to working memory.\n",
    "\n",
    "    This ensures continuity across conversation turns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get or create working memory\n",
    "        _, working_memory = await memory_client.get_or_create_working_memory(\n",
    "            session_id=state.session_id,\n",
    "            user_id=state.student_id,\n",
    "            model_name=\"gpt-4o\"\n",
    "        )\n",
    "\n",
    "        # Clear existing messages and add current conversation\n",
    "        working_memory.messages = []\n",
    "        for msg in state.messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                working_memory.messages.append(MemoryMessage(role='user', content=msg.content))\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                # Only store text content, not tool calls\n",
    "                if msg.content:\n",
    "                    working_memory.messages.append(MemoryMessage(role='assistant', content=msg.content))\n",
    "\n",
    "        # Save to working memory\n",
    "        await memory_client.put_working_memory(\n",
    "            session_id=state.session_id,\n",
    "            memory=working_memory,\n",
    "            user_id=state.student_id,\n",
    "            model_name=\"gpt-4o\"\n",
    "        )\n",
    "\n",
    "        print(f\"   Saved {len(working_memory.messages)} messages to working memory\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Could not save memory: {e}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"âœ… Node 3 defined: save_memory\")\n",
    "print(\"   Purpose: Save conversation to working memory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "demo-recall",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:54.102049Z",
     "iopub.status.busy": "2025-10-31T23:57:54.101962Z",
     "iopub.status.idle": "2025-10-31T23:57:58.356458Z",
     "shell.execute_reply": "2025-10-31T23:57:58.355667Z"
    }
   },
   "source": [
    "### Step 2: Define Routing Logic\n",
    "\n",
    "We need a function to decide: should we call tools or end the conversation?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-4",
   "metadata": {},
   "source": [
    "# Routing function\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determine if we should continue to tools or end.\n",
    "\n",
    "    If the last message has tool calls, route to tools.\n",
    "    Otherwise, we're done.\n",
    "    \"\"\"\n",
    "    last_message = state.messages[-1]\n",
    "\n",
    "    # Check if there are tool calls\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"save_memory\"\n",
    "\n",
    "print(\"âœ… Routing logic defined: should_continue\")\n",
    "print(\"   Routes to 'tools' if LLM wants to call tools, otherwise to 'save_memory'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "demo-personalized",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:57:58.358447Z",
     "iopub.status.busy": "2025-10-31T23:57:58.358312Z",
     "iopub.status.idle": "2025-10-31T23:58:04.410189Z",
     "shell.execute_reply": "2025-10-31T23:58:04.409512Z"
    }
   },
   "source": [
    "### Step 3: Build the Graph\n",
    "\n",
    "Now we assemble all the pieces into a LangGraph workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect-memory",
   "metadata": {},
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"load_memory\", load_memory)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "workflow.add_node(\"save_memory\", save_memory)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"load_memory\")\n",
    "workflow.add_edge(\"load_memory\", \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"save_memory\": \"save_memory\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")  # After tools, go back to agent\n",
    "workflow.add_edge(\"save_memory\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent_graph = workflow.compile()\n",
    "\n",
    "print(\"âœ… Agent graph built and compiled!\")\n",
    "print(\"\\nðŸ“Š Graph structure:\")\n",
    "print(\"   START â†’ load_memory â†’ agent â†’ [tools â†’ agent]* â†’ save_memory â†’ END\")\n",
    "print(\"\\n   * The agent can call tools multiple times before responding\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "check-memories",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:58:04.411898Z",
     "iopub.status.busy": "2025-10-31T23:58:04.411768Z",
     "iopub.status.idle": "2025-10-31T23:58:06.565467Z",
     "shell.execute_reply": "2025-10-31T23:58:06.564738Z"
    }
   },
   "source": [
    "### Step 4: Visualize the Graph\n",
    "\n",
    "Let's see what our agent workflow looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "comparison",
   "metadata": {},
   "source": [
    "# Try to visualize the graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    # Generate graph visualization\n",
    "    graph_image = agent_graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(graph_image))\n",
    "    print(\"\\nâœ… Graph visualization displayed above\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not display graph visualization: {e}\")\n",
    "    print(\"\\nGraph structure (text):\")\n",
    "    print(\"\"\"\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   START     â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ load_memory â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚    agent    â”‚ â—„â”€â”€â”€â”€â”€â”\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "           â”‚              â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”‚\n",
    "      â”‚         â”‚         â”‚\n",
    "      â–¼         â–¼         â”‚\n",
    "   [tools]  [respond]     â”‚\n",
    "      â”‚                   â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚ save_memory â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚     END     â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "architecture-recap",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:58:06.567416Z",
     "iopub.status.busy": "2025-10-31T23:58:06.567279Z",
     "iopub.status.idle": "2025-10-31T23:58:11.047325Z",
     "shell.execute_reply": "2025-10-31T23:58:11.046775Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¬ Part 4: Demo the Agent\n",
    "\n",
    "Now let's see our agent in action! We'll have a conversation with the agent and watch it:\n",
    "- Search for courses\n",
    "- Store memories about preferences\n",
    "- Recall information from previous interactions\n",
    "\n",
    "### Helper Function: Run Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "async def run_agent(user_message: str, verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Run the agent with a user message.\n",
    "\n",
    "    Args:\n",
    "        user_message: The user's input\n",
    "        verbose: Whether to print detailed execution info\n",
    "\n",
    "    Returns:\n",
    "        The agent's response\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ðŸ‘¤ USER: {user_message}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Create initial state\n",
    "    initial_state = AgentState(\n",
    "        messages=[HumanMessage(content=user_message)],\n",
    "        student_id=STUDENT_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        context={}\n",
    "    )\n",
    "\n",
    "    # Run the graph\n",
    "    if verbose:\n",
    "        print(\"\\nðŸ¤– AGENT EXECUTION:\")\n",
    "\n",
    "    final_state = await agent_graph.ainvoke(initial_state)\n",
    "\n",
    "    # Extract the final response\n",
    "    final_message = final_state[\"messages\"][-1]\n",
    "    response = final_message.content if hasattr(final_message, 'content') else str(final_message)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"ðŸ¤– ASSISTANT: {response}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"âœ… Helper function defined: run_agent\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T23:58:11.049386Z",
     "iopub.status.busy": "2025-10-31T23:58:11.049237Z",
     "iopub.status.idle": "2025-10-31T23:58:11.464715Z",
     "shell.execute_reply": "2025-10-31T23:58:11.464089Z"
    }
   },
   "source": [
    "### Demo 1: Search Courses\n",
    "\n",
    "Let's ask the agent to find machine learning courses.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "# Demo 1: Search for courses\n",
    "response1 = await run_agent(\n",
    "    \"What machine learning courses are available? I'm interested in intermediate level courses.\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8c8b43a1a04fff3",
   "metadata": {},
   "source": [
    "### Demo 2: Store Preferences\n",
    "\n",
    "Now let's share some preferences and watch the agent store them.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "97d4b563a3a30240",
   "metadata": {},
   "source": [
    "# Demo 2: Store preferences\n",
    "response2 = await run_agent(\n",
    "    \"I prefer online courses because I have a part-time job. \"\n",
    "    \"Also, I'm really interested in AI and want to work at a startup after graduation.\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2fc05bfee7ece66",
   "metadata": {},
   "source": [
    "### Demo 3: Recall Memories\n",
    "\n",
    "Let's ask the agent to recall what it knows about us.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "437746891b606882",
   "metadata": {},
   "source": [
    "# Demo 3: Recall memories\n",
    "response3 = await run_agent(\n",
    "    \"What do you remember about my preferences and goals?\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d495052317c67bb",
   "metadata": {},
   "source": [
    "### Demo 4: Personalized Recommendations\n",
    "\n",
    "Now let's ask for recommendations and see if the agent uses our stored preferences.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Demo 4: Personalized recommendations\n",
    "response4 = await run_agent(\n",
    "    \"Can you recommend some courses for next semester based on what you know about me?\"\n",
    ")"
   ],
   "id": "3eb0f6ddeb45a9f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Inspect Stored Memories\n",
    "\n",
    "Let's look at what's actually stored in long-term memory.\n"
   ],
   "id": "17dd61ca397db6be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check what's in long-term memory\n",
    "try:\n",
    "    from agent_memory_client.filters import UserId\n",
    "\n",
    "    results = await memory_client.search_long_term_memory(\n",
    "        text=\"preferences goals interests\",\n",
    "        user_id=UserId(eq=STUDENT_ID),\n",
    "        limit=10\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸ’¾ LONG-TERM MEMORY CONTENTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if results.memories and len(results.memories) > 0:\n",
    "        for i, memory in enumerate(results.memories, 1):\n",
    "            print(f\"\\n{i}. [{memory.memory_type}] {memory.text}\")\n",
    "            if memory.topics:\n",
    "                print(f\"   Topics: {', '.join(memory.topics)}\")\n",
    "            if memory.created_at:\n",
    "                print(f\"   Created: {memory.created_at}\")\n",
    "    else:\n",
    "        print(\"\\nNo memories found.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving memories: {e}\")"
   ],
   "id": "19a91887b957f48c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š Part 5: RAG vs Agent Comparison\n",
    "\n",
    "Let's compare what we've built across the sections:\n",
    "\n",
    "### **Section 2: Basic RAG**\n",
    "```python\n",
    "# Simple flow\n",
    "query â†’ search_courses() â†’ generate_response()\n",
    "```\n",
    "- âœ… Can retrieve course information\n",
    "- âŒ No memory of previous interactions\n",
    "- âŒ Can't store user preferences\n",
    "- âŒ Single-step only\n",
    "\n",
    "### **Section 3: Memory-Enhanced RAG**\n",
    "```python\n",
    "# With memory\n",
    "load_memory() â†’ search_courses() â†’ generate_response() â†’ save_memory()\n",
    "```\n",
    "- âœ… Remembers conversation history\n",
    "- âœ… Can reference previous messages\n",
    "- âš ï¸  Limited to predefined flow\n",
    "- âŒ Can't decide when to store memories\n",
    "\n",
    "### **Section 4: Full Agent (This Notebook)**\n",
    "```python\n",
    "# Agent with tools and decision-making\n",
    "load_memory() â†’ agent_decides() â†’ [search_courses | search_memories | store_memory]* â†’ save_memory()\n",
    "```\n",
    "- âœ… Remembers conversation history\n",
    "- âœ… Decides when to search courses\n",
    "- âœ… Decides when to store memories\n",
    "- âœ… Decides when to recall memories\n",
    "- âœ… Can chain multiple operations\n",
    "- âœ… Adaptive to user needs\n",
    "\n",
    "### **Key Differences:**\n",
    "\n",
    "| Feature | RAG | Memory-RAG | Agent |\n",
    "|---------|-----|------------|-------|\n",
    "| **Retrieval** | âœ… | âœ… | âœ… |\n",
    "| **Conversation Memory** | âŒ | âœ… | âœ… |\n",
    "| **Long-term Memory** | âŒ | âš ï¸ (manual) | âœ… (automatic) |\n",
    "| **Decision Making** | âŒ | âŒ | âœ… |\n",
    "| **Multi-step Reasoning** | âŒ | âŒ | âœ… |\n",
    "| **Tool Selection** | âŒ | âŒ | âœ… |\n",
    "| **Complexity** | Low | Medium | High |\n",
    "| **Latency** | Low | Medium | Higher |\n",
    "| **Cost** | Low | Medium | Higher |\n",
    "\n",
    "**ðŸ’¡ Key Insight:** Agents add decision-making and multi-step reasoning to RAG systems.\n"
   ],
   "id": "fd45b11038775302"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ Architecture Recap\n",
    "\n",
    "### **What We Built:**\n",
    "\n",
    "A complete course advisor agent with:\n",
    "\n",
    "**1. Tools (3 total)**\n",
    "- `search_courses` - Semantic search over course catalog\n",
    "- `search_memories` - Recall user preferences and facts\n",
    "- `store_memory` - Save important information\n",
    "\n",
    "**2. Memory Architecture**\n",
    "- **Working Memory** - Conversation history (session-scoped)\n",
    "- **Long-term Memory** - User preferences and facts (persistent)\n",
    "- **Graph State** - Current execution state (turn-scoped)\n",
    "\n",
    "**3. LangGraph Workflow**\n",
    "- **Nodes**: load_memory, agent, tools, save_memory\n",
    "- **Edges**: Conditional routing based on LLM decisions\n",
    "- **State**: Shared data structure flowing through the graph\n",
    "\n",
    "**4. Integration Points**\n",
    "- **Redis** - Course catalog storage and vector search\n",
    "- **Agent Memory Server** - Working and long-term memory\n",
    "- **OpenAI** - LLM for reasoning and tool selection\n",
    "- **LangGraph** - Workflow orchestration\n",
    "\n",
    "### **The Complete Context Engineering Stack:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    AGENT LAYER                          â”‚\n",
    "â”‚  (LangGraph orchestration + tool selection)             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚            â”‚            â”‚\n",
    "        â–¼            â–¼            â–¼\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Tools  â”‚  â”‚ Memory  â”‚  â”‚   RAG   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚            â”‚            â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â–¼\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚  Redis Stack    â”‚\n",
    "            â”‚  (Storage +     â”‚\n",
    "            â”‚   Vector Search)â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "\n"
   ],
   "id": "d4a533d945ca605e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Part 6: Working Memory Compression for Long Conversations\n",
    "\n",
    "Now that we have a working agent, let's address a production challenge: **What happens when conversations get very long?**\n"
   ],
   "id": "c4654c5a2c4e5323"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ”— Connection to Section 3, Notebook 3\n",
    "\n",
    "In **Section 3, Notebook 3**, we learned about working memory compression strategies:\n",
    "- **Truncation** - Keep only recent N messages (fast, simple)\n",
    "- **Priority-Based** - Score messages by importance (balanced)\n",
    "- **Summarization** - LLM creates intelligent summaries (high quality)\n",
    "\n",
    "**In this section**, we'll demonstrate these strategies in our production agent to show how they handle long conversations.\n"
   ],
   "id": "346d2737598bfd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Problem: Unbounded Conversation Growth\n",
    "\n",
    "Every conversation turn adds messages to working memory:\n",
    "\n",
    "```\n",
    "Turn 1:  System (500) + Messages (200) = 700 tokens âœ…\n",
    "Turn 10: System (500) + Messages (2,000) = 2,500 tokens âœ…\n",
    "Turn 30: System (500) + Messages (6,000) = 6,500 tokens âš ï¸\n",
    "Turn 50: System (500) + Messages (10,000) = 10,500 tokens âš ï¸\n",
    "Turn 100: System (500) + Messages (20,000) = 20,500 tokens âŒ\n",
    "```\n",
    "\n",
    "**Without compression:**\n",
    "- ðŸ’° Costs grow quadratically (each turn includes all previous messages)\n",
    "- â±ï¸ Latency increases with context size\n",
    "- ðŸš« Eventually hit token limits (128K for GPT-4o)\n",
    "- ðŸ“‰ Context rot: LLMs struggle with very long contexts\n",
    "\n",
    "**Solution:** Compress working memory while preserving important information.\n"
   ],
   "id": "6a1c7e21740d4240"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Implementation: Three Compression Strategies\n",
    "\n",
    "Let's implement the strategies from Section 3, Notebook 3.\n"
   ],
   "id": "439770b03604fe49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tiktoken\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Token counting utility\n",
    "def count_tokens(text: str, model: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        return len(encoding.encode(text))\n",
    "    except Exception:\n",
    "        # Fallback: rough estimate\n",
    "        return len(text) // 4\n",
    "\n",
    "@dataclass\n",
    "class ConversationMessage:\n",
    "    \"\"\"Represents a conversation message with metadata.\"\"\"\n",
    "    role: str\n",
    "    content: str\n",
    "    token_count: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.token_count == 0:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "print(\"âœ… Token counting utilities defined\")\n"
   ],
   "id": "821ce9b3f3abe835"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Strategy 1: Truncation (Fast, Simple)\n",
    "\n",
    "Keep only the most recent N messages within token budget.\n",
    "\n",
    "**Pros:** Fast, no LLM calls, predictable\n",
    "**Cons:** Loses all old context, no intelligence\n"
   ],
   "id": "f1d1881df6ca55de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TruncationStrategy:\n",
    "    \"\"\"Keep only the most recent messages within token budget.\"\"\"\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Keep most recent messages within token budget.\"\"\"\n",
    "        compressed = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Work backwards from most recent\n",
    "        for msg in reversed(messages):\n",
    "            if total_tokens + msg.token_count <= max_tokens:\n",
    "                compressed.insert(0, msg)\n",
    "                total_tokens += msg.token_count\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return compressed\n",
    "\n",
    "print(\"âœ… Truncation strategy implemented\")\n"
   ],
   "id": "1df1a0aa4aabfb41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Strategy 2: Priority-Based (Balanced)\n",
    "\n",
    "Score messages by importance and keep highest-scoring ones.\n",
    "\n",
    "**Pros:** Preserves important context, no LLM calls\n",
    "**Cons:** Requires good scoring logic, may lose temporal flow\n"
   ],
   "id": "3dcc2d1ef45c9d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PriorityBasedStrategy:\n",
    "    \"\"\"Score messages by importance and keep highest-scoring.\"\"\"\n",
    "\n",
    "    def _score_message(self, msg: ConversationMessage, index: int, total: int) -> float:\n",
    "        \"\"\"\n",
    "        Score message importance.\n",
    "\n",
    "        Higher scores for:\n",
    "        - Recent messages (recency bias)\n",
    "        - Longer messages (more information)\n",
    "        - User messages (user intent)\n",
    "        - Messages with keywords (course names, preferences)\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Recency: Recent messages get higher scores\n",
    "        recency_score = index / total\n",
    "        score += recency_score * 50\n",
    "\n",
    "        # Length: Longer messages likely have more info\n",
    "        length_score = min(msg.token_count / 100, 1.0)\n",
    "        score += length_score * 20\n",
    "\n",
    "        # Role: User messages are important (capture intent)\n",
    "        if msg.role == \"user\":\n",
    "            score += 15\n",
    "\n",
    "        # Keywords: Messages with important terms\n",
    "        keywords = [\"course\", \"RU\", \"prefer\", \"interested\", \"goal\", \"major\", \"graduate\"]\n",
    "        keyword_count = sum(1 for kw in keywords if kw.lower() in msg.content.lower())\n",
    "        score += keyword_count * 5\n",
    "\n",
    "        return score\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Keep highest-scoring messages within token budget.\"\"\"\n",
    "        # Score all messages\n",
    "        scored = [\n",
    "            (self._score_message(msg, i, len(messages)), i, msg)\n",
    "            for i, msg in enumerate(messages)\n",
    "        ]\n",
    "\n",
    "        # Sort by score (descending)\n",
    "        scored.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        # Select messages within budget\n",
    "        selected = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        for score, idx, msg in scored:\n",
    "            if total_tokens + msg.token_count <= max_tokens:\n",
    "                selected.append((idx, msg))\n",
    "                total_tokens += msg.token_count\n",
    "\n",
    "        # Sort by original order to maintain conversation flow\n",
    "        selected.sort(key=lambda x: x[0])\n",
    "\n",
    "        return [msg for idx, msg in selected]\n",
    "\n",
    "print(\"âœ… Priority-based strategy implemented\")\n",
    "\n"
   ],
   "id": "edc2ffeac82e03ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Strategy 3: Summarization (High Quality)\n",
    "\n",
    "Use LLM to create intelligent summaries of old messages, keep recent ones.\n",
    "\n",
    "**Pros:** Preserves meaning, high quality, intelligent compression\n",
    "**Cons:** Slower, costs tokens, requires LLM call\n"
   ],
   "id": "7a8408f151375688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SummarizationStrategy:\n",
    "    \"\"\"Use LLM to create intelligent summaries.\"\"\"\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI, keep_recent: int = 4):\n",
    "        self.llm = llm\n",
    "        self.keep_recent = keep_recent\n",
    "\n",
    "        self.summarization_prompt = \"\"\"You are summarizing a conversation between a student and a course advisor.\n",
    "\n",
    "Create a concise summary that preserves:\n",
    "1. Key decisions made\n",
    "2. Important requirements or prerequisites discussed\n",
    "3. Student's goals, preferences, and constraints\n",
    "4. Specific courses mentioned and recommendations given\n",
    "5. Any problems or issues that need follow-up\n",
    "\n",
    "Format as bullet points. Be specific and actionable.\n",
    "\n",
    "Conversation:\n",
    "{conversation}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "    async def compress_async(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress using summarization (async).\"\"\"\n",
    "        if len(messages) <= self.keep_recent:\n",
    "            return messages\n",
    "\n",
    "        # Split into old (to summarize) and recent (to keep)\n",
    "        old_messages = messages[:-self.keep_recent]\n",
    "        recent_messages = messages[-self.keep_recent:]\n",
    "\n",
    "        # Format old messages for summarization\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{msg.role.title()}: {msg.content}\"\n",
    "            for msg in old_messages\n",
    "        ])\n",
    "\n",
    "        # Generate summary using LLM\n",
    "        prompt = self.summarization_prompt.format(conversation=conversation_text)\n",
    "        response = await self.llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        summary_content = f\"[CONVERSATION SUMMARY]\\n{response.content}\"\n",
    "\n",
    "        # Create summary message\n",
    "        summary_msg = ConversationMessage(\n",
    "            role=\"system\",\n",
    "            content=summary_content\n",
    "        )\n",
    "\n",
    "        # Return summary + recent messages\n",
    "        return [summary_msg] + recent_messages\n",
    "\n",
    "print(\"âœ… Summarization strategy implemented\")\n",
    "\n"
   ],
   "id": "33dd8c677f8c24ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Demo: Simulating a Long Conversation\n",
    "\n",
    "Let's create a realistic 30-turn conversation to demonstrate compression needs.\n"
   ],
   "id": "225f1520b9ed27e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Simulate a long advising conversation (30 turns = 60 messages)\n",
    "long_conversation_turns = [\n",
    "    (\"I'm interested in machine learning courses\", \"Great! Let me help you find ML courses.\"),\n",
    "    (\"What are the prerequisites?\", \"You'll need data structures and linear algebra.\"),\n",
    "    (\"I've completed CS201 Data Structures\", \"Perfect! That's one prerequisite done.\"),\n",
    "    (\"Do I need calculus?\", \"Yes, MATH301 Linear Algebra is required.\"),\n",
    "    (\"I'm taking that next semester\", \"Excellent planning!\"),\n",
    "    (\"What ML courses do you recommend?\", \"RU330 and RU401 are great for ML.\"),\n",
    "    (\"Tell me about RU330\", \"RU330 covers trading engines with ML applications.\"),\n",
    "    (\"Is it available online?\", \"Yes, RU330 is available in online format.\"),\n",
    "    (\"What about RU401?\", \"RU401 focuses on running Redis at scale with vector search.\"),\n",
    "    (\"That sounds perfect for AI\", \"Absolutely! Vector search is key for AI applications.\"),\n",
    "    (\"I prefer online courses\", \"I'll note that preference for future recommendations.\"),\n",
    "    (\"I work part-time\", \"Online courses are great for working students.\"),\n",
    "    (\"When should I take RU330?\", \"After completing your prerequisites.\"),\n",
    "    (\"Can I take both together?\", \"Yes, if you have time. Both are 3-credit courses.\"),\n",
    "    (\"What's the workload like?\", \"Expect 6-8 hours per week for each course.\"),\n",
    "    (\"I'm also interested in databases\", \"RU301 covers querying and indexing.\"),\n",
    "    (\"Is that a prerequisite for RU401?\", \"No, but it's helpful background knowledge.\"),\n",
    "    (\"What order should I take them?\", \"RU301 first, then RU330, then RU401.\"),\n",
    "    (\"That's a good progression\", \"Yes, it builds your skills systematically.\"),\n",
    "    (\"I want to graduate in Spring 2026\", \"Let's plan your course schedule.\"),\n",
    "    (\"I can take 2 courses per semester\", \"That's manageable with work.\"),\n",
    "    (\"Fall 2025: RU301 and what else?\", \"Maybe RU330 if prerequisites are done.\"),\n",
    "    (\"Spring 2026: RU401?\", \"Yes, that completes your ML track.\"),\n",
    "    (\"Are there any capstone projects?\", \"RU401 includes a vector search project.\"),\n",
    "    (\"That sounds challenging\", \"It's practical and portfolio-worthy.\"),\n",
    "    (\"I'm interested in tech startups\", \"These courses are perfect for startup roles.\"),\n",
    "    (\"Do you have career resources?\", \"We have career services and job boards.\"),\n",
    "    (\"Can I get internship help?\", \"Yes, our career center helps with internships.\"),\n",
    "    (\"This has been very helpful\", \"I'm glad I could help plan your path!\"),\n",
    "    (\"I'll start with RU301 next semester\", \"Excellent choice! Good luck!\"),\n",
    "]\n",
    "\n",
    "# Convert to ConversationMessage objects\n",
    "long_conversation = []\n",
    "for user_msg, assistant_msg in long_conversation_turns:\n",
    "    long_conversation.append(ConversationMessage(role=\"user\", content=user_msg))\n",
    "    long_conversation.append(ConversationMessage(role=\"assistant\", content=assistant_msg))\n",
    "\n",
    "# Calculate statistics\n",
    "total_messages = len(long_conversation)\n",
    "total_tokens = sum(msg.token_count for msg in long_conversation)\n",
    "avg_tokens_per_msg = total_tokens / total_messages\n",
    "\n",
    "print(\"ðŸ“Š Long Conversation Statistics\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total turns: {len(long_conversation_turns)}\")\n",
    "print(f\"Total messages: {total_messages}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Average tokens per message: {avg_tokens_per_msg:.1f}\")\n",
    "print(f\"\\nâš ï¸  This conversation is getting expensive!\")\n",
    "print(f\"   Cost per query (at $0.0025/1K tokens): ${(total_tokens / 1000) * 0.0025:.4f}\")\n",
    "print(f\"   Over 1,000 conversations: ${((total_tokens / 1000) * 0.0025) * 1000:.2f}\")\n",
    "\n",
    "\n"
   ],
   "id": "cccf2fb420c9025a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparison: Testing All Three Strategies\n",
    "\n",
    "Let's compress this conversation using all three strategies and compare results.\n"
   ],
   "id": "dcfc2ebd5306f8cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set compression budget\n",
    "max_tokens = 1000  # Target: compress from ~1,500 tokens to ~1,000 tokens\n",
    "\n",
    "print(\"ðŸ”¬ Compression Strategy Comparison\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Original: {total_messages} messages, {total_tokens:,} tokens\")\n",
    "print(f\"Target: {max_tokens:,} tokens (compression needed!)\\n\")\n",
    "\n",
    "# Strategy 1: Truncation\n",
    "truncation = TruncationStrategy()\n",
    "truncated = truncation.compress(long_conversation, max_tokens)\n",
    "truncated_tokens = sum(msg.token_count for msg in truncated)\n",
    "\n",
    "print(\"1ï¸âƒ£  TRUNCATION STRATEGY\")\n",
    "print(f\"   Result: {len(truncated)} messages, {truncated_tokens:,} tokens\")\n",
    "print(f\"   Savings: {total_tokens - truncated_tokens:,} tokens ({((total_tokens - truncated_tokens) / total_tokens * 100):.1f}%)\")\n",
    "print(f\"   Kept: Most recent {len(truncated)} messages\")\n",
    "print(f\"   Lost: First {total_messages - len(truncated)} messages (all early context)\")\n",
    "\n",
    "# Strategy 2: Priority-Based\n",
    "priority = PriorityBasedStrategy()\n",
    "prioritized = priority.compress(long_conversation, max_tokens)\n",
    "prioritized_tokens = sum(msg.token_count for msg in prioritized)\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£  PRIORITY-BASED STRATEGY\")\n",
    "print(f\"   Result: {len(prioritized)} messages, {prioritized_tokens:,} tokens\")\n",
    "print(f\"   Savings: {total_tokens - prioritized_tokens:,} tokens ({((total_tokens - prioritized_tokens) / total_tokens * 100):.1f}%)\")\n",
    "print(f\"   Kept: {len(prioritized)} highest-scoring messages\")\n",
    "print(f\"   Preserved: Important context from throughout conversation\")\n",
    "\n",
    "# Show which messages were kept (by index)\n",
    "kept_indices = []\n",
    "for msg in prioritized:\n",
    "    for i, orig_msg in enumerate(long_conversation):\n",
    "        if msg.content == orig_msg.content and msg.role == orig_msg.role:\n",
    "            kept_indices.append(i)\n",
    "            break\n",
    "print(f\"   Message indices kept: {sorted(set(kept_indices))[:10]}... (showing first 10)\")\n",
    "\n",
    "# Strategy 3: Summarization\n",
    "summarization = SummarizationStrategy(llm=llm, keep_recent=4)\n",
    "summarized = await summarization.compress_async(long_conversation, max_tokens)\n",
    "summarized_tokens = sum(msg.token_count for msg in summarized)\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£  SUMMARIZATION STRATEGY\")\n",
    "print(f\"   Result: {len(summarized)} messages, {summarized_tokens:,} tokens\")\n",
    "print(f\"   Savings: {total_tokens - summarized_tokens:,} tokens ({((total_tokens - summarized_tokens) / total_tokens * 100):.1f}%)\")\n",
    "print(f\"   Structure: 1 summary + {len(summarized) - 1} recent messages\")\n",
    "print(f\"   Preserved: Meaning of all {total_messages - 4} old messages in summary\")\n",
    "\n",
    "# Show summary preview\n",
    "summary_msg = summarized[0]\n",
    "print(f\"\\n   Summary preview:\")\n",
    "summary_lines = summary_msg.content.split('\\n')[:5]\n",
    "for line in summary_lines:\n",
    "    print(f\"   {line}\")\n",
    "if len(summary_msg.content.split('\\n')) > 5:\n",
    "    print(f\"   ... ({len(summary_msg.content.split('\\n')) - 5} more lines)\")\n",
    "\n"
   ],
   "id": "58fab84b7f0fb661",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comparison Table\n",
   "id": "b5874671e946a4d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š COMPRESSION STRATEGY COMPARISON TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Strategy':<20} {'Messages':<12} {'Tokens':<12} {'Savings':<15} {'Quality':<10} {'Speed'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "strategies_data = [\n",
    "    (\"Original\", total_messages, total_tokens, \"0 (0%)\", \"N/A\", \"N/A\"),\n",
    "    (\"Truncation\", len(truncated), truncated_tokens,\n",
    "     f\"{total_tokens - truncated_tokens} ({((total_tokens - truncated_tokens) / total_tokens * 100):.0f}%)\",\n",
    "     \"Low\", \"Fast\"),\n",
    "    (\"Priority-Based\", len(prioritized), prioritized_tokens,\n",
    "     f\"{total_tokens - prioritized_tokens} ({((total_tokens - prioritized_tokens) / total_tokens * 100):.0f}%)\",\n",
    "     \"Medium\", \"Fast\"),\n",
    "    (\"Summarization\", len(summarized), summarized_tokens,\n",
    "     f\"{total_tokens - summarized_tokens} ({((total_tokens - summarized_tokens) / total_tokens * 100):.0f}%)\",\n",
    "     \"High\", \"Slow\"),\n",
    "]\n",
    "\n",
    "for name, msgs, tokens, savings, quality, speed in strategies_data:\n",
    "    print(f\"{name:<20} {msgs:<12} {tokens:<12} {savings:<15} {quality:<10} {speed}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   â€¢ Truncation: Fastest but loses all early context\")\n",
    "print(\"   â€¢ Priority-Based: Good balance, preserves important messages\")\n",
    "print(\"   â€¢ Summarization: Best quality, preserves meaning of entire conversation\")\n",
    "print(\"   â€¢ Choose based on your quality/speed/cost requirements\")\n"
   ],
   "id": "c55826be685cfa3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Agent Memory Server's Automatic Compression\n",
    "\n",
    "The Agent Memory Server provides automatic compression through the `WINDOW_SIZE` configuration.\n",
    "\n",
    "**How it works:**\n",
    "1. You set `WINDOW_SIZE` in environment variables (e.g., `WINDOW_SIZE=20`)\n",
    "2. When working memory exceeds this threshold, automatic compression triggers\n",
    "3. Server uses summarization strategy (similar to our Strategy 3)\n",
    "4. Old messages are summarized, recent messages are kept\n",
    "5. Your application retrieves compressed memory transparently\n",
    "\n",
    "**Configuration Example:**\n",
    "\n",
    "```bash\n",
    "# In .env file\n",
    "WINDOW_SIZE=20                    # Trigger compression after 20 messages\n",
    "LONG_TERM_MEMORY=true            # Enable long-term memory\n",
    "REDIS_URL=redis://localhost:6379\n",
    "```\n",
    "\n",
    "**In production:**\n",
    "- âœ… Automatic compression (no manual intervention)\n",
    "- âœ… Configurable thresholds\n",
    "- âœ… Background processing (async workers)\n",
    "- âœ… Transparent to your application\n"
   ],
   "id": "3df8a7dfed12ad73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### When to Use Each Strategy\n",
    "\n",
    "**Use Truncation when:**\n",
    "- âœ… Speed is critical (real-time chat)\n",
    "- âœ… Recent context is all that matters\n",
    "- âœ… Cost-sensitive (no LLM calls)\n",
    "- âœ… Simple implementation needed\n",
    "\n",
    "**Use Priority-Based when:**\n",
    "- âœ… Need balance between speed and quality\n",
    "- âœ… Important context scattered throughout conversation\n",
    "- âœ… No LLM calls allowed (cost/latency constraints)\n",
    "- âœ… Custom scoring logic available\n",
    "\n",
    "**Use Summarization when:**\n",
    "- âœ… Quality is critical (preserve all important info)\n",
    "- âœ… Long conversations (30+ turns)\n",
    "- âœ… Can afford LLM call latency\n",
    "- âœ… Comprehensive context needed\n",
    "\n",
    "**Use Agent Memory Server when:**\n",
    "- âœ… Production deployment\n",
    "- âœ… Want automatic management\n",
    "- âœ… Need scalability\n",
    "- âœ… Prefer transparent operation\n"
   ],
   "id": "b25ca6d346ac38f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Production Recommendations\n",
    "\n",
    "**For most applications:**\n",
    "```python\n",
    "# Use Agent Memory Server with automatic compression\n",
    "# Configuration in .env:\n",
    "# WINDOW_SIZE=20\n",
    "# LONG_TERM_MEMORY=true\n",
    "```\n",
    "\n",
    "**For high-volume, cost-sensitive:**\n",
    "```python\n",
    "# Use priority-based compression manually\n",
    "priority = PriorityBasedStrategy()\n",
    "compressed = priority.compress(messages, max_tokens=2000)\n",
    "```\n",
    "\n",
    "**For critical conversations:**\n",
    "```python\n",
    "# Use summarization with human review\n",
    "summarization = SummarizationStrategy(llm=llm, keep_recent=6)\n",
    "compressed = await summarization.compress_async(messages, max_tokens=3000)\n",
    "# Store full conversation separately for audit\n",
    "```\n",
    "\n",
    "**For real-time chat:**\n",
    "```python\n",
    "# Use truncation for speed\n",
    "truncation = TruncationStrategy()\n",
    "compressed = truncation.compress(messages, max_tokens=1500)\n",
    "```\n"
   ],
   "id": "f85886cdfd7b8c63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ”— Connection Back to Section 3\n",
    "\n",
    "**Section 3, Notebook 3** taught the theory:\n",
    "- Why compression is needed (token limits, cost, performance)\n",
    "- Three compression strategies (truncation, priority, summarization)\n",
    "- Decision framework for choosing strategies\n",
    "- Agent Memory Server configuration\n",
    "\n",
    "**This section** demonstrated the practice:\n",
    "- âœ… Implemented all three strategies in working code\n",
    "- âœ… Tested with realistic 30-turn conversation\n",
    "- âœ… Compared results with metrics\n",
    "- âœ… Showed when to use each strategy\n",
    "- âœ… Connected to Agent Memory Server's automatic features\n",
    "\n",
    "**Key Takeaway:** You now understand both the theory (Section 3) and practice (Section 4) of working memory compression for production agents!\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "953e03c75beccdb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "### **1. Agents = RAG + Tools + Decision-Making**\n",
    "- RAG retrieves information\n",
    "- Tools enable actions\n",
    "- Agents decide when to use each\n",
    "\n",
    "### **2. Memory is Critical for Personalization**\n",
    "- Working memory enables conversation continuity\n",
    "- Long-term memory enables personalization\n",
    "- Agents can decide when to store/recall memories\n",
    "\n",
    "### **3. LangGraph Simplifies Complex Workflows**\n",
    "- State management is automatic\n",
    "- Conditional routing is declarative\n",
    "- Visualization helps debugging\n",
    "\n",
    "### **4. Tool Design Matters**\n",
    "- Clear descriptions guide LLM selection\n",
    "- Well-defined schemas prevent errors\n",
    "- Focused tools are better than Swiss Army knives\n",
    "\n",
    "### **5. Trade-offs to Consider**\n",
    "- **Complexity**: Agents are more complex than RAG\n",
    "- **Latency**: Multiple tool calls add latency\n",
    "- **Cost**: More LLM calls = higher cost\n",
    "- **Value**: Worth it for complex, multi-step tasks\n",
    "\n",
    "### **6. When to Use Agents vs RAG**\n",
    "\n",
    "**Use RAG when:**\n",
    "- Simple question answering\n",
    "- Single-step retrieval\n",
    "- Low latency required\n",
    "- Predictable workflows\n",
    "\n",
    "**Use Agents when:**\n",
    "- Multi-step reasoning needed\n",
    "- Actions beyond retrieval\n",
    "- Personalization required\n",
    "- Complex decision-making\n"
   ],
   "id": "6064fff959e6e811"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸš€ Next Steps and Extensions\n",
    "\n",
    "### **Ideas to Extend This Agent:**\n",
    "\n",
    "1. **Add More Tools**\n",
    "   - `check_prerequisites` - Verify if student meets course requirements\n",
    "   - `get_course_details` - Get detailed info about a specific course\n",
    "   - `create_schedule` - Build a semester schedule\n",
    "   - `check_conflicts` - Detect time conflicts\n",
    "\n",
    "2. **Enhance Memory**\n",
    "   - Automatic memory extraction from conversations\n",
    "   - Memory summarization for long conversations\n",
    "   - Memory importance scoring\n",
    "   - Memory expiration policies\n",
    "\n",
    "3. **Improve Personalization**\n",
    "   - Learning style detection\n",
    "   - Career path recommendations\n",
    "   - Skill gap analysis\n",
    "   - Progress tracking\n",
    "\n",
    "4. **Add Guardrails**\n",
    "   - Input validation\n",
    "   - Output filtering\n",
    "   - Rate limiting\n",
    "   - Error handling\n",
    "\n",
    "5. **Production Considerations**\n",
    "   - Authentication and authorization\n",
    "   - Logging and monitoring\n",
    "   - Caching for performance\n",
    "   - Fallback strategies\n",
    "\n",
    "### **Reference Implementation:**\n",
    "\n",
    "Check out `reference-agent/` for a full production implementation with:\n",
    "- 7 tools (vs our 3)\n",
    "- Advanced memory management\n",
    "- Semantic tool selection\n",
    "- Comprehensive error handling\n",
    "- CLI interface\n",
    "- Full test suite\n"
   ],
   "id": "ca5250d8cbfa9772"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the Context Engineering course! You've learned:\n",
    "\n",
    "**Section 1:** Context Types\n",
    "- System, User, Conversation, Retrieved context\n",
    "- How context shapes LLM behavior\n",
    "\n",
    "**Section 2:** RAG Foundations\n",
    "- Semantic search with vector embeddings\n",
    "- Context assembly and generation\n",
    "- Building a course search system\n",
    "\n",
    "**Section 3:** Memory Architecture\n",
    "- Working memory for conversation continuity\n",
    "- Long-term memory for persistent knowledge\n",
    "- Memory-enhanced RAG systems\n",
    "\n",
    "**ðŸ”¬ Research Foundation:** Throughout this course, you've learned techniques validated by Context Rot research - prioritizing relevance over quantity, filtering distractors, and structuring context for optimal LLM performance. ([Context Rot paper](https://research.trychroma.com/context-rot))\n",
    "\n",
    "**Section 4:** Agents and Tools\n",
    "- Tool calling fundamentals\n",
    "- LangGraph workflow orchestration\n",
    "- Building a complete course advisor agent\n",
    "- Agents vs RAG trade-offs\n",
    "\n",
    "### **You Can Now:**\n",
    "- âœ… Design effective context strategies\n",
    "- âœ… Build RAG systems with Redis\n",
    "- âœ… Implement dual-memory architectures\n",
    "- âœ… Create agents with tools and decision-making\n",
    "- âœ… Choose the right approach for your use case\n",
    "\n",
    "### **Keep Learning:**\n",
    "- Explore the reference-agent implementation\n",
    "- Experiment with different tools\n",
    "- Try different LLMs and embeddings\n",
    "- Build your own agents!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "\n",
    "- [Agent Memory Server Documentation](https://github.com/redis/agent-memory-server) - Production-ready memory management\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client for Agent Memory Server\n",
    "- [RedisVL Documentation](https://redisvl.com/) - Redis Vector Library\n",
    "- [Retrieval-Augmented Generation Paper](https://arxiv.org/abs/2005.11401) - Original RAG research\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/) - Building RAG systems\n",
    "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/) - Building agents with LangGraph\n",
    "- [Agent Architectures](https://python.langchain.com/docs/modules/agents/) - Different agent patterns\n",
    "- [ReAct: Synergizing Reasoning and Acting](https://arxiv.org/abs/2210.03629) - Reasoning + acting in LLMs\n",
    "- [Anthropic's Guide to Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - Agent design patterns\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this course! ðŸ™**\n"
   ],
   "id": "88773a005e5cba59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "70ab2e1e572d5aa6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval Strategies: RAG, Summaries, and Hybrid Approaches\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you'll learn different strategies for retrieving and providing context to your agent. Not all context should be included all the time - you need smart retrieval strategies to provide relevant information efficiently.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "- Different retrieval strategies (full context, RAG, summaries, hybrid)\n",
        "- When to use each strategy\n",
        "- How to optimize vector search parameters\n",
        "- How to measure retrieval quality and performance\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Completed Section 3 notebooks\n",
        "- Redis 8 running locally\n",
        "- Agent Memory Server running\n",
        "- OpenAI API key set\n",
        "- Course data ingested"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concepts: Retrieval Strategies\n",
        "\n",
        "### The Context Retrieval Problem\n",
        "\n",
        "You have a large knowledge base (courses, memories, documents), but you can't include everything in every request. You need to:\n",
        "\n",
        "1. **Find relevant information** - What's related to the user's query?\n",
        "2. **Limit context size** - Stay within token budgets\n",
        "3. **Maintain quality** - Don't miss important information\n",
        "4. **Optimize performance** - Fast retrieval, low latency\n",
        "\n",
        "### Strategy 1: Full Context (Naive)\n",
        "\n",
        "**Approach:** Include everything in every request\n",
        "\n",
        "```python\n",
        "# Include entire course catalog\n",
        "all_courses = get_all_courses()  # 500 courses\n",
        "context = \"\\n\".join([str(course) for course in all_courses])\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- ✅ Never miss relevant information\n",
        "- ✅ Simple to implement\n",
        "\n",
        "**Cons:**\n",
        "- ❌ Exceeds token limits quickly\n",
        "- ❌ Expensive (more tokens = higher cost)\n",
        "- ❌ Slow (more tokens = higher latency)\n",
        "- ❌ Dilutes relevant information with noise\n",
        "\n",
        "**Verdict:** ❌ Don't use for production\n",
        "\n",
        "### Strategy 2: RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "**Approach:** Retrieve only relevant information using semantic search\n",
        "\n",
        "```python\n",
        "# Search for relevant courses\n",
        "query = \"machine learning courses\"\n",
        "relevant_courses = search_courses(query, limit=5)\n",
        "context = \"\\n\".join([str(course) for course in relevant_courses])\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- ✅ Only includes relevant information\n",
        "- ✅ Stays within token budgets\n",
        "- ✅ Fast and cost-effective\n",
        "- ✅ Semantic search finds related content\n",
        "\n",
        "**Cons:**\n",
        "- ⚠️ May miss relevant information if search isn't perfect\n",
        "- ⚠️ Requires good embeddings and search tuning\n",
        "\n",
        "**Verdict:** ✅ Good for most use cases\n",
        "\n",
        "### Strategy 3: Summaries\n",
        "\n",
        "**Approach:** Pre-compute summaries of large datasets\n",
        "\n",
        "```python\n",
        "# Use pre-computed course catalog summary\n",
        "summary = get_course_catalog_summary()  # \"CS: 50 courses, MATH: 30 courses...\"\n",
        "context = summary\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- ✅ Very compact (low token usage)\n",
        "- ✅ Fast (no search needed)\n",
        "- ✅ Provides high-level overview\n",
        "\n",
        "**Cons:**\n",
        "- ❌ Loses details\n",
        "- ❌ May not have specific information needed\n",
        "- ⚠️ Requires pre-computation\n",
        "\n",
        "**Verdict:** ✅ Good for overviews, combine with RAG for details\n",
        "\n",
        "### Strategy 4: Hybrid (Best)\n",
        "\n",
        "**Approach:** Combine summaries + targeted retrieval\n",
        "\n",
        "```python\n",
        "# Start with summary for overview\n",
        "summary = get_course_catalog_summary()\n",
        "\n",
        "# Add specific relevant courses\n",
        "relevant_courses = search_courses(query, limit=3)\n",
        "\n",
        "context = f\"{summary}\\n\\nRelevant courses:\\n{courses}\"\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- ✅ Best of both worlds\n",
        "- ✅ Overview + specific details\n",
        "- ✅ Efficient token usage\n",
        "- ✅ High quality results\n",
        "\n",
        "**Cons:**\n",
        "- ⚠️ More complex to implement\n",
        "- ⚠️ Requires pre-computed summaries\n",
        "\n",
        "**Verdict:** ✅ Best for production systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import asyncio\n",
        "from typing import List\n",
        "import tiktoken\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from redis_context_course import CourseManager, MemoryClient\n",
        "\n",
        "# Initialize\n",
        "course_manager = CourseManager()\n",
        "# Initialize memory client with proper config\n",
        "import os\n",
        "config = MemoryClientConfig(\n",
        "    base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8000\"),\n",
        "    default_namespace=\"redis_university\"\n",
        ")\n",
        "memory_client = MemoryClient(config=config)\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "print(\"✅ Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hands-on: Comparing Retrieval Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy 1: Full Context (Bad)\n",
        "\n",
        "Let's try including all courses and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"STRATEGY 1: FULL CONTEXT (Naive)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get all courses\n",
        "all_courses = await course_manager.get_all_courses()\n",
        "print(f\"\\nTotal courses in catalog: {len(all_courses)}\")\n",
        "\n",
        "# Build full context\n",
        "full_context = \"\\n\\n\".join([\n",
        "    f\"{c.course_code}: {c.title}\\n{c.description}\\nCredits: {c.credits} | {c.format.value}\"\n",
        "    for c in all_courses[:50]  # Limit to 50 for demo\n",
        "])\n",
        "\n",
        "tokens = count_tokens(full_context)\n",
        "print(f\"\\nTokens for 50 courses: {tokens:,}\")\n",
        "print(f\"Estimated tokens for all {len(all_courses)} courses: {(tokens * len(all_courses) / 50):,.0f}\")\n",
        "\n",
        "# Try to use it\n",
        "user_query = \"I'm interested in machine learning courses\"\n",
        "system_prompt = f\"\"\"You are a class scheduling agent.\n",
        "\n",
        "Available courses:\n",
        "{full_context[:2000]}...\n",
        "\"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_query)\n",
        "]\n",
        "response = llm.invoke(messages)\n",
        "latency = time.time() - start_time\n",
        "\n",
        "print(f\"\\nQuery: {user_query}\")\n",
        "print(f\"Response: {response.content[:200]}...\")\n",
        "print(f\"\\nLatency: {latency:.2f}s\")\n",
        "print(f\"Total tokens used: ~{count_tokens(system_prompt) + count_tokens(user_query):,}\")\n",
        "\n",
        "print(\"\\n❌ PROBLEMS:\")\n",
        "print(\"  - Too many tokens (expensive)\")\n",
        "print(\"  - High latency\")\n",
        "print(\"  - Relevant info buried in noise\")\n",
        "print(\"  - Doesn't scale to full catalog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy 2: RAG with Semantic Search (Good)\n",
        "\n",
        "Now let's use semantic search to retrieve only relevant courses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STRATEGY 2: RAG (Semantic Search)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "user_query = \"I'm interested in machine learning courses\"\n",
        "\n",
        "# Search for relevant courses\n",
        "start_time = time.time()\n",
        "relevant_courses = await course_manager.search_courses(\n",
        "    query=user_query,\n",
        "    limit=5\n",
        ")\n",
        "search_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nSearch time: {search_time:.3f}s\")\n",
        "print(f\"Courses found: {len(relevant_courses)}\")\n",
        "\n",
        "# Build context from relevant courses only\n",
        "rag_context = \"\\n\\n\".join([\n",
        "    f\"{c.course_code}: {c.title}\\n{c.description}\\nCredits: {c.credits} | {c.format.value}\"\n",
        "    for c in relevant_courses\n",
        "])\n",
        "\n",
        "tokens = count_tokens(rag_context)\n",
        "print(f\"Context tokens: {tokens:,}\")\n",
        "\n",
        "# Use it\n",
        "system_prompt = f\"\"\"You are a class scheduling agent.\n",
        "\n",
        "Relevant courses:\n",
        "{rag_context}\n",
        "\"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_query)\n",
        "]\n",
        "response = llm.invoke(messages)\n",
        "latency = time.time() - start_time\n",
        "\n",
        "print(f\"\\nQuery: {user_query}\")\n",
        "print(f\"Response: {response.content[:200]}...\")\n",
        "print(f\"\\nTotal latency: {latency:.2f}s\")\n",
        "print(f\"Total tokens used: ~{count_tokens(system_prompt) + count_tokens(user_query):,}\")\n",
        "\n",
        "print(\"\\n✅ BENEFITS:\")\n",
        "print(\"  - Much fewer tokens (cheaper)\")\n",
        "print(\"  - Lower latency\")\n",
        "print(\"  - Only relevant information\")\n",
        "print(\"  - Scales to any catalog size\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy 3: Pre-computed Summary\n",
        "\n",
        "Let's create a summary of the course catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STRATEGY 3: PRE-COMPUTED SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a summary (in production, this would be pre-computed)\n",
        "all_courses = await course_manager.get_all_courses()\n",
        "\n",
        "# Group by department\n",
        "by_department = {}\n",
        "for course in all_courses:\n",
        "    dept = course.department\n",
        "    if dept not in by_department:\n",
        "        by_department[dept] = []\n",
        "    by_department[dept].append(course)\n",
        "\n",
        "# Create summary\n",
        "summary_lines = [\"Course Catalog Summary:\\n\"]\n",
        "for dept, courses in sorted(by_department.items()):\n",
        "    summary_lines.append(f\"{dept}: {len(courses)} courses\")\n",
        "    # Add a few example courses\n",
        "    examples = [f\"{c.course_code} ({c.title})\" for c in courses[:2]]\n",
        "    summary_lines.append(f\"  Examples: {', '.join(examples)}\")\n",
        "\n",
        "summary = \"\\n\".join(summary_lines)\n",
        "\n",
        "print(f\"\\nSummary:\\n{summary}\")\n",
        "print(f\"\\nSummary tokens: {count_tokens(summary):,}\")\n",
        "\n",
        "# Use it\n",
        "user_query = \"What departments offer courses?\"\n",
        "system_prompt = f\"\"\"You are a class scheduling agent.\n",
        "\n",
        "{summary}\n",
        "\"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_query)\n",
        "]\n",
        "response = llm.invoke(messages)\n",
        "latency = time.time() - start_time\n",
        "\n",
        "print(f\"\\nQuery: {user_query}\")\n",
        "print(f\"Response: {response.content}\")\n",
        "print(f\"\\nLatency: {latency:.2f}s\")\n",
        "\n",
        "print(\"\\n✅ BENEFITS:\")\n",
        "print(\"  - Very compact (minimal tokens)\")\n",
        "print(\"  - Fast (no search needed)\")\n",
        "print(\"  - Good for overview questions\")\n",
        "\n",
        "print(\"\\n⚠️  LIMITATIONS:\")\n",
        "print(\"  - Lacks specific details\")\n",
        "print(\"  - Can't answer detailed questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy 4: Hybrid (Best)\n",
        "\n",
        "Combine summary + targeted retrieval for the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STRATEGY 4: HYBRID (Summary + RAG)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "user_query = \"I'm interested in machine learning. What's available?\"\n",
        "\n",
        "# Start with summary\n",
        "summary_context = summary\n",
        "\n",
        "# Add targeted retrieval\n",
        "relevant_courses = await course_manager.search_courses(\n",
        "    query=user_query,\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "detailed_context = \"\\n\\n\".join([\n",
        "    f\"{c.course_code}: {c.title}\\n{c.description}\\nCredits: {c.credits} | {c.format.value}\"\n",
        "    for c in relevant_courses\n",
        "])\n",
        "\n",
        "# Combine\n",
        "hybrid_context = f\"\"\"{summary_context}\n",
        "\n",
        "Relevant courses for your query:\n",
        "{detailed_context}\n",
        "\"\"\"\n",
        "\n",
        "tokens = count_tokens(hybrid_context)\n",
        "print(f\"\\nHybrid context tokens: {tokens:,}\")\n",
        "\n",
        "# Use it\n",
        "system_prompt = f\"\"\"You are a class scheduling agent.\n",
        "\n",
        "{hybrid_context}\n",
        "\"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_query)\n",
        "]\n",
        "response = llm.invoke(messages)\n",
        "latency = time.time() - start_time\n",
        "\n",
        "print(f\"\\nQuery: {user_query}\")\n",
        "print(f\"Response: {response.content}\")\n",
        "print(f\"\\nLatency: {latency:.2f}s\")\n",
        "print(f\"Total tokens: ~{count_tokens(system_prompt) + count_tokens(user_query):,}\")\n",
        "\n",
        "print(\"\\n✅ BENEFITS:\")\n",
        "print(\"  - Overview + specific details\")\n",
        "print(\"  - Efficient token usage\")\n",
        "print(\"  - High quality responses\")\n",
        "print(\"  - Best of all strategies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing Vector Search Parameters\n",
        "\n",
        "Let's explore how to tune semantic search for better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"OPTIMIZING SEARCH PARAMETERS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "user_query = \"beginner programming courses\"\n",
        "\n",
        "# Test different limits\n",
        "print(f\"\\nQuery: '{user_query}'\\n\")\n",
        "\n",
        "for limit in [3, 5, 10]:\n",
        "    results = await course_manager.search_courses(\n",
        "        query=user_query,\n",
        "        limit=limit\n",
        "    )\n",
        "    \n",
        "    print(f\"Limit={limit}: Found {len(results)} courses\")\n",
        "    for i, course in enumerate(results, 1):\n",
        "        print(f\"  {i}. {course.course_code}: {course.title}\")\n",
        "    print()\n",
        "\n",
        "print(\"💡 TIP: Start with limit=5, adjust based on your needs\")\n",
        "print(\"  - Too few: May miss relevant results\")\n",
        "print(\"  - Too many: Wastes tokens, adds noise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison\n",
        "\n",
        "Let's compare all strategies side-by-side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STRATEGY COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n{'Strategy':<20} {'Tokens':<10} {'Latency':<10} {'Quality':<10} {'Scalability'}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Full Context':<20} {'50,000+':<10} {'High':<10} {'Good':<10} {'Poor'}\")\n",
        "print(f\"{'RAG (Semantic)':<20} {'500-2K':<10} {'Low':<10} {'Good':<10} {'Excellent'}\")\n",
        "print(f\"{'Summary Only':<20} {'100-500':<10} {'Very Low':<10} {'Limited':<10} {'Excellent'}\")\n",
        "print(f\"{'Hybrid':<20} {'1K-3K':<10} {'Low':<10} {'Excellent':<10} {'Excellent'}\")\n",
        "\n",
        "print(\"\\n✅ RECOMMENDATION: Use Hybrid strategy for production\")\n",
        "print(\"  - Provides overview + specific details\")\n",
        "print(\"  - Efficient token usage\")\n",
        "print(\"  - Scales to any dataset size\")\n",
        "print(\"  - High quality results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Choosing a Retrieval Strategy\n",
        "\n",
        "**Use RAG when:**\n",
        "- ✅ You need specific, detailed information\n",
        "- ✅ Dataset is large\n",
        "- ✅ Queries are specific\n",
        "\n",
        "**Use Summaries when:**\n",
        "- ✅ You need high-level overviews\n",
        "- ✅ Queries are general\n",
        "- ✅ Token budget is tight\n",
        "\n",
        "**Use Hybrid when:**\n",
        "- ✅ You want the best quality\n",
        "- ✅ You can pre-compute summaries\n",
        "- ✅ Building production systems\n",
        "\n",
        "### Optimization Tips\n",
        "\n",
        "1. **Start with RAG** - Simple and effective\n",
        "2. **Add summaries** - For overview context\n",
        "3. **Tune search limits** - Balance relevance vs. tokens\n",
        "4. **Pre-compute summaries** - Don't generate on every request\n",
        "5. **Monitor performance** - Track tokens, latency, quality\n",
        "\n",
        "### Vector Search Best Practices\n",
        "\n",
        "- ✅ Use semantic search for finding relevant content\n",
        "- ✅ Start with limit=5, adjust as needed\n",
        "- ✅ Use filters when you have structured criteria\n",
        "- ✅ Test with real user queries\n",
        "- ✅ Monitor search quality over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Implement hybrid retrieval**: Create a function that combines summary + RAG for any query.\n",
        "\n",
        "2. **Measure quality**: Test each strategy with 10 different queries. Which gives the best responses?\n",
        "\n",
        "3. **Optimize search**: Experiment with different search limits. What's the sweet spot for your use case?\n",
        "\n",
        "4. **Create summaries**: Build pre-computed summaries for different views (by department, by difficulty, by format)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "- ✅ Different retrieval strategies have different trade-offs\n",
        "- ✅ RAG (semantic search) is efficient and scalable\n",
        "- ✅ Summaries provide compact overviews\n",
        "- ✅ Hybrid approach combines the best of both\n",
        "- ✅ Proper retrieval is key to production-quality agents\n",
        "\n",
        "**Key insight:** Don't include everything - retrieve smartly. The hybrid strategy (summaries + targeted RAG) provides the best balance of quality, efficiency, and scalability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

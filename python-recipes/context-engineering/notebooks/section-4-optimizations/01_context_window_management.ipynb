{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context Window Management: Handling Token Limits\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you'll learn about context window limits and how to manage them effectively. Every LLM has a maximum number of tokens it can process, and long conversations can exceed this limit. The Agent Memory Server provides automatic summarization to handle this.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "- What context windows are and why they matter\n",
        "- How to count tokens in conversations\n",
        "- Why summarization is necessary\n",
        "- How to configure Agent Memory Server summarization\n",
        "- How summarization works in practice\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Completed Section 3 notebooks\n",
        "- Redis 8 running locally\n",
        "- Agent Memory Server running\n",
        "- OpenAI API key set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concepts: Context Windows and Token Limits\n",
        "\n",
        "### What is a Context Window?\n",
        "\n",
        "A **context window** is the maximum amount of text (measured in tokens) that an LLM can process in a single request. This includes:\n",
        "\n",
        "- System instructions\n",
        "- Conversation history\n",
        "- Retrieved context (memories, documents)\n",
        "- User's current message\n",
        "- Space for the response\n",
        "\n",
        "### Common Context Window Sizes\n",
        "\n",
        "| Model | Context Window | Notes |\n",
        "|-------|----------------|-------|\n",
        "| GPT-4o | 128K tokens | ~96,000 words |\n",
        "| GPT-4 Turbo | 128K tokens | ~96,000 words |\n",
        "| GPT-3.5 Turbo | 16K tokens | ~12,000 words |\n",
        "| Claude 3 Opus | 200K tokens | ~150,000 words |\n",
        "\n",
        "### The Problem: Long Conversations\n",
        "\n",
        "As conversations grow, they consume more tokens:\n",
        "\n",
        "```\n",
        "Turn 1:  System (500) + Messages (200) = 700 tokens ✅\n",
        "Turn 5:  System (500) + Messages (1,000) = 1,500 tokens ✅\n",
        "Turn 20: System (500) + Messages (4,000) = 4,500 tokens ✅\n",
        "Turn 50: System (500) + Messages (10,000) = 10,500 tokens ✅\n",
        "Turn 100: System (500) + Messages (20,000) = 20,500 tokens ⚠️\n",
        "Turn 200: System (500) + Messages (40,000) = 40,500 tokens ⚠️\n",
        "```\n",
        "\n",
        "Eventually, you'll hit the limit!\n",
        "\n",
        "### Why Summarization is Necessary\n",
        "\n",
        "Without summarization:\n",
        "- ❌ Conversations eventually fail\n",
        "- ❌ Costs increase linearly with conversation length\n",
        "- ❌ Latency increases with more tokens\n",
        "- ❌ Important early context gets lost\n",
        "\n",
        "With summarization:\n",
        "- ✅ Conversations can continue indefinitely\n",
        "- ✅ Costs stay manageable\n",
        "- ✅ Latency stays consistent\n",
        "- ✅ Important context is preserved in summaries\n",
        "\n",
        "### How Agent Memory Server Handles This\n",
        "\n",
        "The Agent Memory Server automatically:\n",
        "1. **Monitors message count** in working memory\n",
        "2. **Triggers summarization** when threshold is reached\n",
        "3. **Creates summary** of older messages\n",
        "4. **Replaces old messages** with summary\n",
        "5. **Keeps recent messages** for context\n",
        "\n",
        "### Token Budgets\n",
        "\n",
        "A **token budget** is how you allocate your context window:\n",
        "\n",
        "```\n",
        "Total: 128K tokens\n",
        "├─ System instructions: 1K tokens\n",
        "├─ Working memory: 8K tokens\n",
        "├─ Long-term memories: 2K tokens\n",
        "├─ Retrieved context: 4K tokens\n",
        "├─ User message: 500 tokens\n",
        "└─ Response space: 2K tokens\n",
        "    ────────────────────────────\n",
        "    Used: 17.5K / 128K (13.7%)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import tiktoken\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from redis_context_course import MemoryClient, MemoryClientConfig\n",
        "\n",
        "# Initialize\n",
        "student_id = \"student_context_demo\"\n",
        "session_id = \"long_conversation\"\n",
        "\n",
        "# Initialize memory client with proper config\n",
        "import os\n",
        "config = MemoryClientConfig(\n",
        "    base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8000\"),\n",
        "    default_namespace=\"redis_university\"\n",
        ")\n",
        "memory_client = MemoryClient(config=config)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "# Initialize tokenizer for counting\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    \"\"\"Count tokens in text.\"\"\"\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "print(f\"✅ Setup complete for {student_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hands-on: Understanding Token Counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Counting Tokens in Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure count_tokens is defined (in case cells are run out of order)\n",
        "if \"count_tokens\" not in globals():\n",
        "    import tiktoken\n",
        "    tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "    def count_tokens(text: str) -> int:\n",
        "        return len(tokenizer.encode(text))\n",
        "\n",
        "# Example messages\n",
        "messages = [\n",
        "    \"Hi, I'm interested in machine learning courses.\",\n",
        "    \"Can you recommend some courses for beginners?\",\n",
        "    \"What are the prerequisites for CS401?\",\n",
        "    \"I've completed CS101 and CS201. Can I take CS401?\",\n",
        "    \"Great! When is CS401 offered?\"\n",
        "]\n",
        "\n",
        "print(\"Token counts for individual messages:\\n\")\n",
        "total_tokens = 0\n",
        "for i, msg in enumerate(messages, 1):\n",
        "    tokens = count_tokens(msg)\n",
        "    total_tokens += tokens\n",
        "    print(f\"{i}. \\\"{msg}\\\"\")\n",
        "    print(f\"   Tokens: {tokens}\\n\")\n",
        "\n",
        "print(f\"Total tokens for 5 messages: {total_tokens}\")\n",
        "print(f\"Average tokens per message: {total_tokens / len(messages):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Token Growth Over Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure count_tokens is defined (in case cells are run out of order)\n",
        "if \"count_tokens\" not in globals():\n",
        "    import tiktoken\n",
        "    tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "    def count_tokens(text: str) -> int:\n",
        "        return len(tokenizer.encode(text))\n",
        "\n",
        "# Simulate conversation growth\n",
        "system_prompt = \"\"\"You are a helpful class scheduling agent for Redis University.\n",
        "Help students find courses and plan their schedule.\"\"\"\n",
        "\n",
        "system_tokens = count_tokens(system_prompt)\n",
        "print(f\"System prompt tokens: {system_tokens}\\n\")\n",
        "\n",
        "# Simulate growing conversation\n",
        "conversation_tokens = 0\n",
        "avg_message_tokens = 50  # Typical message size\n",
        "\n",
        "print(\"Token growth over conversation turns:\\n\")\n",
        "print(f\"{'Turn':<6} {'Messages':<10} {'Conv Tokens':<12} {'Total Tokens':<12} {'% of 128K'}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for turn in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
        "    # Each turn = user message + assistant message\n",
        "    conversation_tokens = turn * 2 * avg_message_tokens\n",
        "    total_tokens = system_tokens + conversation_tokens\n",
        "    percentage = (total_tokens / 128000) * 100\n",
        "    \n",
        "    print(f\"{turn:<6} {turn*2:<10} {conversation_tokens:<12,} {total_tokens:<12,} {percentage:>6.1f}%\")\n",
        "\n",
        "print(\"\\n⚠️  Without summarization, long conversations will eventually exceed limits!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuring Summarization\n",
        "\n",
        "The Agent Memory Server provides automatic summarization. Let's see how to configure it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Summarization Settings\n",
        "\n",
        "The Agent Memory Server uses these settings:\n",
        "\n",
        "**Message Count Threshold:**\n",
        "- When working memory exceeds this many messages, summarization triggers\n",
        "- Default: 20 messages (10 turns)\n",
        "- Configurable per session\n",
        "\n",
        "**Summarization Strategy:**\n",
        "- **Recent + Summary**: Keep recent N messages, summarize older ones\n",
        "- **Sliding Window**: Keep only recent N messages\n",
        "- **Full Summary**: Summarize everything\n",
        "\n",
        "**What Gets Summarized:**\n",
        "- Older conversation messages\n",
        "- Key facts and decisions\n",
        "- Important context\n",
        "\n",
        "**What Stays:**\n",
        "- Recent messages (for immediate context)\n",
        "- System instructions\n",
        "- Long-term memories (separate from working memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Demonstrating Summarization\n",
        "\n",
        "Let's create a conversation that triggers summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function for conversation\n",
        "async def have_conversation_turn(user_message, session_id):\n",
        "    \"\"\"Simulate a conversation turn.\"\"\"\n",
        "    # Get working memory\n",
        "    working_memory = await memory_client.get_or_create_working_memory(\n",
        "        session_id=session_id,\n",
        "        model_name=\"gpt-4o\"\n",
        "    )\n",
        "    \n",
        "    # Build messages\n",
        "    messages = [SystemMessage(content=\"You are a helpful class scheduling agent.\")]\n",
        "    \n",
        "    if working_memory and working_memory.messages:\n",
        "        for msg in working_memory.messages:\n",
        "            if msg.role == \"user\":\n",
        "                messages.append(HumanMessage(content=msg.content))\n",
        "            elif msg.role == \"assistant\":\n",
        "                messages.append(AIMessage(content=msg.content))\n",
        "    \n",
        "    messages.append(HumanMessage(content=user_message))\n",
        "    \n",
        "    # Get response\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # Save to working memory\n",
        "    all_messages = []\n",
        "    if working_memory and working_memory.messages:\n",
        "        all_messages = [{\"role\": m.role, \"content\": m.content} for m in working_memory.messages]\n",
        "    \n",
        "    all_messages.extend([\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "        {\"role\": \"assistant\", \"content\": response.content}\n",
        "    ])\n",
        "    \n",
        "    await memory_client.save_working_memory(\n",
        "        session_id=session_id,\n",
        "        messages=all_messages\n",
        "    )\n",
        "    \n",
        "    return response.content, len(all_messages)\n",
        "\n",
        "print(\"✅ Helper function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Have a multi-turn conversation\n",
        "print(\"=\" * 80)\n",
        "print(\"DEMONSTRATING SUMMARIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "conversation_queries = [\n",
        "    \"Hi, I'm a computer science major interested in AI.\",\n",
        "    \"What machine learning courses do you offer?\",\n",
        "    \"Tell me about CS401.\",\n",
        "    \"What are the prerequisites?\",\n",
        "    \"I've completed CS101 and CS201.\",\n",
        "    \"Can I take CS401 next semester?\",\n",
        "    \"When is it offered?\",\n",
        "    \"Is it available online?\",\n",
        "    \"What about CS402?\",\n",
        "    \"Can I take both CS401 and CS402?\",\n",
        "    \"What's the workload like?\",\n",
        "    \"Are there any projects?\",\n",
        "]\n",
        "\n",
        "for i, query in enumerate(conversation_queries, 1):\n",
        "    print(f\"\\nTurn {i}:\")\n",
        "    print(f\"User: {query}\")\n",
        "    \n",
        "    response, message_count = await have_conversation_turn(query, session_id)\n",
        "    \n",
        "    print(f\"Agent: {response[:100]}...\")\n",
        "    print(f\"Total messages in working memory: {message_count}\")\n",
        "    \n",
        "    if message_count > 20:\n",
        "        print(\"⚠️  Message count exceeds threshold - summarization may trigger\")\n",
        "    \n",
        "    await asyncio.sleep(0.5)  # Rate limiting\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ Conversation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 4: Checking Working Memory After Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check working memory state\n",
        "print(\"\\nChecking working memory state...\\n\")\n",
        "\n",
        "working_memory = await memory_client.get_or_create_working_memory(\n",
        "    session_id=session_id,\n",
        "    model_name=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "if working_memory:\n",
        "    print(f\"Total messages: {len(working_memory.messages)}\")\n",
        "    print(f\"\\nMessage breakdown:\")\n",
        "    \n",
        "    user_msgs = [m for m in working_memory.messages if m.role == \"user\"]\n",
        "    assistant_msgs = [m for m in working_memory.messages if m.role == \"assistant\"]\n",
        "    system_msgs = [m for m in working_memory.messages if m.role == \"system\"]\n",
        "    \n",
        "    print(f\"  User messages: {len(user_msgs)}\")\n",
        "    print(f\"  Assistant messages: {len(assistant_msgs)}\")\n",
        "    print(f\"  System messages (summaries): {len(system_msgs)}\")\n",
        "    \n",
        "    # Check for summary messages\n",
        "    if system_msgs:\n",
        "        print(\"\\n✅ Summarization occurred! Summary messages found:\")\n",
        "        for msg in system_msgs:\n",
        "            print(f\"\\n  Summary: {msg.content[:200]}...\")\n",
        "    else:\n",
        "        print(\"\\n⏳ No summarization yet (may need more messages or time)\")\n",
        "else:\n",
        "    print(\"No working memory found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Context Window Management Strategy\n",
        "\n",
        "1. **Monitor token usage** - Know your limits\n",
        "2. **Set message thresholds** - Trigger summarization before hitting limits\n",
        "3. **Keep recent context** - Don't summarize everything\n",
        "4. **Use long-term memory** - Important facts go there, not working memory\n",
        "5. **Trust automatic summarization** - Agent Memory Server handles it\n",
        "\n",
        "### Token Budget Best Practices\n",
        "\n",
        "**Allocate wisely:**\n",
        "- System instructions: 1-2K tokens\n",
        "- Working memory: 4-8K tokens\n",
        "- Long-term memories: 2-4K tokens\n",
        "- Retrieved context: 2-4K tokens\n",
        "- Response space: 2-4K tokens\n",
        "\n",
        "**Total: ~15-20K tokens (leaves plenty of headroom)**\n",
        "\n",
        "### When Summarization Happens\n",
        "\n",
        "The Agent Memory Server triggers summarization when:\n",
        "- ✅ Message count exceeds threshold (default: 20)\n",
        "- ✅ Token count approaches limits\n",
        "- ✅ Configured summarization strategy activates\n",
        "\n",
        "### What Summarization Preserves\n",
        "\n",
        "✅ **Preserved:**\n",
        "- Key facts and decisions\n",
        "- Important context\n",
        "- Recent messages (full text)\n",
        "- Long-term memories (separate storage)\n",
        "\n",
        "❌ **Compressed:**\n",
        "- Older conversation details\n",
        "- Redundant information\n",
        "- Small talk\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "Without proper context window management:\n",
        "- ❌ Conversations fail when limits are hit\n",
        "- ❌ Costs grow linearly with conversation length\n",
        "- ❌ Performance degrades with more tokens\n",
        "\n",
        "With proper management:\n",
        "- ✅ Conversations can continue indefinitely\n",
        "- ✅ Costs stay predictable\n",
        "- ✅ Performance stays consistent\n",
        "- ✅ Important context is preserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Calculate your token budget**: For your agent, allocate tokens across system prompt, working memory, long-term memories, and response space.\n",
        "\n",
        "2. **Test long conversations**: Have a 50-turn conversation and monitor token usage. When does summarization trigger?\n",
        "\n",
        "3. **Compare strategies**: Test different message thresholds (10, 20, 50). How does it affect conversation quality?\n",
        "\n",
        "4. **Measure costs**: Calculate the cost difference between keeping full history vs. using summarization for a 100-turn conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "- ✅ Context windows have token limits that conversations can exceed\n",
        "- ✅ Token budgets help allocate context window space\n",
        "- ✅ Summarization is necessary for long conversations\n",
        "- ✅ Agent Memory Server provides automatic summarization\n",
        "- ✅ Proper management enables indefinite conversations\n",
        "\n",
        "**Key insight:** Context window management isn't about proving you need summarization - it's about understanding the constraints and using the right tools (like Agent Memory Server) to handle them automatically."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

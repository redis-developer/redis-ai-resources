{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Context Engineering with Memory: Building on Your RAG Agent\n",
    "\n",
    "## From Grounding Problem to Memory Solution\n",
    "\n",
    "In the previous notebook, you experienced the **grounding problem** - how references break without memory. Now you'll learn to solve this with **sophisticated memory architecture** that enhances your context engineering.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "Transform your RAG agent with **memory-enhanced context engineering**:\n",
    "\n",
    "- **üß† Working Memory** - Session-scoped conversation context\n",
    "- **üìö Long-term Memory** - Cross-session knowledge and preferences  \n",
    "- **üîÑ Memory Integration** - Seamless working + long-term memory\n",
    "- **‚ö° Agent Memory Server** - Production-ready memory architecture\n",
    "\n",
    "### Context Engineering Focus\n",
    "\n",
    "This notebook teaches **memory-enhanced context engineering best practices**:\n",
    "\n",
    "1. **Memory-Aware Context Assembly** - How memory improves context quality\n",
    "2. **Reference Resolution** - Using memory to resolve pronouns and references\n",
    "3. **Personalized Context** - Leveraging long-term memory for personalization\n",
    "4. **Context Efficiency** - Memory prevents context repetition and bloat\n",
    "5. **Cross-Session Continuity** - Context that survives across conversations\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. **Implement** working memory for conversation context\n",
    "2. **Use** long-term memory for persistent knowledge\n",
    "3. **Build** memory-enhanced context engineering patterns\n",
    "4. **Create** agents that remember and learn from interactions\n",
    "5. **Apply** production-ready memory architecture with Agent Memory Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Agent Memory Server Architecture\n",
    "\n",
    "We'll use the **Agent Memory Server** - a production-ready memory system that provides:\n",
    "\n",
    "- **Working Memory** - Session-scoped conversation storage\n",
    "- **Long-term Memory** - Persistent, searchable knowledge\n",
    "- **Automatic Extraction** - AI-powered fact extraction from conversations\n",
    "- **Vector Search** - Semantic search across memories\n",
    "- **Deduplication** - Prevents redundant memory storage\n",
    "\n",
    "This is the same architecture used in the `redis_context_course` reference agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import the reference agent components and memory client\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "sys.path.append('../../reference-agent')\n",
    "\n",
    "# Import reference agent components\n",
    "from redis_context_course.models import (\n",
    "    Course, StudentProfile, DifficultyLevel, \n",
    "    CourseFormat, Semester\n",
    ")\n",
    "from redis_context_course.course_manager import CourseManager\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Import Agent Memory Server client\n",
    "try:\n",
    "    from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "    from agent_memory_client.models import WorkingMemory, MemoryMessage\n",
    "    MEMORY_SERVER_AVAILABLE = True\n",
    "    print(\"‚úÖ Agent Memory Server client available\")\n",
    "except ImportError:\n",
    "    MEMORY_SERVER_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Agent Memory Server not available\")\n",
    "    print(\"üìù Install with: pip install agent-memory-server\")\n",
    "    print(\"üöÄ Start server with: agent-memory-server\")\n",
    "\n",
    "# Verify environment\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please set in .env file.\")\n",
    "\n",
    "print(f\"\\nüîß Environment Setup:\")\n",
    "print(f\"   OPENAI_API_KEY: {'‚úì Set' if os.getenv('OPENAI_API_KEY') else '‚úó Not set'}\")\n",
    "print(f\"   AGENT_MEMORY_URL: {os.getenv('AGENT_MEMORY_URL', 'http://localhost:8000')}\")\n",
    "print(f\"   Memory Server: {'‚úì Available' if MEMORY_SERVER_AVAILABLE else '‚úó Not available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Working Memory for Context Engineering\n",
    "\n",
    "**Working memory** solves the grounding problem by storing conversation context. Let's see how this enhances context engineering.\n",
    "\n",
    "### Context Engineering Problem Without Memory\n",
    "\n",
    "Recall from the grounding notebook:\n",
    "- **Broken references**: \"What are its prerequisites?\" ‚Üí Agent doesn't know what \"its\" refers to\n",
    "- **Lost context**: Each message is processed in isolation\n",
    "- **Poor UX**: Users must repeat information\n",
    "\n",
    "### Context Engineering Solution With Working Memory\n",
    "\n",
    "Working memory enables **memory-enhanced context engineering**:\n",
    "- **Reference resolution**: \"its\" ‚Üí CS401 (from conversation history)\n",
    "- **Context continuity**: Each message builds on previous messages\n",
    "- **Natural conversations**: Users can speak naturally with pronouns and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Memory Client for working memory\n",
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    # Configure memory client\n",
    "    config = MemoryClientConfig(\n",
    "        base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8000\"),\n",
    "        default_namespace=\"redis_university\"\n",
    "    )\n",
    "    memory_client = MemoryAPIClient(config=config)\n",
    "    \n",
    "    print(\"üß† Memory Client Initialized\")\n",
    "    print(f\"   Base URL: {config.base_url}\")\n",
    "    print(f\"   Namespace: {config.default_namespace}\")\n",
    "    print(\"   Ready for working memory operations\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Simulating memory operations (Memory Server not available)\")\n",
    "    memory_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Memory Structure\n",
    "\n",
    "Working memory contains the essential context for the current conversation:\n",
    "\n",
    "- **Messages**: The conversation history (user and assistant messages)\n",
    "- **Session ID**: Identifies this specific conversation\n",
    "- **User ID**: Identifies the user across sessions\n",
    "- **Task Data**: Optional task-specific context (current goals, temporary state)\n",
    "\n",
    "This structure gives the LLM everything it needs to understand the current conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate working memory with a conversation that has references\n",
    "async def demonstrate_working_memory():\n",
    "    \"\"\"Show how working memory enables reference resolution in context engineering\"\"\"\n",
    "    \n",
    "    if not MEMORY_SERVER_AVAILABLE:\n",
    "        print(\"üìù This would demonstrate working memory with Agent Memory Server\")\n",
    "        return\n",
    "    \n",
    "    # Create a student and session\n",
    "    student_id = \"demo_student_working_memory\"\n",
    "    session_id = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    print(f\"üí¨ Starting Conversation with Working Memory\")\n",
    "    print(f\"   Student ID: {student_id}\")\n",
    "    print(f\"   Session ID: {session_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Simulate a conversation with references\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about RU301 Vector Search\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"RU301 Vector Search teaches you to build semantic search with Redis. It covers vector embeddings, similarity search, and practical applications.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are its prerequisites?\"},  # \"its\" refers to RU301\n",
    "        {\"role\": \"assistant\", \"content\": \"RU301 requires RU101 (Redis Fundamentals) and RU201 (Redis for Python Developers) as prerequisites.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can I take it if I've completed those?\"}  # \"it\" refers to RU301, \"those\" refers to prerequisites\n",
    "    ]\n",
    "    \n",
    "    # Convert to MemoryMessage format\n",
    "    memory_messages = [MemoryMessage(**msg) for msg in conversation]\n",
    "    \n",
    "    # Create WorkingMemory object\n",
    "    working_memory = WorkingMemory(\n",
    "        session_id=session_id,\n",
    "        user_id=student_id,\n",
    "        messages=memory_messages,\n",
    "        memories=[],  # Long-term memories will be added here\n",
    "        data={}  # Task-specific data\n",
    "    )\n",
    "    \n",
    "    # Store working memory\n",
    "    await memory_client.put_working_memory(\n",
    "        session_id=session_id,\n",
    "        memory=working_memory,\n",
    "        user_id=student_id,\n",
    "        model_name=\"gpt-4o\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Conversation stored in working memory\")\n",
    "    print(f\"üìä Messages stored: {len(conversation)}\")\n",
    "    print()\n",
    "    \n",
    "    # Retrieve working memory to show context engineering\n",
    "    _, retrieved_memory = await memory_client.get_or_create_working_memory(\n",
    "        session_id=session_id,\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_id=student_id\n",
    "    )\n",
    "    \n",
    "    if retrieved_memory:\n",
    "        print(\"üéØ Context Engineering with Working Memory:\")\n",
    "        print(\"   The LLM now has access to full conversation context\")\n",
    "        print(\"   References can be resolved:\")\n",
    "        print(\"   ‚Ä¢ 'its prerequisites' ‚Üí RU301's prerequisites\")\n",
    "        print(\"   ‚Ä¢ 'Can I take it' ‚Üí Can I take RU301\")\n",
    "        print(\"   ‚Ä¢ 'those' ‚Üí RU101 and RU201\")\n",
    "        print()\n",
    "        print(f\"üìã Retrieved {len(retrieved_memory.messages)} messages from working memory\")\n",
    "        \n",
    "        return session_id, student_id\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Run the demonstration\n",
    "session_id, student_id = await demonstrate_working_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Long-term Memory for Personalized Context Engineering\n",
    "\n",
    "**Long-term memory** stores persistent knowledge that enhances context engineering across sessions:\n",
    "\n",
    "- **Semantic Memory**: Facts and preferences (\"Student prefers online courses\")\n",
    "- **Episodic Memory**: Events and experiences (\"Student enrolled in CS101 on 2024-09-15\")\n",
    "- **Message Memory**: Important conversation snippets\n",
    "\n",
    "### Context Engineering Benefits\n",
    "\n",
    "Long-term memory enables **personalized context engineering**:\n",
    "- **Preference-aware context**: Include user preferences in context assembly\n",
    "- **Historical context**: Reference past interactions and decisions\n",
    "- **Efficient context**: Avoid repeating known information\n",
    "- **Cross-session continuity**: Context that survives across conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate long-term memory for context engineering\n",
    "async def demonstrate_long_term_memory():\n",
    "    \"\"\"Show how long-term memory enhances context engineering with persistent knowledge\"\"\"\n",
    "    \n",
    "    if not MEMORY_SERVER_AVAILABLE:\n",
    "        print(\"üìù This would demonstrate long-term memory with Agent Memory Server\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìö Long-term Memory for Context Engineering\")\n",
    "    print()\n",
    "    \n",
    "    # Store some semantic memories (facts and preferences)\n",
    "    semantic_memories = [\n",
    "        \"Student prefers online courses over in-person\",\n",
    "        \"Student's major is Computer Science\",\n",
    "        \"Student wants to specialize in machine learning\",\n",
    "        \"Student has completed RU101 and RU201\",\n",
    "        \"Student prefers hands-on learning with practical projects\"\n",
    "    ]\n",
    "    \n",
    "    user_id = student_id or \"demo_student_longterm\"\n",
    "    \n",
    "    print(f\"üíæ Storing semantic memories for user: {user_id}\")\n",
    "    \n",
    "    for memory_text in semantic_memories:\n",
    "        try:\n",
    "            await memory_client.create_semantic_memory(\n",
    "                user_id=user_id,\n",
    "                text=memory_text\n",
    "            )\n",
    "            print(f\"   ‚úÖ Stored: {memory_text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not store: {memory_text} ({e})\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Search long-term memory to show context engineering benefits\n",
    "    search_queries = [\n",
    "        \"course preferences\",\n",
    "        \"learning style\",\n",
    "        \"completed courses\",\n",
    "        \"career goals\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üîç Searching long-term memory for context engineering:\")\n",
    "    \n",
    "    for query in search_queries:\n",
    "        try:\n",
    "            results = await memory_client.search_memories(\n",
    "                user_id=user_id,\n",
    "                query=query,\n",
    "                limit=3\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n   Query: '{query}'\")\n",
    "            if results:\n",
    "                for i, result in enumerate(results, 1):\n",
    "                    print(f\"   {i}. {result.text} (score: {result.score:.3f})\")\n",
    "            else:\n",
    "                print(\"   No results found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Search failed for '{query}': {e}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üéØ Context Engineering Impact:\")\n",
    "    print(\"   ‚Ä¢ Personalized recommendations based on preferences\")\n",
    "    print(\"   ‚Ä¢ Efficient context assembly (no need to re-ask preferences)\")\n",
    "    print(\"   ‚Ä¢ Cross-session continuity (remembers across conversations)\")\n",
    "    print(\"   ‚Ä¢ Semantic search finds relevant context automatically\")\n",
    "\n",
    "# Run long-term memory demonstration\n",
    "await demonstrate_long_term_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Memory Integration - Complete Context Engineering\n",
    "\n",
    "The power of memory-enhanced context engineering comes from **integrating working and long-term memory**.\n",
    "\n",
    "### Complete Memory Flow for Context Engineering\n",
    "\n",
    "```\n",
    "User Query ‚Üí Agent Processing\n",
    "     ‚Üì\n",
    "1. Load Working Memory (conversation context)\n",
    "     ‚Üì\n",
    "2. Search Long-term Memory (relevant facts)\n",
    "     ‚Üì\n",
    "3. Assemble Enhanced Context:\n",
    "   ‚Ä¢ Current conversation (working memory)\n",
    "   ‚Ä¢ Relevant preferences (long-term memory)\n",
    "   ‚Ä¢ Historical context (long-term memory)\n",
    "     ‚Üì\n",
    "4. LLM processes with complete context\n",
    "     ‚Üì\n",
    "5. Save response to working memory\n",
    "     ‚Üì\n",
    "6. Extract important facts ‚Üí long-term memory\n",
    "```\n",
    "\n",
    "This creates **memory-enhanced context engineering** that provides:\n",
    "- **Complete context**: Both immediate and historical\n",
    "- **Personalized context**: Tailored to user preferences\n",
    "- **Efficient context**: No redundant information\n",
    "- **Persistent context**: Survives across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Building the Memory-Enhanced RAG Agent Foundation\n",
    "\n",
    "Let's start by creating the basic structure of our memory-enhanced agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Memory-Enhanced RAG Agent using reference agent components\n",
    "class MemoryEnhancedRAGAgent:\n",
    "    \"\"\"RAG Agent with sophisticated memory-enhanced context engineering\"\"\"\n",
    "    \n",
    "    def __init__(self, course_manager: CourseManager, memory_client=None):\n",
    "        self.course_manager = course_manager\n",
    "        self.memory_client = memory_client\n",
    "        self.llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "    \n",
    "    async def create_memory_enhanced_context(\n",
    "        self, \n",
    "        student: StudentProfile, \n",
    "        query: str, \n",
    "        session_id: str,\n",
    "        courses: List[Course] = None\n",
    "    ) -> str:\n",
    "        \"\"\"üéØ MEMORY-ENHANCED CONTEXT ENGINEERING\n",
    "        \n",
    "        This demonstrates advanced context engineering with memory integration.\n",
    "        \n",
    "        CONTEXT ENGINEERING ENHANCEMENTS:\n",
    "        ‚úÖ Working Memory - Current conversation context\n",
    "        ‚úÖ Long-term Memory - Persistent user knowledge\n",
    "        ‚úÖ Semantic Search - Relevant memory retrieval\n",
    "        ‚úÖ Reference Resolution - Pronouns and implicit references\n",
    "        ‚úÖ Personalization - User-specific context assembly\n",
    "        \"\"\"\n",
    "        \n",
    "        context_parts = []\n",
    "        \n",
    "        # 1. STUDENT PROFILE CONTEXT (Base layer)\n",
    "        student_context = f\"\"\"STUDENT PROFILE:\n",
    "Name: {student.name}\n",
    "Email: {student.email}\n",
    "Major: {student.major}, Year {student.year}\n",
    "Completed Courses: {', '.join(student.completed_courses) if student.completed_courses else 'None'}\n",
    "Current Courses: {', '.join(student.current_courses) if student.current_courses else 'None'}\n",
    "Interests: {', '.join(student.interests)}\n",
    "Preferred Format: {student.preferred_format.value if student.preferred_format else 'Any'}\n",
    "Preferred Difficulty: {student.preferred_difficulty.value if student.preferred_difficulty else 'Any'}\"\"\"\n",
    "        \n",
    "        context_parts.append(student_context)\n",
    "        \n",
    "        # 2. LONG-TERM MEMORY CONTEXT (Personalization layer)\n",
    "        if self.memory_client:\n",
    "            try:\n",
    "                # Search for relevant long-term memories\n",
    "                memory_results = await self.memory_client.search_memories(\n",
    "                    user_id=student.email,\n",
    "                    query=query,\n",
    "                    limit=5\n",
    "                )\n",
    "                \n",
    "                if memory_results:\n",
    "                    memory_context = \"\\nRELEVANT MEMORIES:\\n\"\n",
    "                    for i, memory in enumerate(memory_results, 1):\n",
    "                        memory_context += f\"{i}. {memory.text}\\n\"\n",
    "                    context_parts.append(memory_context)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not retrieve long-term memories: {e}\")\n",
    "        \n",
    "        # 3. COURSE CONTEXT (RAG layer)\n",
    "        if courses:\n",
    "            courses_context = \"\\nRELEVANT COURSES:\\n\"\n",
    "            for i, course in enumerate(courses, 1):\n",
    "                courses_context += f\"\"\"{i}. {course.course_code}: {course.title}\n",
    "   Description: {course.description}\n",
    "   Level: {course.difficulty_level.value}\n",
    "   Format: {course.format.value}\n",
    "   Credits: {course.credits}\n",
    "   Prerequisites: {', '.join(course.prerequisites) if course.prerequisites else 'None'}\n",
    "\n",
    "\"\"\"\n",
    "            context_parts.append(courses_context)\n",
    "        \n",
    "        # 4. WORKING MEMORY CONTEXT (Conversation layer)\n",
    "        if self.memory_client:\n",
    "            try:\n",
    "                # Get working memory for conversation context\n",
    "                _, working_memory = await self.memory_client.get_or_create_working_memory(\n",
    "                    session_id=session_id,\n",
    "                    model_name=\"gpt-3.5-turbo\",\n",
    "                    user_id=student.email\n",
    "                )\n",
    "                \n",
    "                if working_memory and working_memory.messages:\n",
    "                    conversation_context = \"\\nCONVERSATION HISTORY:\\n\"\n",
    "                    # Show recent messages for reference resolution\n",
    "                    for msg in working_memory.messages[-6:]:  # Last 6 messages\n",
    "                        conversation_context += f\"{msg.role.title()}: {msg.content}\\n\"\n",
    "                    context_parts.append(conversation_context)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not retrieve working memory: {e}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    async def chat_with_memory(\n",
    "        self, \n",
    "        student: StudentProfile, \n",
    "        query: str, \n",
    "        session_id: str\n",
    "    ) -> str:\n",
    "        \"\"\"Enhanced chat with complete memory integration\"\"\"\n",
    "        \n",
    "        # 1. Search for relevant courses\n",
    "        relevant_courses = await self.course_manager.search_courses(query, limit=3)\n",
    "        \n",
    "        # 2. Create memory-enhanced context\n",
    "        context = await self.create_memory_enhanced_context(\n",
    "            student, query, session_id, relevant_courses\n",
    "        )\n",
    "        \n",
    "        # 3. Create messages for LLM\n",
    "        system_message = SystemMessage(content=\"\"\"You are a helpful academic advisor for Redis University.\n",
    "Use the provided context to give personalized advice. Pay special attention to:\n",
    "- Student's learning history and preferences from memories\n",
    "- Current conversation context for reference resolution\n",
    "- Course recommendations based on student profile and interests\n",
    "\n",
    "Be specific, helpful, and reference the student's known preferences and history.\"\"\")\n",
    "        \n",
    "        human_message = HumanMessage(content=f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Student Question: {query}\n",
    "\n",
    "Please provide helpful academic advice based on the complete context.\"\"\")\n",
    "        \n",
    "        # 4. Get LLM response\n",
    "        response = self.llm.invoke([system_message, human_message])\n",
    "        \n",
    "        # 5. Store conversation in working memory\n",
    "        if self.memory_client:\n",
    "            await self._update_working_memory(student.email, session_id, query, response.content)\n",
    "        \n",
    "        return response.content\n",
    "    \n",
    "    async def _update_working_memory(self, user_id: str, session_id: str, user_message: str, assistant_message: str):\n",
    "        \"\"\"Update working memory with new conversation turn\"\"\"\n",
    "        try:\n",
    "            # Get current working memory\n",
    "            _, working_memory = await self.memory_client.get_or_create_working_memory(\n",
    "                session_id=session_id,\n",
    "                model_name=\"gpt-3.5-turbo\",\n",
    "                user_id=user_id\n",
    "            )\n",
    "            \n",
    "            # Add new messages\n",
    "            new_messages = [\n",
    "                MemoryMessage(role=\"user\", content=user_message),\n",
    "                MemoryMessage(role=\"assistant\", content=assistant_message)\n",
    "            ]\n",
    "            \n",
    "            working_memory.messages.extend(new_messages)\n",
    "            \n",
    "            # Save updated working memory\n",
    "            await self.memory_client.put_working_memory(\n",
    "                session_id=session_id,\n",
    "                memory=working_memory,\n",
    "                user_id=user_id,\n",
    "                model_name=\"gpt-3.5-turbo\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not update working memory: {e}\")\n",
    "\n",
    "print(\"üß† MemoryEnhancedRAGAgent created with sophisticated context engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing Memory-Enhanced Context Engineering\n",
    "\n",
    "Let's test our memory-enhanced agent to see how it solves the grounding problem and improves context engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the memory-enhanced agent\n",
    "async def test_memory_enhanced_context_engineering():\n",
    "    \"\"\"Demonstrate how memory solves context engineering challenges\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    course_manager = CourseManager()\n",
    "    agent = MemoryEnhancedRAGAgent(course_manager, memory_client)\n",
    "    \n",
    "    # Create test student\n",
    "    sarah = StudentProfile(\n",
    "        name='Sarah Chen',\n",
    "        email='sarah.chen@university.edu',\n",
    "        major='Computer Science',\n",
    "        year=3,\n",
    "        completed_courses=['RU101', 'RU201'],\n",
    "        current_courses=[],\n",
    "        interests=['machine learning', 'data science', 'python'],\n",
    "        preferred_format=CourseFormat.ONLINE,\n",
    "        preferred_difficulty=DifficultyLevel.INTERMEDIATE,\n",
    "        max_credits_per_semester=15\n",
    "    )\n",
    "    \n",
    "    # Create session\n",
    "    test_session_id = f\"test_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    print(\"üß™ Testing Memory-Enhanced Context Engineering\")\n",
    "    print(f\"   Student: {sarah.name}\")\n",
    "    print(f\"   Session: {test_session_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Test conversation with references (the grounding problem)\n",
    "    test_conversation = [\n",
    "        \"Hi! I'm interested in machine learning courses. What do you recommend?\",\n",
    "        \"What are the prerequisites for it?\",  # \"it\" should resolve to the recommended ML course\n",
    "        \"I prefer hands-on learning. Does it have practical projects?\",  # \"it\" = same course\n",
    "        \"Perfect! Can I take it next semester?\",  # \"it\" = same course\n",
    "        \"What about the course you mentioned earlier?\",  # temporal reference\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_conversation, 1):\n",
    "        print(f\"--- Turn {i} ---\")\n",
    "        print(f\"üë§ Student: {query}\")\n",
    "        \n",
    "        if MEMORY_SERVER_AVAILABLE:\n",
    "            try:\n",
    "                response = await agent.chat_with_memory(sarah, query, test_session_id)\n",
    "                print(f\"ü§ñ Agent: {response[:200]}...\" if len(response) > 200 else f\"ü§ñ Agent: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error: {e}\")\n",
    "        else:\n",
    "            print(\"ü§ñ Agent: [Would respond with memory-enhanced context]\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"‚úÖ Context Engineering Success:\")\n",
    "    print(\"   ‚Ä¢ References resolved using working memory\")\n",
    "    print(\"   ‚Ä¢ Personalized responses using long-term memory\")\n",
    "    print(\"   ‚Ä¢ Natural conversation flow maintained\")\n",
    "    print(\"   ‚Ä¢ No need for users to repeat information\")\n",
    "\n",
    "# Run the test\n",
    "await test_memory_enhanced_context_engineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Memory-Enhanced Context Engineering\n",
    "\n",
    "### üéØ **Context Engineering Principles with Memory**\n",
    "\n",
    "#### **1. Reference Resolution**\n",
    "- **Working Memory** enables pronoun resolution (\"it\" ‚Üí specific course)\n",
    "- **Conversation History** provides context for temporal references (\"you mentioned\")\n",
    "- **Natural Language** patterns work without explicit clarification\n",
    "\n",
    "#### **2. Personalized Context Assembly**\n",
    "- **Long-term Memory** provides user preferences and history\n",
    "- **Semantic Search** finds relevant memories automatically\n",
    "- **Context Efficiency** avoids repeating known information\n",
    "\n",
    "#### **3. Cross-Session Continuity**\n",
    "- **Persistent Knowledge** survives across conversations\n",
    "- **Learning Accumulation** builds better understanding over time\n",
    "- **Context Evolution** improves with each interaction\n",
    "\n",
    "#### **4. Production-Ready Architecture**\n",
    "- **Agent Memory Server** provides scalable memory management\n",
    "- **Automatic Extraction** learns from conversations\n",
    "- **Vector Search** enables semantic memory retrieval\n",
    "- **Deduplication** prevents redundant memory storage\n",
    "\n",
    "### üöÄ **Memory-Enhanced Context Engineering Best Practices**\n",
    "\n",
    "1. **Layer Your Context**:\n",
    "   - Base: Student profile\n",
    "   - Personalization: Long-term memories\n",
    "   - Domain: Relevant courses/content\n",
    "   - Conversation: Working memory\n",
    "\n",
    "2. **Enable Reference Resolution**:\n",
    "   - Store conversation history in working memory\n",
    "   - Provide recent messages for pronoun resolution\n",
    "   - Use temporal context for \"you mentioned\" references\n",
    "\n",
    "3. **Leverage Semantic Search**:\n",
    "   - Search long-term memory with user queries\n",
    "   - Include relevant memories in context\n",
    "   - Let the system find connections automatically\n",
    "\n",
    "4. **Optimize Context Efficiency**:\n",
    "   - Avoid repeating information stored in memory\n",
    "   - Use memory to reduce context bloat\n",
    "   - Focus context on new and relevant information\n",
    "\n",
    "### üéì **Next Steps**\n",
    "\n",
    "You've now mastered **memory-enhanced context engineering**! In Section 4, you'll learn:\n",
    "\n",
    "- **Tool Selection** - Semantic routing to specialized tools\n",
    "- **Multi-Tool Coordination** - Memory-aware tool orchestration\n",
    "- **Advanced Agent Patterns** - Building sophisticated AI assistants\n",
    "\n",
    "**Your RAG agent now has the memory foundation for advanced AI capabilities!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

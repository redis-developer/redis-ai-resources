{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Building on Your RAG Agent: Adding Memory for Context Engineering\n",
    "\n",
    "## From Grounding Problem to Memory Solution\n",
    "\n",
    "In the previous notebook, you experienced the **grounding problem** - how references break without memory. Now you'll enhance your existing RAG agent from Section 2 with memory capabilities.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "**Enhance your existing `SimpleRAGAgent`** with memory:\n",
    "\n",
    "- **üß† Working Memory** - Session-scoped conversation context\n",
    "- **üìö Long-term Memory** - Cross-session knowledge and preferences  \n",
    "- **üîÑ Memory Integration** - Seamless working + long-term memory\n",
    "- **‚ö° Agent Memory Server** - Production-ready memory architecture\n",
    "\n",
    "### Context Engineering Focus\n",
    "\n",
    "This notebook teaches **memory-enhanced context engineering** by building on your existing agent:\n",
    "\n",
    "1. **Reference Resolution** - Using memory to resolve pronouns and references\n",
    "2. **Memory-Aware Context Assembly** - How memory improves context quality\n",
    "3. **Personalized Context** - Leveraging long-term memory for personalization\n",
    "4. **Cross-Session Continuity** - Context that survives across conversations\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. **Enhance** your existing RAG agent with memory capabilities\n",
    "2. **Implement** working memory for conversation context\n",
    "3. **Use** long-term memory for persistent knowledge\n",
    "4. **Build** memory-enhanced context engineering patterns\n",
    "5. **Create** a final production-ready memory-enhanced agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Components and Initialize Environment\n",
    "\n",
    "Let's start by importing your RAG agent from Section 2 and the memory components we'll use to enhance it.\n",
    "\n",
    "### üéØ **What We're Importing**\n",
    "- **Your RAG agent models** from Section 2 (`StudentProfile`, `Course`, etc.)\n",
    "- **Course manager** for searching Redis University courses\n",
    "- **LangChain components** for LLM interaction\n",
    "- **Agent Memory Server client** for production-ready memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent Memory Server client available\n",
      "‚úÖ OPENAI_API_KEY found\n",
      "\n",
      "üîß Environment Setup:\n",
      "   OPENAI_API_KEY: ‚úì Set\n",
      "   AGENT_MEMORY_URL: http://localhost:8088\n",
      "   Memory Server: ‚úì Available\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import your RAG agent and memory components\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "sys.path.append('../../reference-agent')\n",
    "\n",
    "# Import your RAG agent components from Section 2\n",
    "from redis_context_course.models import (\n",
    "    Course, StudentProfile, DifficultyLevel, \n",
    "    CourseFormat, Semester\n",
    ")\n",
    "from redis_context_course.course_manager import CourseManager\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Import Agent Memory Server client\n",
    "try:\n",
    "    from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "    from agent_memory_client.models import WorkingMemory, MemoryMessage\n",
    "    MEMORY_SERVER_AVAILABLE = True\n",
    "    print(\"‚úÖ Agent Memory Server client available\")\n",
    "except ImportError:\n",
    "    MEMORY_SERVER_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Agent Memory Server not available\")\n",
    "    print(\"üìù Install with: pip install agent-memory-client\")\n",
    "    print(\"üöÄ Start server with: docker-compose up\")\n",
    "\n",
    "# Verify environment\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ùå OPENAI_API_KEY not found. Please set in .env file.\")\n",
    "else:\n",
    "    print(\"‚úÖ OPENAI_API_KEY found\")\n",
    "\n",
    "print(f\"\\nüîß Environment Setup:\")\n",
    "print(f\"   OPENAI_API_KEY: {'‚úì Set' if os.getenv('OPENAI_API_KEY') else '‚úó Not set'}\")\n",
    "print(f\"   AGENT_MEMORY_URL: {os.getenv('AGENT_MEMORY_URL', 'http://localhost:8088')}\")\n",
    "print(f\"   Memory Server: {'‚úì Available' if MEMORY_SERVER_AVAILABLE else '‚úó Not available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ **What We Just Did**\n",
    "\n",
    "**Successfully Imported:**\n",
    "- ‚úÖ **Your RAG agent models** from Section 2\n",
    "- ‚úÖ **Agent Memory Server client** for production-ready memory\n",
    "- ‚úÖ **Environment verified** - OpenAI API key and memory server ready\n",
    "\n",
    "**Why This Matters:**\n",
    "- We're building **on top of your existing Section 2 foundation**\n",
    "- **Agent Memory Server** provides scalable, persistent memory (vs simple in-memory storage)\n",
    "- **Production-ready architecture** that can handle real applications\n",
    "\n",
    "**Next:** We'll recreate your `SimpleRAGAgent` from Section 2 as our starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Your RAG Agent from Section 2\n",
    "\n",
    "Let's start with your `SimpleRAGAgent` from Section 2. This is the foundation we'll enhance with memory.\n",
    "\n",
    "### üîç **Current Limitations (What We'll Fix)**\n",
    "- **Session-bound memory** - Forgets everything when restarted\n",
    "- **No reference resolution** - Can't understand \"it\", \"that\", \"you mentioned\"\n",
    "- **Limited conversation history** - Only keeps last 2 messages\n",
    "- **No personalization** - Doesn't learn student preferences\n",
    "\n",
    "### üöÄ **What We'll Add**\n",
    "- **Working memory** - Persistent conversation context for reference resolution\n",
    "- **Long-term memory** - Cross-session knowledge and preferences\n",
    "- **Memory-enhanced context** - Smarter context assembly using memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù SimpleRAGAgent defined (your Section 2 foundation)\n",
      "‚ùå Limitations: Session-bound memory, no reference resolution, limited context\n"
     ]
    }
   ],
   "source": [
    "# Your SimpleRAGAgent from Section 2 - the foundation we'll enhance\n",
    "class SimpleRAGAgent:\n",
    "    \"\"\"Your RAG agent from Section 2 - foundation for memory enhancement\"\"\"\n",
    "    \n",
    "    def __init__(self, course_manager: CourseManager):\n",
    "        self.course_manager = course_manager\n",
    "        self.llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "        self.conversation_history = {}  # In-memory only - lost when restarted!\n",
    "    \n",
    "    async def search_courses(self, query: str, limit: int = 3) -> List[Course]:\n",
    "        \"\"\"Search for relevant courses using the course manager\"\"\"\n",
    "        results = await self.course_manager.search_courses(query, limit=limit)\n",
    "        return results\n",
    "    \n",
    "    def create_context(self, student: StudentProfile, query: str, courses: List[Course]) -> str:\n",
    "        \"\"\"Create context for the LLM - your excellent context engineering from Section 2\"\"\"\n",
    "        \n",
    "        # Student context\n",
    "        student_context = f\"\"\"STUDENT PROFILE:\n",
    "Name: {student.name}\n",
    "Academic Status: {student.major}, Year {student.year}\n",
    "Completed Courses: {', '.join(student.completed_courses) if student.completed_courses else 'None'}\n",
    "Learning Interests: {', '.join(student.interests)}\n",
    "Preferred Format: {student.preferred_format.value if student.preferred_format else 'Any'}\"\"\"\n",
    "        \n",
    "        # Courses context\n",
    "        courses_context = \"RELEVANT COURSES:\\n\"\n",
    "        for i, course in enumerate(courses, 1):\n",
    "            courses_context += f\"{i}. {course.course_code}: {course.title}\\n\"\n",
    "        \n",
    "        # Basic conversation history (limited and session-bound)\n",
    "        history_context = \"\"\n",
    "        if student.email in self.conversation_history:\n",
    "            history = self.conversation_history[student.email]\n",
    "            if history:\n",
    "                history_context = \"\\nRECENT CONVERSATION:\\n\"\n",
    "                for msg in history[-2:]:  # Only last 2 messages\n",
    "                    history_context += f\"User: {msg['user']}\\nAssistant: {msg['assistant']}\\n\"\n",
    "        \n",
    "        return student_context + \"\\n\\n\" + courses_context + history_context\n",
    "    \n",
    "    async def chat(self, student: StudentProfile, query: str) -> str:\n",
    "        \"\"\"Chat with the student using RAG\"\"\"\n",
    "        relevant_courses = await self.search_courses(query, limit=3)\n",
    "        context = self.create_context(student, query, relevant_courses)\n",
    "        \n",
    "        system_message = SystemMessage(content=\"\"\"You are a helpful academic advisor for Redis University. \n",
    "Use the provided context to give personalized course recommendations.\n",
    "Be specific and explain why courses are suitable for the student.\"\"\")\n",
    "        \n",
    "        human_message = HumanMessage(content=f\"Context: {context}\\n\\nStudent Question: {query}\")\n",
    "        response = self.llm.invoke([system_message, human_message])\n",
    "        \n",
    "        # Store in basic memory (session-bound)\n",
    "        if student.email not in self.conversation_history:\n",
    "            self.conversation_history[student.email] = []\n",
    "        \n",
    "        self.conversation_history[student.email].append({\n",
    "            \"user\": query,\n",
    "            \"assistant\": response.content\n",
    "        })\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "print(\"üìù SimpleRAGAgent defined (your Section 2 foundation)\")\n",
    "print(\"‚ùå Limitations: Session-bound memory, no reference resolution, limited context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ **What We Just Built**\n",
    "\n",
    "**Your `SimpleRAGAgent` from Section 2:**\n",
    "- ‚úÖ **Course search** - Finds relevant courses using vector search\n",
    "- ‚úÖ **Context engineering** - Assembles student profile + courses + basic history\n",
    "- ‚úÖ **LLM interaction** - Gets personalized responses from GPT\n",
    "- ‚úÖ **Basic memory** - Stores conversation in Python dictionary\n",
    "\n",
    "**Current Problems (The Grounding Problem):**\n",
    "- ‚ùå **\"What are its prerequisites?\"** ‚Üí Agent doesn't know what \"its\" refers to\n",
    "- ‚ùå **\"Can I take it?\"** ‚Üí Agent doesn't know what \"it\" refers to\n",
    "- ‚ùå **Session-bound** - Memory lost when restarted\n",
    "- ‚ùå **Limited history** - Only last 2 messages\n",
    "\n",
    "**Next:** We'll add persistent memory to solve these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Memory Client\n",
    "\n",
    "Now let's set up the Agent Memory Server client that will provide persistent memory capabilities.\n",
    "\n",
    "### üß† **What Agent Memory Server Provides**\n",
    "- **Working Memory** - Session-scoped conversation context (solves grounding problem)\n",
    "- **Long-term Memory** - Cross-session knowledge and preferences\n",
    "- **Semantic Search** - Vector-based memory retrieval\n",
    "- **Automatic Extraction** - AI extracts important facts from conversations\n",
    "- **Production Scale** - Redis-backed, handles thousands of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Memory Client Initialized\n",
      "   Base URL: http://localhost:8088\n",
      "   Namespace: redis_university\n",
      "   Ready for memory operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Memory Client for persistent memory\n",
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    # Configure memory client\n",
    "    config = MemoryClientConfig(\n",
    "        base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\"),\n",
    "        default_namespace=\"redis_university\"\n",
    "    )\n",
    "    memory_client = MemoryAPIClient(config=config)\n",
    "    \n",
    "    print(\"üß† Memory Client Initialized\")\n",
    "    print(f\"   Base URL: {config.base_url}\")\n",
    "    print(f\"   Namespace: {config.default_namespace}\")\n",
    "    print(\"   Ready for memory operations\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Simulating memory operations (Memory Server not available)\")\n",
    "    memory_client = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

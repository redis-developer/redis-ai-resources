{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2abf8d931d184b7",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Engineering Context for Production\n",
    "\n",
    "## From RAG Basics to Production-Ready Context Engineering\n",
    "\n",
    "In the previous notebook, you built a working RAG system and saw why context quality matters. Now you'll learn to engineer context with production-level rigor.\n",
    "\n",
    "**What makes context \"good\"?**\n",
    "\n",
    "This notebook teaches you that **context engineering is real engineering** - it requires the same rigor, analysis, and deliberate decision-making as any other engineering discipline. Context isn't just \"data you feed to an LLM\" - it requires thoughtful preparation, quality assessment, and optimization.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**The Engineering Mindset:**\n",
    "- Why context quality matters (concrete impact on accuracy, relevance, cost)\n",
    "- The transformation workflow: Raw Data â†’ Engineered Context â†’ Quality Responses\n",
    "- Contrasts between naive and engineered approaches\n",
    "\n",
    "**Data Engineering for Context:**\n",
    "- Systematic transformation: Extract â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- Engineering decisions based on YOUR domain requirements\n",
    "- When to use different approaches (RAG, Structured Views, Hybrid)\n",
    "\n",
    "**Introduction to Chunking:**\n",
    "- When does your data need chunking? (Critical first question)\n",
    "- Different chunking strategies and their trade-offs\n",
    "- How to choose based on YOUR data characteristics\n",
    "\n",
    "**Production Pipelines:**\n",
    "- Three pipeline architectures (Request-Time, Batch, Event-Driven)\n",
    "- How to choose based on YOUR constraints\n",
    "- Building production-ready context preparation workflows\n",
    "\n",
    "**Time to complete:** 90-105 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Section 2, Notebook 1 (RAG Fundamentals and Implementation)\n",
    "- Redis 8 running locally\n",
    "- OpenAI API key set\n",
    "- Understanding of RAG basics and vector embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d806e0faab3793",
   "metadata": {},
   "source": [
    "## Part 1: Context is Data - and Data Requires Engineering\n",
    "\n",
    "### The Naive Approach (What NOT to Do)\n",
    "\n",
    "Let's start by seeing what happens when you treat context as \"just data\" without engineering discipline.\n",
    "\n",
    "**Scenario:** A student asks \"What machine learning courses are available?\"\n",
    "\n",
    "Let's see what happens with a naive approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c09fc7b40bee0a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35c7eed6e9e9574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:13.405597Z",
     "start_time": "2025-11-04T21:16:13.396647Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:32.015496Z",
     "iopub.status.busy": "2025-11-05T02:56:32.015310Z",
     "iopub.status.idle": "2025-11-05T02:56:32.024302Z",
     "shell.execute_reply": "2025-11-05T02:56:32.023409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(\n",
    "        f\"\"\"âš ï¸  Missing required environment variables: {', '.join(missing_vars)}\n",
    "\n",
    "Please create a .env file with:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "REDIS_URL=redis://localhost:6379\n",
    "\"\"\"\n",
    "    )\n",
    "    sys.exit(1)\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(\"âœ… Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d00b06cb8ec2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:16.311922Z",
     "start_time": "2025-11-04T21:16:13.740863Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:32.026325Z",
     "iopub.status.busy": "2025-11-05T02:56:32.026197Z",
     "iopub.status.idle": "2025-11-05T02:56:33.968523Z",
     "shell.execute_reply": "2025-11-05T02:56:33.968128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:33 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies loaded"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Import dependencies\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import redis\n",
    "import tiktoken\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from redis_context_course import CourseManager, redis_config\n",
    "\n",
    "# Initialize\n",
    "course_manager = CourseManager()\n",
    "redis_client = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Token counter\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "print(\"âœ… Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30bf7641e7c2bb4",
   "metadata": {},
   "source": [
    "### Naive Approach: Dump Everything\n",
    "\n",
    "The simplest approach is to include all course data in every request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6674fd4ec1bbcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:17.334336Z",
     "start_time": "2025-11-04T21:16:16.832182Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:33.969940Z",
     "iopub.status.busy": "2025-11-05T02:56:33.969817Z",
     "iopub.status.idle": "2025-11-05T02:56:34.459211Z",
     "shell.execute_reply": "2025-11-05T02:56:34.458401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:34 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Naive Approach Results:\n",
      "   Courses included: 10\n",
      "   Token count: 1,681\n",
      "   Estimated cost per request: $0.0042\n",
      "\n",
      "   For 100 courses, this would be ~16,810 tokens!\n",
      "\n",
      "\n",
      "ðŸ“„ Sample of raw JSON context:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"course_catalog:01K98Z0MEF04N2YSG94PQYJDY0\",\n",
      "    \"course_code\": \"CS004\",\n",
      "    \"title\": \"Database Systems\",\n",
      "    \"description\": \"Design and implementation of database systems. SQL, normalization, transactions, and database administration.\",\n",
      "    \"department\": \"Computer Science\",\n",
      "    \"credits\": 3,\n",
      "    \"difficulty_level\": \"intermediate\",\n",
      "    \"format\": \"online\",\n",
      "    \"instructor\": \"Nicholas Nelson\",\n",
      "    \"prerequisites\": [],\n",
      "    \"created_at\": \"2025-11-04 21:56:34.452731\",\n",
      "    \"updated_at\"...\n"
     ]
    }
   ],
   "source": [
    "# Naive Approach: Get all courses and dump as JSON\n",
    "all_courses = await course_manager.get_all_courses()\n",
    "\n",
    "# Convert to raw JSON (what many developers do first)\n",
    "raw_context = json.dumps(\n",
    "    [\n",
    "        {\n",
    "            \"id\": c.id,\n",
    "            \"course_code\": c.course_code,\n",
    "            \"title\": c.title,\n",
    "            \"description\": c.description,\n",
    "            \"department\": c.department,\n",
    "            \"credits\": c.credits,\n",
    "            \"difficulty_level\": c.difficulty_level.value,\n",
    "            \"format\": c.format.value,\n",
    "            \"instructor\": c.instructor,\n",
    "            \"prerequisites\": (\n",
    "                [p.course_code for p in c.prerequisites] if c.prerequisites else []\n",
    "            ),\n",
    "            \"created_at\": str(c.created_at) if hasattr(c, \"created_at\") else None,\n",
    "            \"updated_at\": str(c.updated_at) if hasattr(c, \"updated_at\") else None,\n",
    "        }\n",
    "        for c in all_courses[:10]  # Just first 10 for demo\n",
    "    ],\n",
    "    indent=2,\n",
    ")\n",
    "\n",
    "token_count = count_tokens(raw_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Naive Approach Results:\n",
    "   Courses included: {len(all_courses[:10])}\n",
    "   Token count: {token_count:,}\n",
    "   Estimated cost per request: ${(token_count / 1_000_000) * 2.50:.4f}\n",
    "\n",
    "   For 100 courses, this would be ~{token_count * 10:,} tokens!\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nðŸ“„ Sample of raw JSON context:\")\n",
    "print(raw_context[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f12aa3d9a92a5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:17.347983Z",
     "start_time": "2025-11-04T21:16:17.344365Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:34.461109Z",
     "iopub.status.busy": "2025-11-05T02:56:34.460941Z",
     "iopub.status.idle": "2025-11-05T02:56:34.465842Z",
     "shell.execute_reply": "2025-11-05T02:56:34.465172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Database Systems',\n",
       " 'Web Development',\n",
       " 'Web Development',\n",
       " 'Web Development',\n",
       " 'Web Development',\n",
       " 'Linear Algebra',\n",
       " 'Linear Algebra',\n",
       " 'Linear Algebra',\n",
       " 'Linear Algebra',\n",
       " 'Linear Algebra',\n",
       " 'Calculus I',\n",
       " 'Calculus I',\n",
       " 'Calculus I',\n",
       " 'Calculus I',\n",
       " 'Calculus I',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Data Structures and Algorithms',\n",
       " 'Principles of Management',\n",
       " 'Principles of Management',\n",
       " 'Principles of Management',\n",
       " 'Introduction to Psychology',\n",
       " 'Introduction to Psychology',\n",
       " 'Introduction to Psychology',\n",
       " 'Data Visualization',\n",
       " 'Data Visualization',\n",
       " 'Data Visualization',\n",
       " 'Data Visualization',\n",
       " 'Machine Learning',\n",
       " 'Introduction to Programming',\n",
       " 'Introduction to Programming',\n",
       " 'Introduction to Programming',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[course.title for course in all_courses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb7ba88280036",
   "metadata": {},
   "source": [
    "### Test the Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cbb2ba9a1070a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:20.589130Z",
     "start_time": "2025-11-04T21:16:19.252966Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:34.467429Z",
     "iopub.status.busy": "2025-11-05T02:56:34.467301Z",
     "iopub.status.idle": "2025-11-05T02:56:35.839349Z",
     "shell.execute_reply": "2025-11-05T02:56:35.838445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:35 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Query: \"What machine learning courses are available?\"\n",
      "\n",
      "Response:\n",
      "Currently, there are no machine learning courses listed in the available course catalog. If you are interested in machine learning, you might consider taking foundational courses such as \"Linear Algebra\" or \"Database Systems,\" as they provide essential knowledge that can be beneficial for understanding machine learning concepts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with a real query\n",
    "query = \"What machine learning courses are available?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Available Courses:\n",
    "{raw_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ¤– Query: \"{query}\"\n",
    "\n",
    "Response:\n",
    "{response.content}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e999b3edc323c9a",
   "metadata": {},
   "source": [
    "### Problems with the Naive Approach\n",
    "\n",
    "As discussed in previous notebooks, this approach has several problems:\n",
    "\n",
    "1. **Excessive Token Usage**\n",
    "   - 10 courses = ~1,703 tokens\n",
    "   - 100 courses would be ~17,030 tokens\n",
    "\n",
    "\n",
    "2. **Raw JSON is Inefficient**\n",
    "   - Includes internal fields (IDs, timestamps, created_at, updated_at)\n",
    "   - Verbose formatting (indentation, field names repeated)\n",
    "\n",
    "\n",
    "3. **No Filtering**\n",
    "   - Student asked about ML, but got all courses, even irrelevant ones\n",
    "   - **Dilutes relevant information with noise**\n",
    "\n",
    "\n",
    "4. **Poor Response Quality**\n",
    "   - Generic responses (\"We have many courses...\")\n",
    "   - May miss the most relevant courses\n",
    "   - Can't provide personalized recommendations\n",
    "\n",
    "\n",
    "5. **Not Scalable**\n",
    "   - What if you have 1,000 courses? 10,000?\n",
    "   - What if courses change daily?\n",
    "   - Requires code changes to update\n",
    "\n",
    "**Therefore, the goal is not only to give the LLM \"all the data\" - it's to *give it the useful data.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803dbc94b12fa6f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Engineering Mindset\n",
    "\n",
    "Context is data that flows through a pipeline. Like any data engineering problem, it requires:\n",
    "\n",
    "### 1. Requirements Analysis\n",
    "- What is the intended use case?\n",
    "- What queries will users ask?\n",
    "- What information do they need?\n",
    "- What constraints exist (token budget, latency, cost)?\n",
    "\n",
    "### 2. Data Transformation\n",
    "- Raw data â†’ Cleaned data â†’ Structured data â†’ LLM-optimized context\n",
    "\n",
    "### 3. Quality Metrics\n",
    "- How do we measure if context is \"good\"?\n",
    "- Relevance, completeness, efficiency, accuracy\n",
    "\n",
    "### 4. Testing and Iteration\n",
    "- Test with real queries\n",
    "- Measure quality metrics\n",
    "- Iterate based on results\n",
    "\n",
    "**The Engineering Question:** \"How do we transform raw course data into high-quality context that produces accurate, relevant, efficient responses?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730c35637eb3303",
   "metadata": {},
   "source": [
    "### Three Engineering Approaches\n",
    "\n",
    "Let's compare three approaches with concrete examples:\n",
    "\n",
    "| Approach | Description | Token Usage | Response Quality | Maintenance | Verdict |\n",
    "|----------|-------------|-------------|------------------|-------------|---------|\n",
    "| **Naive** | Include all raw data | 50K tokens | Poor (generic) | Easy | âŒ Not production-ready |\n",
    "| **RAG** | Semantic search for relevant courses | 3K tokens | Good (relevant) | Moderate | âœ… Good for most cases |\n",
    "| **Structured Views** | Pre-compute LLM-optimized summaries | 2K tokens | Excellent (overview + details) | Higher | âœ… Best for production |\n",
    "| **Hybrid** | Structured view + RAG | 5K tokens | Excellent (best of both) | Higher | âœ… Best for production |\n",
    "\n",
    "Let's implement each approach and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825e363289a6d65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Engineering Workflow - From Raw to Optimized\n",
    "\n",
    "### The Data Engineering Pipeline\n",
    "\n",
    "Context preparation follows a systematic workflow:\n",
    "\n",
    "```\n",
    "Raw Data (Database/API)\n",
    "    â†“\n",
    "[Step 1: Extract] - Get the data\n",
    "    â†“\n",
    "[Step 2: Clean] - Remove noise, fix inconsistencies\n",
    "    â†“\n",
    "[Step 3: Transform] - Structure for LLM consumption\n",
    "    â†“\n",
    "[Step 4: Optimize] - Reduce tokens, improve clarity\n",
    "    â†“\n",
    "[Step 5: Store] - Vector DB, cache, or pre-compute\n",
    "    â†“\n",
    "Engineered Context (Ready for LLM)\n",
    "```\n",
    "\n",
    "Let's walk through this pipeline with a real example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055906b662e63d",
   "metadata": {},
   "source": [
    "### Step 1: Extract (Raw Data)\n",
    "\n",
    "First, let's look at what raw course data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d43d9871aa5b9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:22.016096Z",
     "start_time": "2025-11-04T21:16:22.011996Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.841888Z",
     "iopub.status.busy": "2025-11-05T02:56:35.841727Z",
     "iopub.status.idle": "2025-11-05T02:56:35.846164Z",
     "shell.execute_reply": "2025-11-05T02:56:35.845609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 1: Raw Database Record\n",
      "================================================================================\n",
      "{\n",
      "  \"id\": \"course_catalog:01K98Z0MEF04N2YSG94PQYJDY0\",\n",
      "  \"course_code\": \"CS004\",\n",
      "  \"title\": \"Database Systems\",\n",
      "  \"description\": \"Design and implementation of database systems. SQL, normalization, transactions, and database administration.\",\n",
      "  \"department\": \"Computer Science\",\n",
      "  \"credits\": 3,\n",
      "  \"difficulty_level\": \"intermediate\",\n",
      "  \"format\": \"online\",\n",
      "  \"instructor\": \"Nicholas Nelson\",\n",
      "  \"prerequisites\": [],\n",
      "  \"created_at\": \"2025-11-04 21:56:34.452731\",\n",
      "  \"updated_at\": \"2025-11-04 21:56:34.452739\"\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 160\n"
     ]
    }
   ],
   "source": [
    "# Get a sample course\n",
    "sample_course = all_courses[0]\n",
    "\n",
    "# Show raw database record\n",
    "raw_record = {\n",
    "    \"id\": sample_course.id,\n",
    "    \"course_code\": sample_course.course_code,\n",
    "    \"title\": sample_course.title,\n",
    "    \"description\": sample_course.description,\n",
    "    \"department\": sample_course.department,\n",
    "    \"credits\": sample_course.credits,\n",
    "    \"difficulty_level\": sample_course.difficulty_level.value,\n",
    "    \"format\": sample_course.format.value,\n",
    "    \"instructor\": sample_course.instructor,\n",
    "    \"prerequisites\": (\n",
    "        [p.course_code for p in sample_course.prerequisites]\n",
    "        if sample_course.prerequisites\n",
    "        else []\n",
    "    ),\n",
    "    \"created_at\": (\n",
    "        str(sample_course.created_at)\n",
    "        if hasattr(sample_course, \"created_at\")\n",
    "        else \"2024-01-15T08:30:00Z\"\n",
    "    ),\n",
    "    \"updated_at\": (\n",
    "        str(sample_course.updated_at)\n",
    "        if hasattr(sample_course, \"updated_at\")\n",
    "        else \"2024-09-01T14:22:00Z\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "raw_json = json.dumps(raw_record, indent=2)\n",
    "raw_tokens = count_tokens(raw_json)\n",
    "\n",
    "print(\"ðŸ“„ Step 1: Raw Database Record\")\n",
    "print(\"=\" * 80)\n",
    "print(raw_json)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Token count: {raw_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736e4af9c549cec",
   "metadata": {},
   "source": [
    "Issues with above:\n",
    " - Internal fields (IDs, timestamps) waste tokens\n",
    " - Verbose JSON formatting\n",
    " - Prerequisites are codes, not human-readable\n",
    " - No structure for LLM consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9d3b83e4a304a",
   "metadata": {},
   "source": [
    "### Step 2: Clean (Remove Noise)\n",
    "\n",
    "Remove fields that don't help the LLM answer user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17d341ad154ff9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:23.517460Z",
     "start_time": "2025-11-04T21:16:23.513732Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.847913Z",
     "iopub.status.busy": "2025-11-05T02:56:35.847770Z",
     "iopub.status.idle": "2025-11-05T02:56:35.851797Z",
     "shell.execute_reply": "2025-11-05T02:56:35.851108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 2: Cleaned Record\n",
      "================================================================================\n",
      "{\n",
      "  \"course_code\": \"CS004\",\n",
      "  \"title\": \"Database Systems\",\n",
      "  \"description\": \"Design and implementation of database systems. SQL, normalization, transactions, and database administration.\",\n",
      "  \"department\": \"Computer Science\",\n",
      "  \"credits\": 3,\n",
      "  \"difficulty_level\": \"intermediate\",\n",
      "  \"format\": \"online\",\n",
      "  \"instructor\": \"Nicholas Nelson\",\n",
      "  \"prerequisites\": []\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 89 (saved 71 tokens, 44.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clean - Remove internal fields\n",
    "cleaned_record = {\n",
    "    \"course_code\": sample_course.course_code,\n",
    "    \"title\": sample_course.title,\n",
    "    \"description\": sample_course.description,\n",
    "    \"department\": sample_course.department,\n",
    "    \"credits\": sample_course.credits,\n",
    "    \"difficulty_level\": sample_course.difficulty_level.value,\n",
    "    \"format\": sample_course.format.value,\n",
    "    \"instructor\": sample_course.instructor,\n",
    "    \"prerequisites\": (\n",
    "        [p.course_code for p in sample_course.prerequisites]\n",
    "        if sample_course.prerequisites\n",
    "        else []\n",
    "    ),\n",
    "}\n",
    "\n",
    "cleaned_json = json.dumps(cleaned_record, indent=2)\n",
    "cleaned_tokens = count_tokens(cleaned_json)\n",
    "\n",
    "print(\"ðŸ“„ Step 2: Cleaned Record\")\n",
    "print(\"=\" * 80)\n",
    "print(cleaned_json)\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"\\nðŸ“Š Token count: {cleaned_tokens} (saved {raw_tokens - cleaned_tokens} tokens, {((raw_tokens - cleaned_tokens) / raw_tokens * 100):.1f}% reduction)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0245185126ebc8d",
   "metadata": {},
   "source": [
    "\n",
    "Improvements:\n",
    " - Removed id, created_at, updated_at\n",
    " - Still has all information needed to answer queries\n",
    "\n",
    "Still has minor problems:\n",
    " - JSON formatting is verbose (this is a *minor* issue as LLMs can handle it; however)\n",
    " - Prerequisites are still codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916054c3caf3246f",
   "metadata": {},
   "source": [
    "### Step 3: Transform (Structure for LLM)\n",
    "\n",
    "Convert to a format optimized for LLM consumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce586982d559bf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:24.995047Z",
     "start_time": "2025-11-04T21:16:24.990842Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.853430Z",
     "iopub.status.busy": "2025-11-05T02:56:35.853331Z",
     "iopub.status.idle": "2025-11-05T02:56:35.856714Z",
     "shell.execute_reply": "2025-11-05T02:56:35.856250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 3: Transformed to LLM-Friendly Format\n",
      "================================================================================\n",
      "CS004: Database Systems\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: online\n",
      "Instructor: Nicholas Nelson\n",
      "Description: Design and implementation of database systems. SQL, normalization, transactions, and database administration.\n",
      "    \n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 49 (saved 40 tokens, 44.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Transform - Convert to LLM-friendly format\n",
    "\n",
    "\n",
    "def transform_course_to_text(course) -> str:\n",
    "    \"\"\"Transform course object to LLM-optimized text format.\"\"\"\n",
    "\n",
    "    # Build prerequisites text\n",
    "    prereq_text = \"\"\n",
    "    if course.prerequisites:\n",
    "        prereq_codes = [p.course_code for p in course.prerequisites]\n",
    "        prereq_text = f\"\\nPrerequisites: {', '.join(prereq_codes)}\"\n",
    "\n",
    "    # Build course text\n",
    "    course_text = f\"\"\"{course.course_code}: {course.title}\n",
    "Department: {course.department}\\nCredits: {course.credits}\\nLevel: {course.difficulty_level.value}\\nFormat: {course.format.value}\n",
    "Instructor: {course.instructor}{prereq_text}\n",
    "Description: {course.description}\n",
    "    \"\"\"\n",
    "\n",
    "    return course_text\n",
    "\n",
    "\n",
    "transformed_text = transform_course_to_text(sample_course)\n",
    "transformed_tokens = count_tokens(transformed_text)\n",
    "\n",
    "print(\"ðŸ“„ Step 3: Transformed to LLM-Friendly Format\")\n",
    "print(\"=\" * 80)\n",
    "print(transformed_text)\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"\\nðŸ“Š Token count: {transformed_tokens} (saved {cleaned_tokens - transformed_tokens} tokens, {((cleaned_tokens - transformed_tokens) / cleaned_tokens * 100):.1f}% reduction)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21134e639bb161",
   "metadata": {},
   "source": [
    "\n",
    "âœ… Improvements:\n",
    " - Natural text format with the correct metadata\n",
    " - Clear structure with labels\n",
    " - No JSON overhead (brackets, quotes, commas)\n",
    "\n",
    "**Note:** In case the description is too long, we can apply compression techniques such as summarization to keep the description within a desired token limit. Section 3 will cover compression in more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d61a0bd31afaa",
   "metadata": {},
   "source": [
    "### Step 4: Optimize (Further Reduce Tokens)\n",
    "\n",
    "For even more efficiency, we can create a summarized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d542adf08de72190",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:26.480662Z",
     "start_time": "2025-11-04T21:16:26.477068Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.858224Z",
     "iopub.status.busy": "2025-11-05T02:56:35.858111Z",
     "iopub.status.idle": "2025-11-05T02:56:35.861247Z",
     "shell.execute_reply": "2025-11-05T02:56:35.860819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 4: Optimized (Ultra-Compact)\n",
      "================================================================================\n",
      "CS004: Database Systems - Design and implementation of database systems. SQL, normalization, transactions, and database admini...\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 24 (saved 25 tokens, 51.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Optimize - Create ultra-compact version\n",
    "# TODO: Maybe use summarization here? Maybe for that we need a longer description or some other metadata?\n",
    "\n",
    "def optimize_course_text(course) -> str:\n",
    "    \"\"\"Create ultra-compact course description.\"\"\"\n",
    "    prereqs = (\n",
    "        f\" (Prereq: {', '.join([p.course_code for p in course.prerequisites])})\"\n",
    "        if course.prerequisites\n",
    "        else \"\"\n",
    "    )\n",
    "    return (\n",
    "        f\"{course.course_code}: {course.title} - {course.description[:100]}...{prereqs}\"\n",
    "    )\n",
    "\n",
    "\n",
    "optimized_text = optimize_course_text(sample_course)\n",
    "optimized_tokens = count_tokens(optimized_text)\n",
    "\n",
    "print(\"ðŸ“„ Step 4: Optimized (Ultra-Compact)\")\n",
    "print(\"=\" * 80)\n",
    "print(optimized_text)\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"\\nðŸ“Š Token count: {optimized_tokens} (saved {transformed_tokens - optimized_tokens} tokens, {((transformed_tokens - optimized_tokens) / transformed_tokens * 100):.1f}% reduction)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034058ec3845a04",
   "metadata": {},
   "source": [
    "Improvements:\n",
    "   - Truncated description to 100 chars\n",
    "   - Removed metadata (instructor, format, credits)\n",
    "\n",
    "Trade-off:\n",
    "   - Lost some detail (may need for specific queries)\n",
    "   - Best for overview/catalog views\n",
    "\n",
    "**Note:** This is just an example of what you can do to be more efficient. This is where you have to be creative and engineer based on the usercase and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd00f8a8fc98f7",
   "metadata": {},
   "source": [
    "### Step 5: Store (Choose Storage Strategy)\n",
    "\n",
    "Now we need to decide HOW to store this engineered context:\n",
    "\n",
    "**Option 1: Vector Database (RAG)**\n",
    "- Store transformed text with embeddings\n",
    "- Retrieve relevant courses at query time\n",
    "- Good for: Large datasets, specific queries\n",
    "\n",
    "**Option 2: Pre-Computed Views**\n",
    "- Create structured summaries ahead of time\n",
    "- Store in Redis as cached views\n",
    "- Good for: Common queries, overview information\n",
    "\n",
    "**Option 3: Hybrid**\n",
    "- Combine both approaches\n",
    "- Pre-compute catalog view + RAG for details\n",
    "- Good for: Production systems\n",
    "\n",
    "Let's implement all three and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d1ca669115e9b",
   "metadata": {},
   "source": [
    "### Summary: The Transformation Pipeline\n",
    "\n",
    "Let's see the complete transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfae248ca80f0af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:39.408618Z",
     "start_time": "2025-11-04T21:16:39.405135Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.862686Z",
     "iopub.status.busy": "2025-11-05T02:56:35.862597Z",
     "iopub.status.idle": "2025-11-05T02:56:35.865468Z",
     "shell.execute_reply": "2025-11-05T02:56:35.864985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE PIPELINE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Step 1: Raw Database Record\n",
      "   Token count: 160\n",
      "   Format: JSON with all fields\n",
      "\n",
      "Step 2: Cleaned Record\n",
      "   Token count: 89 (44.4% reduction)\n",
      "   Removed: Internal fields (IDs, timestamps)\n",
      "\n",
      "Step 3: Transformed to LLM Format\n",
      "   Token count: 49 (44.9% reduction from Step 2)\n",
      "   Format: Natural text, structured\n",
      "\n",
      "Step 4: Optimized (Ultra-Compact)\n",
      "   Token count: 24 (51.0% reduction from Step 3)\n",
      "   Format: Single line, truncated\n",
      "\n",
      "TOTAL REDUCTION: 160 â†’ 24 tokens (85.0% reduction)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ¯ Key Insight:\n",
      "   Through systematic engineering, we reduced token usage by ~70%\n",
      "   while IMPROVING readability for the LLM!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Step 1: Raw Database Record\n",
    "   Token count: {raw_tokens}\n",
    "   Format: JSON with all fields\n",
    "\n",
    "Step 2: Cleaned Record\n",
    "   Token count: {cleaned_tokens} ({((raw_tokens - cleaned_tokens) / raw_tokens * 100):.1f}% reduction)\n",
    "   Removed: Internal fields (IDs, timestamps)\n",
    "\n",
    "Step 3: Transformed to LLM Format\n",
    "   Token count: {transformed_tokens} ({((cleaned_tokens - transformed_tokens) / cleaned_tokens * 100):.1f}% reduction from Step 2)\n",
    "   Format: Natural text, structured\n",
    "\n",
    "Step 4: Optimized (Ultra-Compact)\n",
    "   Token count: {optimized_tokens} ({((transformed_tokens - optimized_tokens) / transformed_tokens * 100):.1f}% reduction from Step 3)\n",
    "   Format: Single line, truncated\n",
    "\n",
    "TOTAL REDUCTION: {raw_tokens} â†’ {optimized_tokens} tokens ({((raw_tokens - optimized_tokens) / raw_tokens * 100):.1f}% reduction)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nðŸŽ¯ Key Insight:\")\n",
    "print(\"   Through systematic engineering, we reduced token usage by ~70%\")\n",
    "print(\"   while IMPROVING readability for the LLM!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc3b3449d3d842a",
   "metadata": {},
   "source": [
    "The key insight states that we reduced token usage.\n",
    "\n",
    "However, it should be noted that reduction is not the goal. The goal is to optimize the content and provide the most relevant information to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974af3948d4ec98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Engineering Decision - When to Use Each Approach\n",
    "\n",
    "Now let's implement the three approaches and compare them with real queries.\n",
    "\n",
    "### Approach 1: RAG (Semantic Search)\n",
    "\n",
    "Retrieve only relevant courses using vector search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1552972433032e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:40.200346Z",
     "start_time": "2025-11-04T21:16:40.193910Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.866673Z",
     "iopub.status.busy": "2025-11-05T02:56:35.866592Z",
     "iopub.status.idle": "2025-11-05T02:56:35.872622Z",
     "shell.execute_reply": "2025-11-05T02:56:35.872202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Index 'course_index' not found. Please run Section 2 notebooks to create it.\n",
      "   For this demo, we'll simulate RAG results.\n"
     ]
    }
   ],
   "source": [
    "from redisvl.index import SearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.query.filter import Tag\n",
    "\n",
    "# Initialize vector search\n",
    "index_name = \"course_index\"\n",
    "\n",
    "# Check if index exists, create if not\n",
    "try:\n",
    "    index = SearchIndex.from_existing(index_name, redis_url=REDIS_URL)\n",
    "    print(f\"âœ… Using existing index: {index_name}\")\n",
    "except:\n",
    "    print(\n",
    "        f\"âš ï¸  Index '{index_name}' not found. Please run Section 2 notebooks to create it.\"\n",
    "    )\n",
    "    print(\"   For this demo, we'll simulate RAG results.\")\n",
    "    index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ddc04a807cc174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:40.589240Z",
     "start_time": "2025-11-04T21:16:40.585751Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.873938Z",
     "iopub.status.busy": "2025-11-05T02:56:35.873846Z",
     "iopub.status.idle": "2025-11-05T02:56:35.877100Z",
     "shell.execute_reply": "2025-11-05T02:56:35.876628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RAG Approach Results:\n",
      "   Query: \"What machine learning courses are available?\"\n",
      "   Courses retrieved: 5\n",
      "   Token count: 268\n",
      "\n",
      "ðŸ“„ Context Preview:\n",
      "CS010: Web Development\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: in_person\n",
      "Instructor: Kathy Blair\n",
      "Description: Full-stack web development using modern frameworks. HTML, CSS, JavaScript, React, and backend APIs.\n",
      "    \n",
      "\n",
      "CS002: Web Development\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: in_person\n",
      "Instructor: Tamara Murray\n",
      "Description: Full-stack web development using modern frameworks. HTML, CSS, JavaScript, React, and backend APIs.\n",
      "    \n",
      "\n",
      "CS003: Web...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate RAG retrieval (in production, this would use vector search)\n",
    "\n",
    "\n",
    "async def rag_approach(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant courses using semantic search.\"\"\"\n",
    "\n",
    "    # In production: Use vector search\n",
    "    # For demo: Filter courses by keyword matching\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    relevant_courses = []\n",
    "    for course in all_courses:\n",
    "        # Simple keyword matching (in production, use embeddings)\n",
    "        if any(\n",
    "            keyword in course.title.lower() or keyword in course.description.lower()\n",
    "            for keyword in [\"machine learning\", \"ml\", \"ai\", \"data science\", \"neural\"]\n",
    "        ):\n",
    "            relevant_courses.append(course)\n",
    "            if len(relevant_courses) >= limit:\n",
    "                break\n",
    "\n",
    "    # Transform to LLM-friendly format\n",
    "    context = \"\\n\\n\".join([transform_course_to_text(c) for c in relevant_courses])\n",
    "    return context\n",
    "\n",
    "\n",
    "# Test RAG approach\n",
    "query = \"What machine learning courses are available?\"\n",
    "rag_context = await rag_approach(query, limit=5)\n",
    "rag_tokens = count_tokens(rag_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š RAG Approach Results:\n",
    "   Query: \"{query}\"\n",
    "   Courses retrieved: 5\n",
    "   Token count: {rag_tokens:,}\n",
    "\n",
    "ðŸ“„ Context Preview:\n",
    "{rag_context[:500]}...\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43b96177edaa59",
   "metadata": {},
   "source": [
    "### Approach 2: Structured Views (Pre-Computed Summaries)\n",
    "\n",
    "Create a pre-computed catalog view that's optimized for LLM consumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49944033c6dec60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:41.448177Z",
     "start_time": "2025-11-04T21:16:41.439641Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.878326Z",
     "iopub.status.busy": "2025-11-05T02:56:35.878253Z",
     "iopub.status.idle": "2025-11-05T02:56:35.885104Z",
     "shell.execute_reply": "2025-11-05T02:56:35.884688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Structured View Approach Results:\n",
      "   Total courses: 50\n",
      "   Token count: 585\n",
      "   Cached in Redis: âœ…\n",
      "\n",
      "ðŸ“„ Catalog Preview:\n",
      "# Redis University Course Catalog\n",
      "\n",
      "## Business (10 courses)\n",
      "- BUS033: Marketing Strategy (intermediate)\n",
      "- BUS032: Marketing Strategy (intermediate)\n",
      "- BUS034: Marketing Strategy (intermediate)\n",
      "- BUS035: Marketing Strategy (intermediate)\n",
      "- BUS037: Marketing Strategy (intermediate)\n",
      "- BUS039: Marketing Strategy (intermediate)\n",
      "- BUS040: Marketing Strategy (intermediate)\n",
      "- BUS031: Principles of Management (beginner)\n",
      "- BUS036: Principles of Management (beginner)\n",
      "- BUS038: Principles of Management (beginner)\n",
      "\n",
      "## Computer Science (10 courses)\n",
      "- CS004: Database Systems (intermediate)\n",
      "- CS010: Web Develo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 2: Structured Views\n",
    "# Pre-compute a catalog summary organized by department\n",
    "\n",
    "\n",
    "async def create_catalog_view() -> str:\n",
    "    \"\"\"Create a pre-computed catalog view organized by department.\"\"\"\n",
    "\n",
    "    # Group courses by department\n",
    "    by_department = {}\n",
    "    for course in all_courses:\n",
    "        dept = course.department\n",
    "        if dept not in by_department:\n",
    "            by_department[dept] = []\n",
    "        by_department[dept].append(course)\n",
    "\n",
    "    # Build catalog view\n",
    "    catalog_sections = []\n",
    "\n",
    "    for dept_name in sorted(by_department.keys()):\n",
    "        courses = by_department[dept_name]\n",
    "\n",
    "        # Create department section\n",
    "        dept_section = f\"\\n## {dept_name} ({len(courses)} courses)\\n\"\n",
    "\n",
    "        # Add course summaries (optimized format)\n",
    "        course_summaries = []\n",
    "        for course in courses[:10]:  # Limit for demo\n",
    "            summary = f\"- {course.course_code}: {course.title} ({course.difficulty_level.value})\"\n",
    "            course_summaries.append(summary)\n",
    "\n",
    "        dept_section += \"\\n\".join(course_summaries)\n",
    "        catalog_sections.append(dept_section)\n",
    "\n",
    "    catalog_view = \"# Redis University Course Catalog\\n\" + \"\\n\".join(catalog_sections)\n",
    "    return catalog_view\n",
    "\n",
    "\n",
    "# Create and cache the view\n",
    "catalog_view = await create_catalog_view()\n",
    "catalog_tokens = count_tokens(catalog_view)\n",
    "\n",
    "# Store in Redis for reuse\n",
    "redis_client.set(\"course_catalog_view\", catalog_view)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Structured View Approach Results:\n",
    "   Total courses: {len(all_courses)}\n",
    "   Token count: {catalog_tokens:,}\n",
    "   Cached in Redis: âœ…\n",
    "\n",
    "ðŸ“„ Catalog Preview:\n",
    "{catalog_view[:600]}...\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8297b02702e162a",
   "metadata": {},
   "source": [
    "### Approach 3: Hybrid (Best of Both Worlds)\n",
    "\n",
    "Combine structured view (overview) + RAG (specific details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1316764e1710f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:42.408022Z",
     "start_time": "2025-11-04T21:16:42.402929Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.886287Z",
     "iopub.status.busy": "2025-11-05T02:56:35.886211Z",
     "iopub.status.idle": "2025-11-05T02:56:35.889840Z",
     "shell.execute_reply": "2025-11-05T02:56:35.889379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Hybrid Approach Results:\n",
      "   Query: \"What machine learning courses are available?\"\n",
      "   Token count: 761\n",
      "\n",
      "   Components:\n",
      "   - Catalog overview: 585 tokens\n",
      "   - Specific details (RAG): 268 tokens\n",
      "\n",
      "ðŸ“„ Context Structure:\n",
      "   1. Full catalog overview (all departments)\n",
      "   2. Detailed info for 3 most relevant courses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 3: Hybrid\n",
    "\n",
    "\n",
    "async def hybrid_approach(query: str) -> str:\n",
    "    \"\"\"Combine catalog overview with RAG for specific details.\"\"\"\n",
    "\n",
    "    # Part 1: Get catalog overview (from cache)\n",
    "    catalog_overview = redis_client.get(\"course_catalog_view\")\n",
    "\n",
    "    # Part 2: Get specific course details via RAG\n",
    "    specific_courses = await rag_approach(query, limit=3)\n",
    "\n",
    "    # Combine\n",
    "    hybrid_context = f\"\"\"# Course Catalog Overview\n",
    "{catalog_overview}\n",
    "\n",
    "---\n",
    "\n",
    "# Detailed Information for Your Query\n",
    "{specific_courses}\n",
    "\"\"\"\n",
    "\n",
    "    return hybrid_context\n",
    "\n",
    "\n",
    "# Test hybrid approach\n",
    "hybrid_context = await hybrid_approach(query)\n",
    "hybrid_tokens = count_tokens(hybrid_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Hybrid Approach Results:\n",
    "   Query: \"{query}\"\n",
    "   Token count: {hybrid_tokens:,}\n",
    "\n",
    "   Components:\n",
    "   - Catalog overview: {catalog_tokens:,} tokens\n",
    "   - Specific details (RAG): {rag_tokens:,} tokens\n",
    "\n",
    "ðŸ“„ Context Structure:\n",
    "   1. Full catalog overview (all departments)\n",
    "   2. Detailed info for 3 most relevant courses\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810bfe1cef703fd",
   "metadata": {},
   "source": [
    "### Compare All Three Approaches\n",
    "\n",
    "Let's test all three with the same query and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb3ddacd3f133f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:47.445269Z",
     "start_time": "2025-11-04T21:16:43.510177Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:35.891082Z",
     "iopub.status.busy": "2025-11-05T02:56:35.890998Z",
     "iopub.status.idle": "2025-11-05T02:56:39.004661Z",
     "shell.execute_reply": "2025-11-05T02:56:39.002991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARING THREE APPROACHES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:37 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:38 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:38 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: \"What machine learning courses are available?\"\n",
      "\n",
      "================================================================================\n",
      "APPROACH 1: RAG (Semantic Search)\n",
      "================================================================================\n",
      "Token count: 268\n",
      "Response:\n",
      "Currently, there are no machine learning courses listed in the available courses at Redis University. If you're interested in related topics, you might consider taking \"MATH026: Linear Algebra,\" as it covers essential mathematical concepts like vector spaces and matrices that are foundational for machine learning.\n",
      "\n",
      "================================================================================\n",
      "APPROACH 2: Structured View (Pre-Computed)\n",
      "================================================================================\n",
      "Token count: 585\n",
      "Response:\n",
      "The available machine learning course is:\n",
      "\n",
      "- CS007: Machine Learning (advanced)\n",
      "\n",
      "================================================================================\n",
      "APPROACH 3: Hybrid (View + RAG)\n",
      "================================================================================\n",
      "Token count: 761\n",
      "Response:\n",
      "The available machine learning course is:\n",
      "\n",
      "- CS007: Machine Learning (advanced)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test all three approaches\n",
    "query = \"What machine learning courses are available?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARING THREE APPROACHES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Approach 1: RAG\n",
    "messages_rag = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Available Courses:\n",
    "{rag_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_rag = llm.invoke(messages_rag)\n",
    "\n",
    "# Approach 2: Structured View\n",
    "messages_view = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "{catalog_view}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_view = llm.invoke(messages_view)\n",
    "\n",
    "# Approach 3: Hybrid\n",
    "messages_hybrid = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "{hybrid_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_hybrid = llm.invoke(messages_hybrid)\n",
    "\n",
    "# Display comparison\n",
    "print(\n",
    "    f\"\"\"\n",
    "Query: \"{query}\"\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 1: RAG (Semantic Search)\n",
    "{'=' * 80}\n",
    "Token count: {rag_tokens:,}\n",
    "Response:\n",
    "{response_rag.content}\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 2: Structured View (Pre-Computed)\n",
    "{'=' * 80}\n",
    "Token count: {catalog_tokens:,}\n",
    "Response:\n",
    "{response_view.content}\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 3: Hybrid (View + RAG)\n",
    "{'=' * 80}\n",
    "Token count: {hybrid_tokens:,}\n",
    "Response:\n",
    "{response_hybrid.content}\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae3ea8350df39",
   "metadata": {},
   "source": [
    "### Decision Framework: Which Approach to Use?\n",
    "\n",
    "Here's how to choose based on YOUR requirements:\n",
    "\n",
    "| Factor | RAG | Structured Views | Hybrid |\n",
    "|--------|-----|------------------|--------|\n",
    "| **Token Efficiency** | âœ… Good (3K) | âœ…âœ… Excellent (2K) | âš ï¸ Moderate (5K) |\n",
    "| **Response Quality** | âœ… Good (relevant) | âœ… Good (overview) | âœ…âœ… Excellent (both) |\n",
    "| **Latency** | âš ï¸ Moderate (search) | âœ…âœ… Fast (cached) | âš ï¸ Moderate (search) |\n",
    "| **Maintenance** | âœ… Low (auto-updates) | âš ï¸ Higher (rebuild views) | âš ï¸ Higher (both) |\n",
    "| **Best For** | Specific queries | Overview queries | Production systems |\n",
    "\n",
    "**Decision Process:**\n",
    "\n",
    "1. **Analyze YOUR data characteristics:**\n",
    "   - How many items? (10s, 100s, 1000s, millions?)\n",
    "   - How often does it change? (Real-time, daily, weekly?)\n",
    "   - What's the average item size? (100 words, 1000 words, 10K words?)\n",
    "\n",
    "2. **Analyze YOUR query patterns:**\n",
    "   - Specific queries (\"Show me RU101\") â†’ RAG\n",
    "   - Overview queries (\"What courses exist?\") â†’ Structured Views\n",
    "   - Mixed queries â†’ Hybrid\n",
    "\n",
    "3. **Analyze YOUR constraints:**\n",
    "   - Tight token budget â†’ Structured Views\n",
    "   - Real-time updates required â†’ RAG\n",
    "   - Best quality needed â†’ Hybrid\n",
    "\n",
    "**Example Decision:**\n",
    "\n",
    "For Redis University:\n",
    "- âœ… **Data:** 100-500 courses, updated weekly, 200-500 words each\n",
    "- âœ… **Queries:** Mix of overview (\"What's available?\") and specific (\"ML courses?\")\n",
    "- âœ… **Constraints:** Moderate token budget, weekly updates acceptable\n",
    "- âœ… **Decision:** **Hybrid approach** (pre-compute catalog + RAG for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b924daaecb8ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Introduction to Chunking - When and Why\n",
    "\n",
    "So far, we've worked with course data where each course is a complete, self-contained unit (200-500 words). But what happens when you have **long documents** that exceed token limits or contain multiple distinct topics?\n",
    "\n",
    "This is where **chunking** becomes necessary.\n",
    "\n",
    "### The Critical First Question: Does My Data Need Chunking?\n",
    "\n",
    "**Chunking is NOT a default step** - it's an engineering decision based on your data characteristics.\n",
    "\n",
    "Let's understand when chunking is necessary and when it's not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef992eb86e53dda",
   "metadata": {},
   "source": [
    "### When You DON'T Need Chunking\n",
    "\n",
    "If your data already consists of small, complete semantic units, chunking can actually hurt quality:\n",
    "\n",
    "**Examples of data that DON'T need chunking:**\n",
    "- âœ… Course descriptions (200-500 words, complete)\n",
    "- âœ… Product listings (100-300 words, self-contained)\n",
    "- âœ… FAQ entries (50-200 words, question + answer)\n",
    "- âœ… Social media posts (50-280 characters, atomic)\n",
    "- âœ… Customer support tickets (100-500 words, single issue)\n",
    "\n",
    "**Why not chunk?**\n",
    "- Already at optimal size for retrieval\n",
    "- Each unit is semantically complete\n",
    "- Chunking would break coherent information\n",
    "- Adds unnecessary complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192ce568978f11a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:47.462636Z",
     "start_time": "2025-11-04T21:16:47.459982Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:39.007422Z",
     "iopub.status.busy": "2025-11-05T02:56:39.007207Z",
     "iopub.status.idle": "2025-11-05T02:56:39.011271Z",
     "shell.execute_reply": "2025-11-05T02:56:39.010482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Example: Course Description\n",
      "================================================================================\n",
      "CS004: Database Systems\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: online\n",
      "Instructor: Nicholas Nelson\n",
      "Description: Design and implementation of database systems. SQL, normalization, transactions, and database administration.\n",
      "    \n",
      "================================================================================\n",
      "\n",
      "Token count: 49\n",
      "Semantic completeness: âœ… Complete (has all info about this course)\n",
      "Chunking needed? âŒ NO\n",
      "\n",
      "Why not?\n",
      "- Under 500 tokens (well within limits)\n",
      "- Self-contained (doesn't reference other sections)\n",
      "- Semantically complete (has all course details)\n",
      "- Breaking it up would lose context\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Course data (NO chunking needed)\n",
    "sample_course_text = transform_course_to_text(all_courses[0])\n",
    "sample_tokens = count_tokens(sample_course_text)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Example: Course Description\n",
    "{'=' * 80}\n",
    "{sample_course_text}\n",
    "{'=' * 80}\n",
    "\n",
    "Token count: {sample_tokens}\n",
    "Semantic completeness: âœ… Complete (has all info about this course)\n",
    "Chunking needed? âŒ NO\n",
    "\n",
    "Why not?\n",
    "- Under 500 tokens (well within limits)\n",
    "- Self-contained (doesn't reference other sections)\n",
    "- Semantically complete (has all course details)\n",
    "- Breaking it up would lose context\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c1dca350c1f2d",
   "metadata": {},
   "source": [
    "### When You DO Need Chunking\n",
    "\n",
    "Chunking becomes necessary when documents are too long or contain multiple distinct topics:\n",
    "\n",
    "**Examples of data that NEED chunking:**\n",
    "- âœ… Research papers (multiple sections)\n",
    "- âœ… Technical documentation (many topics)\n",
    "- âœ… Books/chapters (many concepts)\n",
    "- âœ… Legal contracts (multiple clauses)\n",
    "- âœ… Medical records (multiple visits/conditions)\n",
    "\n",
    "**Why chunk?**\n",
    "- **Exceeds embedding model limits** - Most embedding models have context windows of 512-8192 tokens\n",
    "- **Contains multiple distinct topics** - Should be retrieved separately for precision\n",
    "- **Too large for LLM to process effectively** - Even if it fits, quality degrades\n",
    "- **Improves retrieval precision** - Find specific sections, not whole document\n",
    "- **Prevents context quality problems**:\n",
    "    - **\"Needle in the Haystack\" Problem**\n",
    "       - LLMs struggle to find relevant information buried in long context\n",
    "       - Performance degrades significantly in middle of long documents\n",
    "       - Even GPT-4 shows 10-30% accuracy drop with irrelevant context\n",
    "\n",
    "    - **Context Poisoning**\n",
    "       - Irrelevant information actively degrades response quality\n",
    "       - LLM may focus on wrong parts of context\n",
    "       - Contradictory information causes confusion\n",
    "\n",
    "    - **Context Rot (Lost in the Middle)**\n",
    "       - Information in middle of long context is often ignored\n",
    "       - LLMs have recency bias (focus on start/end)\n",
    "       - Critical details get \"lost\" even if technically present\n",
    "\n",
    "**ðŸ’¡ Solution:** Chunk documents so each chunk is focused, relevant, and within optimal context size (typically 200-800 tokens per chunk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8367ddebc584554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:47.479008Z",
     "start_time": "2025-11-04T21:16:47.474875Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:39.013096Z",
     "iopub.status.busy": "2025-11-05T02:56:39.012967Z",
     "iopub.status.idle": "2025-11-05T02:56:39.018197Z",
     "shell.execute_reply": "2025-11-05T02:56:39.017512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1,035 | Words: ~634\n"
     ]
    }
   ],
   "source": [
    "# Example: Research paper (NEEDS chunking)\n",
    "# Let's simulate a long research paper about Redis\n",
    "\n",
    "research_paper = \"\"\"\n",
    "# Optimizing Vector Search Performance in Redis\n",
    "\n",
    "## Abstract\n",
    "This paper presents a comprehensive analysis of vector search optimization techniques in Redis,\n",
    "examining the trade-offs between search quality, latency, and memory usage. We evaluate multiple\n",
    "indexing strategies including HNSW and FLAT indexes across datasets ranging from 10K to 10M vectors.\n",
    "Our results demonstrate that careful index configuration can improve search latency by up to 10x\n",
    "while maintaining 95%+ recall. We also introduce novel compression techniques that reduce memory\n",
    "usage by 75% with minimal impact on search quality.\n",
    "\n",
    "## 1. Introduction\n",
    "Vector databases have become essential infrastructure for modern AI applications, enabling semantic\n",
    "search, recommendation systems, and retrieval-augmented generation (RAG). Redis, traditionally known\n",
    "as an in-memory data structure store, has evolved to support high-performance vector search through\n",
    "the RediSearch module. However, optimizing vector search performance requires understanding complex\n",
    "trade-offs between multiple dimensions...\n",
    "\n",
    "[... 5,000 more words covering methodology, experiments, results, discussion ...]\n",
    "\n",
    "## 2. Background and Related Work\n",
    "Previous work on vector search optimization has focused primarily on algorithmic improvements to\n",
    "approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) introduced HNSW, which has\n",
    "become the de facto standard for high-dimensional vector search. Johnson et al. (2019) developed\n",
    "FAISS, demonstrating that product quantization can significantly reduce memory usage...\n",
    "\n",
    "[... 2,000 more words ...]\n",
    "\n",
    "## 3. Performance Analysis and Results\n",
    "\n",
    "### 3.1 HNSW Configuration Trade-offs\n",
    "\n",
    "Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64,\n",
    "we observe significant improvements in recall (0.89 to 0.97) but at the cost of increased latency (2.1ms to 8.7ms)\n",
    "and memory usage (1.2GB to 3.8GB). The sweet spot for most production workloads is M=32 with ef_construction=200,\n",
    "which achieves 0.94 recall with 4.3ms latency.\n",
    "\n",
    "Table 1: HNSW Performance Comparison\n",
    "| M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB) | Build Time (min) |\n",
    "|----|-----------------|-----------|--------------|-------------|------------------|\n",
    "| 16 | 100            | 0.89      | 2.1          | 1.2         | 8                |\n",
    "| 32 | 200            | 0.94      | 4.3          | 2.1         | 15               |\n",
    "| 64 | 400            | 0.97      | 8.7          | 3.8         | 32               |\n",
    "\n",
    "The data clearly demonstrates the fundamental trade-off between search quality and resource consumption.\n",
    "For applications requiring high recall (>0.95), the increased latency and memory costs are unavoidable.\n",
    "\n",
    "### 3.2 Mathematical Model\n",
    "\n",
    "The recall-latency trade-off can be modeled as a quadratic function of the HNSW parameters:\n",
    "\n",
    "Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³\n",
    "\n",
    "Where:\n",
    "- M = number of connections per layer (controls graph connectivity)\n",
    "- ef = size of dynamic candidate list (controls search breadth)\n",
    "- Î±, Î², Î³ = dataset-specific constants (fitted from experimental data)\n",
    "\n",
    "For our e-commerce dataset, we fitted: Î±=0.002, Î²=0.015, Î³=1.2 (RÂ²=0.94)\n",
    "\n",
    "This model allows us to predict latency for untested configurations and optimize for specific\n",
    "recall targets. The quadratic dependency on M explains why doubling M more than doubles latency.\n",
    "\n",
    "## 4. Implementation Recommendations\n",
    "\n",
    "Based on our findings, we recommend the following configuration for production deployments:\n",
    "\n",
    "```python\n",
    "# Optimal HNSW configuration for balanced performance\n",
    "index_params = {\n",
    "    \"M\": 32,                  # Balance recall and latency\n",
    "    \"ef_construction\": 200,   # Higher quality index\n",
    "    \"ef_runtime\": 100         # Fast search with good recall\n",
    "}\n",
    "```\n",
    "\n",
    "This configuration achieves 0.94 recall with 4.3ms p95 latency, suitable for most real-time applications.\n",
    "For applications with stricter latency requirements (<2ms), consider M=16 with ef_construction=100,\n",
    "accepting the lower recall of 0.89. For applications requiring maximum recall (>0.95), use M=64\n",
    "with ef_construction=400, but ensure adequate memory and accept higher latency.\n",
    "\n",
    "[... 1,500 more words with additional analysis ...]\n",
    "\n",
    "## 5. Discussion and Conclusion\n",
    "Our findings demonstrate that vector search optimization is fundamentally about understanding\n",
    "YOUR specific requirements and constraints. There is no one-size-fits-all configuration. The choice\n",
    "between HNSW parameters depends on your specific recall requirements, latency budget, and memory constraints.\n",
    "We provide a mathematical model and practical guidelines to help practitioners make informed decisions...\n",
    "\"\"\"\n",
    "\n",
    "paper_tokens = count_tokens(research_paper)\n",
    "print(f\"Token count: {paper_tokens:,} | Words: ~{len(research_paper.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2bdec6f414d76",
   "metadata": {},
   "source": [
    "**ðŸ“Š Analysis: Research Paper Example**\n",
    "\n",
    "**Document:** \"Optimizing Vector Search Performance in Redis\"\n",
    "\n",
    "**Structure:** Abstract, Introduction, Background, Methodology, Results, Discussion\n",
    "\n",
    "**Chunking needed?** âœ… **YES**\n",
    "\n",
    "**Why This Document May Benefit from Chunking (Even with Large Context Windows):**\n",
    "\n",
    "> **Note:** Modern LLMs can handle 128K+ tokens, so \"fitting in context\" isn't the issue. The real value of chunking is **better data modeling and retrieval precision**.\n",
    "\n",
    "**1. Retrieval Precision vs. Recall Trade-off**\n",
    "\n",
    "Without chunking (embed entire paper):\n",
    "- Query: \"What compression techniques were used?\"\n",
    "- Retrieved: Entire 15,000-token paper (includes Abstract, Background, Results, Discussion)\n",
    "- Problem: 80% of retrieved content is irrelevant to the query\n",
    "- LLM must process 15,000 tokens to find 200 tokens of relevant information\n",
    "\n",
    "With chunking (embed by section):\n",
    "- Query: \"What compression techniques were used?\"\n",
    "- Retrieved: Methodology section (800 tokens)\n",
    "- Result: 90%+ of retrieved content is directly relevant\n",
    "- LLM processes 800 focused tokens with high signal-to-noise ratio\n",
    "\n",
    "**2. Structured Content Requires Specialized Chunking**\n",
    "\n",
    "Research papers contain heterogeneous content types that need different handling. Without specialized chunking, there will be a danger of mixing incompatible content types, chunking in the middle of tables, etc.\n",
    "\n",
    "**Tables and Charts:**\n",
    "```\n",
    "Table 1: HNSW Performance Comparison\n",
    "| M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB) |\n",
    "|----|-----------------|-----------|--------------|-------------|\n",
    "| 16 | 100            | 0.89      | 2.1          | 1.2         |\n",
    "| 32 | 200            | 0.94      | 4.3          | 2.1         |\n",
    "| 64 | 400            | 0.97      | 8.7          | 3.8         |\n",
    "```\n",
    "\n",
    "**Best practice:** Chunk table WITH its caption and explanation:\n",
    "- âœ… \"Table 1 shows HNSW performance trade-offs. As M increases from 16 to 64, recall improves from 0.89 to 0.97, but latency increases from 2.1ms to 8.7ms...\"\n",
    "- âŒ Don't chunk table separately from context - it becomes meaningless\n",
    "\n",
    "**Mathematical Formulas:**\n",
    "```\n",
    "The recall-latency trade-off can be modeled as:\n",
    "Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³\n",
    "\n",
    "Where:\n",
    "- M = number of connections per layer\n",
    "- ef = size of dynamic candidate list\n",
    "- Î±, Î², Î³ = dataset-specific constants\n",
    "```\n",
    "\n",
    "**Best practice:** Chunk formula WITH its explanation and variable definitions\n",
    "- âœ… Keep formula + explanation + interpretation together\n",
    "- âŒ Don't separate formula from its meaning\n",
    "\n",
    "**Code Snippets:**\n",
    "```python\n",
    "# Optimal HNSW configuration for our use case\n",
    "index_params = {\n",
    "    \"M\": 32,              # Balance recall and latency\n",
    "    \"ef_construction\": 200,  # Higher quality index\n",
    "    \"ef_runtime\": 100     # Fast search\n",
    "}\n",
    "```\n",
    "\n",
    "**Best practice:** Chunk code WITH its context and rationale\n",
    "- âœ… \"For production deployment, we recommend M=32 and ef_construction=200 because...\"\n",
    "- âŒ Don't chunk code without explaining WHY these values\n",
    "\n",
    "**3. Query-Specific Retrieval Patterns**\n",
    "\n",
    "Different queries need different chunks:\n",
    "\n",
    "| Query | Needs | Without Chunking | With Chunking |\n",
    "|-------|-------|------------------|---------------|\n",
    "| \"What compression techniques?\" | Methodology section | Entire paper (15K tokens) | Methodology (800 tokens) |\n",
    "| \"What were recall results?\" | Results + Table 1 | Entire paper (15K tokens) | Results section (600 tokens) |\n",
    "| \"How does HNSW work?\" | Background + Formula | Entire paper (15K tokens) | Background (500 tokens) |\n",
    "| \"What's the recommended config?\" | Discussion + Code | Entire paper (15K tokens) | Discussion (400 tokens) |\n",
    "\n",
    "**Impact:** 10-20x reduction in irrelevant context, leading to faster responses and better quality.\n",
    "\n",
    "**4. Embedding Quality: Focused vs. Averaged**\n",
    "\n",
    "**Without chunking:**\n",
    "- Embedding represents \"a paper about vector search, HNSW, compression, benchmarks, Redis...\"\n",
    "- Generic, averaged representation\n",
    "- Matches weakly with specific queries\n",
    "\n",
    "**With chunking:**\n",
    "- Methodology chunk: \"compression techniques, quantization, memory reduction, implementation details...\"\n",
    "- Results chunk: \"recall metrics, latency measurements, performance comparisons, benchmark data...\"\n",
    "- Each embedding is focused and matches strongly with relevant queries\n",
    "\n",
    "**ðŸ’¡ Key Insight:** Chunking isn't about fitting in context windows - it's about **data modeling for retrieval**. Just like you wouldn't store all customer data in one database row, you shouldn't embed all document content in one vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6dbd30ec917f3",
   "metadata": {},
   "source": [
    "### Chunking Strategies: Engineering Trade-Offs\n",
    "\n",
    "Once you've determined that your data needs chunking, the next question is: **How should you chunk it?**\n",
    "\n",
    "There's no single \"best\" chunking strategy - the optimal approach depends on YOUR data characteristics and query patterns. Let's explore different strategies and their trade-offs.\n",
    "\n",
    "**ðŸ”§ Using LangChain for Production-Ready Chunking**\n",
    "\n",
    "In this section, we'll use **LangChain's text splitting utilities** for Strategies 2 and 3. LangChain provides battle-tested, production-ready implementations that handle edge cases and optimize for LLM consumption.\n",
    "\n",
    "**Why LangChain?**\n",
    "- **Industry-standard**: Used by thousands of production applications\n",
    "- **Smart boundary detection**: Respects natural text boundaries (paragraphs, sentences, words)\n",
    "- **Local embeddings**: Free semantic chunking with HuggingFace models (no API costs)\n",
    "- **Well-tested**: Handles edge cases (empty chunks, unicode, special characters)\n",
    "\n",
    "We'll use:\n",
    "- `RecursiveCharacterTextSplitter` (Strategy 2): Smart fixed-size chunking with boundary awareness\n",
    "- `SemanticChunker` + `HuggingFaceEmbeddings` (Strategy 3): Meaning-based chunking with local models\n",
    "\n",
    "### Strategy 1: Document-Based Chunking (Structure-Aware)\n",
    "\n",
    "**Concept:** Split documents based on their inherent structure (sections, paragraphs, headings, and as mentioned earlier, tables, code, and formulas).\n",
    "\n",
    "**Best for:** Structured documents with clear logical divisions (research papers, technical docs, books, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e395e0c41fc50ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:48.570565Z",
     "start_time": "2025-11-04T21:16:48.565095Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:39.019872Z",
     "iopub.status.busy": "2025-11-05T02:56:39.019743Z",
     "iopub.status.idle": "2025-11-05T02:56:39.024249Z",
     "shell.execute_reply": "2025-11-05T02:56:39.023613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Strategy 1: Document-Based (Structure-Aware) Chunking\n",
      "================================================================================\n",
      "Original document: 1,035 tokens\n",
      "Number of chunks: 7\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 8 tokens - # Optimizing Vector Search Performance in Redis...\n",
      "\n",
      "   Chunk 2: 108 tokens - ## Abstract This paper presents a comprehensive analysis of vector search optimization techniques in Redis, examining the trade-offs between search quality, latency, and memory usage. We evaluate multiple indexing strategies including HNSW and FLAT indexes across datasets ranging from 10K to 10M vec...\n",
      "\n",
      "   Chunk 3: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for modern AI applications, enabling semantic search, recommendation systems, and retrieval-augmented generation (RAG). Redis, traditionally known as an in-memory data structure store, has evolved to support high-performance ve...\n",
      "\n",
      "   Chunk 4: 98 tokens - ## 2. Background and Related Work Previous work on vector search optimization has focused primarily on algorithmic improvements to approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) introduced HNSW, which has become the de facto standard for high-dimensional vector search. Johnson...\n",
      "\n",
      "   Chunk 5: 464 tokens - ## 3. Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64, we observe significant improvements in recall (0.89 to 0.97) but at the cost of increased latency (2.1ms to 8.7m...\n",
      "\n",
      "   Chunk 6: 187 tokens - ## 4. Implementation Recommendations  Based on our findings, we recommend the following configuration for production deployments:  ```python # Optimal HNSW configuration for balanced performance index_params = {     \"M\": 32,                  # Balance recall and latency     \"ef_construction\": 200,  ...\n",
      "\n",
      "   Chunk 7: 73 tokens - ## 5. Discussion and Conclusion Our findings demonstrate that vector search optimization is fundamentally about understanding YOUR specific requirements and constraints. There is no one-size-fits-all configuration. The choice between HNSW parameters depends on your specific recall requirements, late...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Document-Based Chunking\n",
    "# Split research paper by sections (using markdown headers)\n",
    "\n",
    "\n",
    "def chunk_by_structure(text: str, separator: str = \"\\n## \") -> List[str]:\n",
    "    \"\"\"Split text by structural markers (e.g., markdown headers).\"\"\"\n",
    "\n",
    "    # Split by headers\n",
    "    sections = text.split(separator)\n",
    "\n",
    "    # Clean and format chunks\n",
    "    chunks = []\n",
    "    for i, section in enumerate(sections):\n",
    "        if section.strip():\n",
    "            # Add header back (except for first chunk which is title)\n",
    "            if i > 0:\n",
    "                chunk = \"## \" + section\n",
    "            else:\n",
    "                chunk = section\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Apply to research paper\n",
    "structure_chunks = chunk_by_structure(research_paper)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Strategy 1: Document-Based (Structure-Aware) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Number of chunks: {len(structure_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(structure_chunks):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    # Show first 100 chars of each chunk\n",
    "    preview = chunk[:300].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f1d657ce79b657",
   "metadata": {},
   "source": [
    "**Strategy 1 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- Respects document structure (sections stay together)\n",
    "- Semantically coherent (each chunk is a complete section)\n",
    "- Easy to implement for structured documents\n",
    "- Preserves author's logical organization\n",
    "- **Keeps tables, formulas, and code WITH their context** (e.g., \"## 3. Performance Analysis\" section includes Table 1 WITH its explanation, and \"## 3.2 Mathematical Model\" includes the formula WITH its variable definitions)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Variable chunk sizes (some sections longer than others)\n",
    "- Requires documents to have clear structure\n",
    "- May create chunks that are still too large\n",
    "- Doesn't work for unstructured text\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Research papers with clear sections\n",
    "- Technical documentation with headers\n",
    "- Books with chapters/sections\n",
    "- Any markdown/HTML content with structural markers\n",
    "\n",
    "ðŸ’¡ **Key Insight:**\n",
    "Notice how Chunk 3 (\"## 3. Performance Analysis and Results\") contains Table 1 along with its explanation and interpretation. This is the correct approach - the table is meaningless without context. Similarly, the mathematical formula in section 3.2 stays with its variable definitions and interpretation. This is why structure-aware chunking is superior to fixed-size chunking for technical documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ccfaddbd73afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:59.014056Z",
     "start_time": "2025-11-04T21:16:59.012297Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cb95169c19d8fd",
   "metadata": {},
   "source": [
    "### Strategy 2: Fixed-Size Chunking (Token-Based)\n",
    "\n",
    "**Concept:** Split text into chunks of a predetermined size (e.g., 512 tokens) with overlap.\n",
    "\n",
    "**Best for:** Unstructured text, quick prototyping, when you need consistent chunk sizes.\n",
    "\n",
    "Trade-offs:\n",
    "- Ignores document structure (may split mid-sentence or mid-paragraph or mid-table)\n",
    "- Can break semantic coherence\n",
    "- May split important information across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c370a104c561f59a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:17:00.068503Z",
     "start_time": "2025-11-04T21:17:00.051846Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:39.025918Z",
     "iopub.status.busy": "2025-11-05T02:56:39.025827Z",
     "iopub.status.idle": "2025-11-05T02:56:39.036582Z",
     "shell.execute_reply": "2025-11-05T02:56:39.036158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running fixed-size chunking with LangChain...\n",
      "   Trying to split on: paragraphs â†’ sentences â†’ words â†’ characters\n",
      "\n",
      "ðŸ“Š Strategy 2: Fixed-Size (LangChain) Chunking\n",
      "================================================================================\n",
      "Original document: 1,035 tokens\n",
      "Target chunk size: 800 characters (~200 words)\n",
      "Overlap: 100 characters\n",
      "Number of chunks: 8\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 117 tokens - # Optimizing Vector Search Performance in Redis  ## Abstract This paper presents a comprehensive ana...\n",
      "   Chunk 2: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for modern AI applications,...\n",
      "   Chunk 3: 134 tokens - [... 5,000 more words covering methodology, experiments, results, discussion ...]  ## 2. Background ...\n",
      "   Chunk 4: 128 tokens - ## 3. Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  Table 1 shows the per...\n",
      "   Chunk 5: 206 tokens - Table 1: HNSW Performance Comparison | M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB)...\n",
      "... (3 more chunks)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 2: Fixed-Size Chunking (Using LangChain)\n",
    "# Industry-standard approach with smart boundary detection\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter with smart boundary detection\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # Target chunk size in characters\n",
    "    chunk_overlap=100,  # Overlap to preserve context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try these in order\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Running fixed-size chunking with LangChain...\")\n",
    "print(\"   Trying to split on: paragraphs â†’ sentences â†’ words â†’ characters\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "fixed_chunks_docs = text_splitter.create_documents([research_paper])\n",
    "fixed_chunks = [doc.page_content for doc in fixed_chunks_docs]\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Strategy 2: Fixed-Size (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Target chunk size: 800 characters (~200 words)\n",
    "Overlap: 100 characters\n",
    "Number of chunks: {len(fixed_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(fixed_chunks[:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(f\"... ({len(fixed_chunks) - 5} more chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403c38d9a9d0a06",
   "metadata": {},
   "source": [
    "**Strategy 2 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- **Respects natural boundaries**: Tries paragraphs â†’ sentences â†’ words â†’ characters\n",
    "- Consistent chunk sizes (predictable token usage)\n",
    "- Works on any text (structured or unstructured)\n",
    "- Fast processing\n",
    "- **Doesn't split mid-sentence** (unless absolutely necessary)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Ignores document structure (doesn't understand sections)\n",
    "- Can break semantic coherence (may split related content)\n",
    "- Overlap creates redundancy (increases storage/cost)\n",
    "- May split important information across chunks\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Unstructured text (no clear sections)\n",
    "- Quick prototyping and baselines\n",
    "- When consistent chunk sizes are required\n",
    "- Simple documents where structure doesn't matter\n",
    "\n",
    "ðŸ’¡ **How RecursiveCharacterTextSplitter Works:**\n",
    "\n",
    "Unlike naive fixed-size splitting, this algorithm:\n",
    "\n",
    "1. **Tries separators in order**: `[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]`\n",
    "2. **Splits on first successful separator** that keeps chunks under target size\n",
    "3. **Falls back to next separator** if chunks are still too large\n",
    "4. **Preserves natural boundaries** (paragraphs > sentences > words > characters)\n",
    "\n",
    "**Example:**\n",
    "- Target: 800 characters\n",
    "- First try: Split on `\\n\\n` (paragraphs)\n",
    "- If paragraph > 800 chars: Split on `\\n` (lines)\n",
    "- If line > 800 chars: Split on `. ` (sentences)\n",
    "- And so on...\n",
    "\n",
    "**Why this is better than naive splitting:**\n",
    "- âœ… Respects natural text boundaries\n",
    "- âœ… Doesn't split mid-sentence (unless necessary)\n",
    "- âœ… Maintains readability\n",
    "- âœ… Better for LLM comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37f437b3aa4541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:17:04.920222Z",
     "start_time": "2025-11-04T21:17:04.917767Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca46498e87604fa5",
   "metadata": {},
   "source": [
    "### Strategy 3: Semantic Chunking (Meaning-Based)\n",
    "\n",
    "**Concept:** Split text based on semantic similarity using embeddings - create new chunks when topic changes significantly.\n",
    "\n",
    "**How it works:**\n",
    "1. Split text into sentences or paragraphs\n",
    "2. Generate embeddings for each segment\n",
    "3. Calculate similarity between consecutive segments\n",
    "4. Create chunk boundaries where similarity drops (topic shift detected)\n",
    "\n",
    "**Best for:** Dense academic text, legal documents, narratives where semantic boundaries don't align with structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de5dadba814863e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:17:06.687906Z",
     "start_time": "2025-11-04T21:17:06.551885Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:39.038210Z",
     "iopub.status.busy": "2025-11-05T02:56:39.038107Z",
     "iopub.status.idle": "2025-11-05T02:56:41.038975Z",
     "shell.execute_reply": "2025-11-05T02:56:41.038443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:40 sentence_transformers.SentenceTransformer INFO   Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running semantic chunking with LangChain...\n",
      "   Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n",
      "   Breakpoint detection: 25th percentile of similarity scores\n",
      "\n",
      "ðŸ“Š Strategy 3: Semantic (LangChain) Chunking\n",
      "================================================================================\n",
      "Original document: 1,035 tokens\n",
      "Breakpoint method: Percentile (25th percentile)\n",
      "Number of chunks: 25\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 70 tokens -  # Optimizing Vector Search Performance in Redis  ## Abstract This paper presents a comprehensive analysis of vector search optimization techniques in Redis, examining the trade-offs between search qu...\n",
      "\n",
      "   Chunk 2: 26 tokens - Our results demonstrate that careful index configuration can improve search latency by up to 10x while maintaining 95%+ recall....\n",
      "\n",
      "   Chunk 3: 22 tokens - We also introduce novel compression techniques that reduce memory usage by 75% with minimal impact on search quality....\n",
      "\n",
      "   Chunk 4: 4 tokens - ## 1....\n",
      "\n",
      "   Chunk 5: 60 tokens - Introduction Vector databases have become essential infrastructure for modern AI applications, enabling semantic search, recommendation systems, and retrieval-augmented generation (RAG). Redis, tradit...\n",
      "\n",
      "   Chunk 6: 16 tokens - However, optimizing vector search performance requires understanding complex trade-offs between multiple dimensions......\n",
      "\n",
      "   Chunk 7: 2 tokens - [......\n",
      "\n",
      "   Chunk 8: 18 tokens - 5,000 more words covering methodology, experiments, results, discussion ...]  ## 2....\n",
      "\n",
      "   Chunk 9: 60 tokens - Background and Related Work Previous work on vector search optimization has focused primarily on algorithmic improvements to approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) intro...\n",
      "\n",
      "   Chunk 10: 4 tokens - Johnson et al....\n",
      "\n",
      "   Chunk 11: 20 tokens - (2019) developed FAISS, demonstrating that product quantization can significantly reduce memory usage......\n",
      "\n",
      "   Chunk 12: 2 tokens - [......\n",
      "\n",
      "   Chunk 13: 10 tokens - 2,000 more words ...]  ## 3....\n",
      "\n",
      "   Chunk 14: 91 tokens - Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64, we observe signifi...\n",
      "\n",
      "   Chunk 15: 33 tokens - The sweet spot for most production workloads is M=32 with ef_construction=200, which achieves 0.94 recall with 4.3ms latency....\n",
      "\n",
      "   Chunk 16: 177 tokens - Table 1: HNSW Performance Comparison | M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB) | Build Time (min) | |----|-----------------|-----------|--------------|-------------|-------------...\n",
      "\n",
      "   Chunk 17: 159 tokens - ### 3.2 Mathematical Model  The recall-latency trade-off can be modeled as a quadratic function of the HNSW parameters:  Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³  Where: - M = number of connections per layer ...\n",
      "\n",
      "   Chunk 18: 4 tokens - ## 4....\n",
      "\n",
      "   Chunk 19: 139 tokens - Implementation Recommendations  Based on our findings, we recommend the following configuration for production deployments:  ```python # Optimal HNSW configuration for balanced performance index_param...\n",
      "\n",
      "   Chunk 20: 31 tokens - For applications requiring maximum recall (>0.95), use M=64 with ef_construction=400, but ensure adequate memory and accept higher latency....\n",
      "\n",
      "   Chunk 21: 2 tokens - [......\n",
      "\n",
      "   Chunk 22: 35 tokens - 1,500 more words with additional analysis ...]  ## 5. Discussion and Conclusion Our findings demonstrate that vector search optimization is fundamentally about understanding YOUR specific requirements...\n",
      "\n",
      "   Chunk 23: 10 tokens - There is no one-size-fits-all configuration....\n",
      "\n",
      "   Chunk 24: 37 tokens - The choice between HNSW parameters depends on your specific recall requirements, latency budget, and memory constraints. We provide a mathematical model and practical guidelines to help practitioners ...\n",
      "\n",
      "   Chunk 25: 0 tokens - ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strategy 3: Semantic Chunking (Using LangChain)\n",
    "# Industry-standard approach with local embeddings (no API costs!)\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Suppress tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize local embeddings (no API costs!)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# Create semantic chunker with percentile-based breakpoint detection\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",  # Split at bottom 25% of similarities\n",
    "    breakpoint_threshold_amount=25,  # 25th percentile\n",
    "    buffer_size=1,  # Compare consecutive sentences\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Running semantic chunking with LangChain...\")\n",
    "print(\"   Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\")\n",
    "print(\"   Breakpoint detection: 25th percentile of similarity scores\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "semantic_chunks_docs = semantic_chunker.create_documents([research_paper])\n",
    "\n",
    "# Extract text from Document objects\n",
    "semantic_chunks = [doc.page_content for doc in semantic_chunks_docs]\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Strategy 3: Semantic (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Breakpoint method: Percentile (25th percentile)\n",
    "Number of chunks: {len(semantic_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:200].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4d65b4ad36fd9",
   "metadata": {},
   "source": [
    "**Strategy 3 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- **Detects actual topic changes** using semantic similarity (not just structural markers)\n",
    "- Preserves semantic coherence (topics stay together even without headers)\n",
    "- Better retrieval quality (chunks are topically focused)\n",
    "- Adapts to content (works on unstructured text)\n",
    "- Reduces context loss at boundaries (doesn't split mid-topic)\n",
    "- **Free and local**: Uses sentence-transformers (no API costs)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Slower processing (must compute embeddings for each sentence)\n",
    "- Variable chunk sizes (depends on topic boundaries)\n",
    "- Higher computational cost (embedding computation + similarity calculations)\n",
    "- Requires initial model download (~90MB for all-MiniLM-L6-v2)\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Dense academic papers with complex topic transitions\n",
    "- Legal documents where semantic sections don't have headers\n",
    "- Narratives where topics don't align with structure\n",
    "- Unstructured text (emails, transcripts, conversations)\n",
    "- When retrieval quality is more important than processing speed\n",
    "\n",
    "ðŸ’¡ **How Percentile-Based Breakpoint Detection Works:**\n",
    "\n",
    "Instead of using a fixed similarity threshold (e.g., 0.75), the percentile method:\n",
    "\n",
    "1. **Computes all similarities** between consecutive sentences\n",
    "2. **Calculates percentiles** of the similarity distribution\n",
    "3. **Creates breakpoints** where similarity is in the bottom X percentile\n",
    "\n",
    "**Example:**\n",
    "- Similarities: [0.92, 0.88, 0.45, 0.91, 0.35, 0.89]\n",
    "- 25th percentile: 0.45\n",
    "- Breakpoints created at: positions 2 (0.45) and 4 (0.35)\n",
    "\n",
    "**Why this is better than fixed threshold:**\n",
    "- âœ… Adapts to document's similarity distribution\n",
    "- âœ… Works across different document types\n",
    "- âœ… No manual threshold tuning needed\n",
    "- âœ… More robust to outliers\n",
    "\n",
    "**Alternative Breakpoint Methods:**\n",
    "- `\"gradient\"`: Detects sudden drops in similarity (topic shifts)\n",
    "- `\"standard_deviation\"`: Uses statistical deviation from mean\n",
    "- `\"interquartile\"`: Uses IQR-based outlier detection\n",
    "\n",
    "This is fundamentally different from structure-based chunking - it detects semantic boundaries regardless of headers or formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fca0e15cdba8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed2aaad7a055002d",
   "metadata": {},
   "source": [
    "### Strategy 4: Hierarchical Chunking (Multi-Level)\n",
    "\n",
    "**Concept:** Create multiple levels of chunks - large chunks for overview, small chunks for details.\n",
    "\n",
    "**Best for:** Very large documents where users need both high-level summaries and specific details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7299e6c8f02028dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:41.040596Z",
     "iopub.status.busy": "2025-11-05T02:56:41.040318Z",
     "iopub.status.idle": "2025-11-05T02:56:41.044841Z",
     "shell.execute_reply": "2025-11-05T02:56:41.044524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Strategy 4: Hierarchical (Multi-Level) Chunking\n",
      "================================================================================\n",
      "Original document: 1,035 tokens\n",
      "\n",
      "Level 1 (Sections): 7 chunks\n",
      "\n",
      "   L1-1: 8 tokens - # Optimizing Vector Search Performance in Redis...\n",
      "   L1-2: 108 tokens - ## Abstract This paper presents a comprehensive analysis of vector search optimi...\n",
      "   L1-3: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for mod...\n",
      "   L1-4: 98 tokens - ## 2. Background and Related Work Previous work on vector search optimization ha...\n",
      "   L1-5: 464 tokens - ## 3. Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  T...\n",
      "   L1-6: 187 tokens - ## 4. Implementation Recommendations  Based on our findings, we recommend the fo...\n",
      "   L1-7: 73 tokens - ## 5. Discussion and Conclusion Our findings demonstrate that vector search opti...\n",
      "\n",
      "Level 2 (Subsections): 13 chunks\n",
      "\n",
      "   L2-1: 8 tokens - # Optimizing Vector Search Performance in Redis...\n",
      "   L2-2: 108 tokens - ## Abstract This paper presents a comprehensive analysis of vector search optimi...\n",
      "   L2-3: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for mod...\n",
      "   L2-4: 98 tokens - ## 2. Background and Related Work Previous work on vector search optimization ha...\n",
      "   L2-5: 107 tokens - Table 1 shows the performance comparison across different HNSW configurations. A...\n",
      "... (8 more L2 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 4: Hierarchical Chunking\n",
    "\n",
    "\n",
    "def chunk_hierarchically(text: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Create multiple levels of chunks.\n",
    "    Level 1: Large sections (by ## headers)\n",
    "    Level 2: Subsections (by paragraphs within sections)\n",
    "    \"\"\"\n",
    "\n",
    "    # Level 1: Split by major sections\n",
    "    level1_chunks = chunk_by_structure(text, separator=\"\\n## \")\n",
    "\n",
    "    # Level 2: Further split large sections into paragraphs\n",
    "    level2_chunks = []\n",
    "    for section in level1_chunks:\n",
    "        # If section is large, split into paragraphs\n",
    "        if count_tokens(section) > 400:\n",
    "            paragraphs = [\n",
    "                p.strip() for p in section.split(\"\\n\\n\") if p.strip() and len(p) > 50\n",
    "            ]\n",
    "            level2_chunks.extend(paragraphs)\n",
    "        else:\n",
    "            level2_chunks.append(section)\n",
    "\n",
    "    return {\n",
    "        \"level1\": level1_chunks,  # Large sections\n",
    "        \"level2\": level2_chunks,  # Smaller subsections\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply to research paper\n",
    "hierarchical_chunks = chunk_hierarchically(research_paper)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Strategy 4: Hierarchical (Multi-Level) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "\n",
    "Level 1 (Sections): {len(hierarchical_chunks['level1'])} chunks\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(hierarchical_chunks[\"level1\"]):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:80].replace(\"\\n\", \" \")\n",
    "    print(f\"   L1-{i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Level 2 (Subsections): {len(hierarchical_chunks['level2'])} chunks\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(hierarchical_chunks[\"level2\"][:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:80].replace(\"\\n\", \" \")\n",
    "    print(f\"   L2-{i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(f\"... ({len(hierarchical_chunks['level2']) - 5} more L2 chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf78622dce4de75",
   "metadata": {},
   "source": [
    "**Strategy 4 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- Supports both overview and detailed queries\n",
    "- Flexible retrieval (can search at different levels)\n",
    "- Preserves document hierarchy\n",
    "- Better for complex documents\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- More complex to implement and maintain\n",
    "- Requires more storage (multiple levels)\n",
    "- Need strategy to choose which level to search\n",
    "- Higher indexing cost\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Very large documents (textbooks, manuals)\n",
    "- When users need both summaries and details\n",
    "- Technical documentation with nested structure\n",
    "- Legal contracts with sections and subsections\n",
    "\n",
    "ðŸ’¡ **Retrieval Strategy:**\n",
    "- Start with Level 1 for overview\n",
    "- If user needs more detail, retrieve Level 2 chunks\n",
    "- Can combine: \"Show section summary + relevant details\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811e94608a8eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc19c72b00b7f36",
   "metadata": {},
   "source": [
    "### Comparing Chunking Strategies: Decision Framework\n",
    "\n",
    "Now let's compare all four strategies side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33137223a8d1c166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:41.046089Z",
     "iopub.status.busy": "2025-11-05T02:56:41.046027Z",
     "iopub.status.idle": "2025-11-05T02:56:41.050168Z",
     "shell.execute_reply": "2025-11-05T02:56:41.049825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHUNKING STRATEGY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Document: Research Paper (1,035 tokens)\n",
      "\n",
      "Strategy              | Chunks | Avg Size | Complexity | Best For\n",
      "--------------------- | ------ | -------- | ---------- | --------\n",
      "Document-Based        |      7 |      148 | Low        | Structured docs\n",
      "Fixed-Size            |      8 |      140 | Low        | Unstructured text\n",
      "Semantic              |     25 |       41 | High       | Dense academic text\n",
      "Hierarchical (L1)     |      7 |      148 | Medium     | Large complex docs\n",
      "Hierarchical (L2)     |     13 |       76 | Medium     | Large complex docs\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "{'=' * 80}\n",
    "CHUNKING STRATEGY COMPARISON\n",
    "{'=' * 80}\n",
    "\n",
    "Document: Research Paper ({paper_tokens:,} tokens)\n",
    "\n",
    "Strategy              | Chunks | Avg Size | Complexity | Best For\n",
    "--------------------- | ------ | -------- | ---------- | --------\n",
    "Document-Based        | {len(structure_chunks):>6} | {sum(count_tokens(c) for c in structure_chunks) // len(structure_chunks):>8} | Low        | Structured docs\n",
    "Fixed-Size            | {len(fixed_chunks):>6} | {sum(count_tokens(c) for c in fixed_chunks) // len(fixed_chunks):>8} | Low        | Unstructured text\n",
    "Semantic              | {len(semantic_chunks):>6} | {sum(count_tokens(c) for c in semantic_chunks) // len(semantic_chunks):>8} | High       | Dense academic text\n",
    "Hierarchical (L1)     | {len(hierarchical_chunks['level1']):>6} | {sum(count_tokens(c) for c in hierarchical_chunks['level1']) // len(hierarchical_chunks['level1']):>8} | Medium     | Large complex docs\n",
    "Hierarchical (L2)     | {len(hierarchical_chunks['level2']):>6} | {sum(count_tokens(c) for c in hierarchical_chunks['level2']) // len(hierarchical_chunks['level2']):>8} | Medium     | Large complex docs\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1367dee484df00",
   "metadata": {},
   "source": [
    "### YOUR Chunking Decision Framework\n",
    "\n",
    "Here's how to choose the right chunking strategy for YOUR domain:\n",
    "\n",
    "**Step 1: Analyze YOUR Data Characteristics**\n",
    "\n",
    "Ask these questions about your documents:\n",
    "\n",
    "1. **Structure:** Do documents have clear structural markers (headers, sections)?\n",
    "   - âœ… Yes â†’ Consider Document-Based or Hierarchical\n",
    "   - âŒ No â†’ Consider Fixed-Size or Semantic\n",
    "\n",
    "2. **Length:** How long are documents?\n",
    "   - < 500 tokens â†’ Don't chunk!\n",
    "   - 500-2000 tokens â†’ Document-Based (if structured) or Fixed-Size\n",
    "   - 2000-10000 tokens â†’ Semantic or Hierarchical\n",
    "   - > 10000 tokens â†’ Hierarchical\n",
    "\n",
    "3. **Homogeneity:** Are all documents similar in structure?\n",
    "   - âœ… Yes â†’ Use single strategy\n",
    "   - âŒ No â†’ Consider Adaptive (different strategies for different doc types)\n",
    "\n",
    "4. **Topic Density:** How many topics per document?\n",
    "   - Single topic â†’ Don't chunk or use large chunks\n",
    "   - Multiple related topics â†’ Document-Based\n",
    "   - Many distinct topics â†’ Semantic or Fixed-Size\n",
    "\n",
    "**Step 2: Analyze YOUR Query Patterns**\n",
    "\n",
    "1. **Query Specificity:**\n",
    "   - Specific (\"What is HNSW?\") â†’ Smaller chunks (Fixed-Size, Semantic)\n",
    "   - Overview (\"Summarize the paper\") â†’ Larger chunks (Document-Based, Hierarchical L1)\n",
    "   - Mixed â†’ Hierarchical\n",
    "\n",
    "2. **Query Scope:**\n",
    "   - Single-section queries â†’ Document-Based\n",
    "   - Cross-section queries â†’ Semantic or Fixed-Size\n",
    "\n",
    "**Step 3: Analyze YOUR Constraints**\n",
    "\n",
    "1. **Token Budget:** How many tokens can you afford per query?\n",
    "   - Tight budget â†’ Smaller chunks, fewer retrieved\n",
    "   - Generous budget â†’ Larger chunks or Hierarchical\n",
    "\n",
    "2. **Latency Requirements:**\n",
    "   - Real-time â†’ Fixed-Size (fast, simple)\n",
    "   - Batch processing â†’ Semantic (slower but better quality)\n",
    "\n",
    "3. **Quality Requirements:**\n",
    "   - Highest quality â†’ Semantic or Hierarchical\n",
    "   - Good enough â†’ Document-Based or Fixed-Size\n",
    "\n",
    "**Example Decisions:**\n",
    "\n",
    "| Domain | Data Characteristics | Decision | Why |\n",
    "|--------|---------------------|----------|-----|\n",
    "| **Research Papers** | 5-10K tokens, clear sections, dense topics | Document-Based | Sections are natural semantic units |\n",
    "| **Customer Support** | 100-500 tokens, unstructured | Don't chunk! | Already optimal size |\n",
    "| **Legal Contracts** | 10-50K tokens, nested structure | Hierarchical | Need both overview and clause-level detail |\n",
    "| **Product Docs** | 1-5K tokens, mixed structure | Fixed-Size (512 tokens, 50 overlap) | Simple, works for varied content |\n",
    "| **Medical Records** | 1-3K tokens, chronological | Semantic | Topic changes (visits, conditions) don't align with structure |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb14c97495b45de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building Production-Ready Context Pipelines\n",
    "\n",
    "Now that you understand data transformation and chunking, let's discuss how to build production-ready pipelines.\n",
    "\n",
    "### Three Pipeline Architectures\n",
    "\n",
    "There are three main approaches to context preparation in production:\n",
    "\n",
    "### Architecture 1: Request-Time Processing\n",
    "\n",
    "**Concept:** Transform data on-the-fly when a query arrives.\n",
    "\n",
    "```\n",
    "User Query â†’ Retrieve Raw Data â†’ Transform â†’ Chunk (if needed) â†’ Embed â†’ Search â†’ Return Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Always up-to-date (no stale data)\n",
    "- âœ… No pre-processing required\n",
    "- âœ… Simple to implement\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Higher latency (processing happens during request)\n",
    "- âŒ Repeated work (same transformations for every query)\n",
    "- âŒ Not suitable for large datasets\n",
    "\n",
    "**Best for:**\n",
    "- Small datasets (< 1,000 documents)\n",
    "- Frequently changing data\n",
    "- Simple transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785624fc38e46d77",
   "metadata": {},
   "source": [
    "### Architecture 2: Batch Processing\n",
    "\n",
    "**Concept:** Pre-process all data in batches (nightly, weekly) and store results.\n",
    "\n",
    "```\n",
    "[Scheduled Job]\n",
    "Raw Data â†’ Extract â†’ Clean â†’ Transform â†’ Chunk â†’ Embed â†’ Store in Vector DB\n",
    "\n",
    "[Query Time]\n",
    "User Query â†’ Search Vector DB â†’ Return Pre-Processed Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Fast query time (all processing done ahead)\n",
    "- âœ… Efficient (process once, use many times)\n",
    "- âœ… Can use expensive transformations (LLM-based chunking, semantic analysis)\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Data can be stale (until next batch run)\n",
    "- âŒ Requires scheduling infrastructure\n",
    "- âŒ Higher storage costs (store processed data)\n",
    "\n",
    "**Best for:**\n",
    "- Large datasets (> 10,000 documents)\n",
    "- Infrequently changing data (daily/weekly updates)\n",
    "- Complex transformations (semantic chunking, LLM summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d956da9e767913",
   "metadata": {},
   "source": [
    "### Architecture 3: Event-Driven Processing\n",
    "\n",
    "**Concept:** Process data as it changes (real-time updates).\n",
    "\n",
    "```\n",
    "Data Change Event â†’ Trigger Pipeline â†’ Extract â†’ Clean â†’ Transform â†’ Chunk â†’ Embed â†’ Update Vector DB\n",
    "\n",
    "[Query Time]\n",
    "User Query â†’ Search Vector DB â†’ Return Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Always up-to-date (real-time)\n",
    "- âœ… Fast query time (pre-processed)\n",
    "- âœ… Efficient (only process changed data)\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Complex infrastructure (event streams, queues)\n",
    "- âŒ Requires change detection\n",
    "- âŒ Higher operational complexity\n",
    "\n",
    "**Best for:**\n",
    "- Real-time data (news, social media, live updates)\n",
    "- Large datasets that change frequently\n",
    "- When both freshness and speed are critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8677cf43366989a",
   "metadata": {},
   "source": [
    "### Choosing YOUR Pipeline Architecture\n",
    "\n",
    "Use this decision tree:\n",
    "\n",
    "**Question 1: How often does your data change?**\n",
    "- Real-time (seconds/minutes) â†’ Event-Driven\n",
    "- Frequently (hourly/daily) â†’ Batch or Event-Driven\n",
    "- Infrequently (weekly/monthly) â†’ Batch\n",
    "- Rarely (manual updates) â†’ Request-Time or Batch\n",
    "\n",
    "**Question 2: How large is your dataset?**\n",
    "- Small (< 1,000 docs) â†’ Request-Time\n",
    "- Medium (1,000-100,000 docs) â†’ Batch\n",
    "- Large (> 100,000 docs) â†’ Batch or Event-Driven\n",
    "\n",
    "**Question 3: What are your latency requirements?**\n",
    "- Real-time (< 100ms) â†’ Batch or Event-Driven (pre-processed)\n",
    "- Interactive (< 1s) â†’ Any approach\n",
    "- Batch queries â†’ Request-Time acceptable\n",
    "\n",
    "**Question 4: How complex are your transformations?**\n",
    "- Simple (cleaning, formatting) â†’ Any approach\n",
    "- Moderate (chunking, basic NLP) â†’ Batch or Event-Driven\n",
    "- Complex (LLM-based, semantic analysis) â†’ Batch (pre-compute)\n",
    "\n",
    "**Example Decision:**\n",
    "\n",
    "For Redis University:\n",
    "- âœ… **Data changes:** Weekly (new courses added)\n",
    "- âœ… **Dataset size:** 100-500 courses (medium)\n",
    "- âœ… **Latency:** Interactive (< 1s acceptable)\n",
    "- âœ… **Transformations:** Moderate (structured views + embeddings)\n",
    "- âœ… **Decision:** **Batch Processing** (weekly job to rebuild catalog + embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4003fc42ee8ddc",
   "metadata": {},
   "source": [
    "### Example: Batch Processing Pipeline for Redis University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68dc14ed5084daaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:41.051422Z",
     "iopub.status.busy": "2025-11-05T02:56:41.051360Z",
     "iopub.status.idle": "2025-11-05T02:56:41.301330Z",
     "shell.execute_reply": "2025-11-05T02:56:41.300657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BATCH PROCESSING PIPELINE - Redis University Courses\n",
      "================================================================================\n",
      "\n",
      "[Step 1/5] Extracting course data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:41 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Extracted 50 courses\n",
      "\n",
      "   ðŸ“„ Sample raw course:\n",
      "      CS004: Database Systems\n",
      "      Department: Computer Science, Credits: 3, Level: intermediate\n",
      "\n",
      "[Step 2/5] Cleaning data...\n",
      "   âœ… Cleaned: 20 courses (removed 30 test courses)\n",
      "\n",
      "   ðŸ“„ Example removed course:\n",
      "      ðŸ—‘ï¸  BUS033: Marketing Strategy (filtered out)\n",
      "\n",
      "[Step 3/5] Transforming to LLM-friendly format...\n",
      "   âœ… Transformed: 20 courses (1,079 total tokens)\n",
      "\n",
      "   ðŸ“„ Transformation example:\n",
      "      Before: CS004 (Course object)\n",
      "      After (LLM-friendly text):\n",
      "      CS004: Database Systems\n",
      "      Department: Computer Science\n",
      "      Credits: 3\n",
      "      Level: intermediate\n",
      "      Format: online\n",
      "      Instructor: Nicholas Nelson\n",
      "      Description: Design and implementation of database systems. SQL, normalization, transac...\n",
      "\n",
      "[Step 4/5] Creating structured catalog view...\n",
      "   âœ… Created catalog view (585 tokens)\n",
      "   âœ… Cached in Redis\n",
      "\n",
      "   ðŸ“„ Catalog view structure:\n",
      "      # Redis University Course Catalog\n",
      "      \n",
      "      ## Business (10 courses)\n",
      "      - BUS033: Marketing Strategy (intermediate)\n",
      "      - BUS032: Marketing Strategy (intermediate)\n",
      "      - BUS034: Marketing Strategy (intermediate)\n",
      "      - BUS035: Marketing Strategy (intermediate)\n",
      "      - BUS037: Marketing Strategy (intermediate)\n",
      "      - BUS039: Marketing ...\n",
      "\n",
      "[Step 5/5] Storing processed data...\n",
      "   âœ… Stored 20 processed courses in Redis\n",
      "\n",
      "   ðŸ“„ Storage example:\n",
      "      Key: course:processed:CS004\n",
      "      Value: CS004: Database Systems\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: online\n",
      "I...\n",
      "\n",
      "================================================================================\n",
      "BATCH PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "- Courses processed: 20\n",
      "- Total tokens: 1,079\n",
      "- Catalog view tokens: 585\n",
      "- Storage: Redis\n",
      "- Next run: 2024-10-07 (weekly)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Batch Processing Pipeline\n",
    "# This would run as a scheduled job (e.g., weekly)\n",
    "\n",
    "\n",
    "async def batch_process_courses():\n",
    "    \"\"\"\n",
    "    Batch processing pipeline for Redis University courses.\n",
    "    Runs weekly to update catalog and embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BATCH PROCESSING PIPELINE - Redis University Courses\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Extract\n",
    "    print(\"\\n[Step 1/5] Extracting course data...\")\n",
    "    all_courses = await course_manager.get_all_courses()\n",
    "    print(f\"   âœ… Extracted {len(all_courses)} courses\")\n",
    "\n",
    "    # Show sample raw data\n",
    "    if all_courses:\n",
    "        sample = all_courses[0]\n",
    "        print(f\"\\n   ðŸ“„ Sample raw course:\")\n",
    "        print(f\"      {sample.course_code}: {sample.title}\")\n",
    "        print(f\"      Department: {sample.department}, Credits: {sample.credits}, Level: {sample.difficulty_level.value}\")\n",
    "\n",
    "    # Step 2: Clean\n",
    "    print(\"\\n[Step 2/5] Cleaning data...\")\n",
    "    # Remove test courses, validate fields, etc.\n",
    "    cleaned_courses = [\n",
    "        c for c in all_courses if c.course_code.startswith((\"RU\", \"CS\", \"MATH\"))\n",
    "    ]\n",
    "    print(\n",
    "        f\"   âœ… Cleaned: {len(cleaned_courses)} courses (removed {len(all_courses) - len(cleaned_courses)} test courses)\"\n",
    "    )\n",
    "\n",
    "    # Show what was filtered out\n",
    "    removed_courses = [c for c in all_courses if not c.course_code.startswith((\"RU\", \"CS\", \"MATH\"))]\n",
    "    if removed_courses:\n",
    "        print(f\"\\n   ðŸ“„ Example removed course:\")\n",
    "        print(f\"      ðŸ—‘ï¸  {removed_courses[0].course_code}: {removed_courses[0].title} (filtered out)\")\n",
    "\n",
    "    # Step 3: Transform\n",
    "    print(\"\\n[Step 3/5] Transforming to LLM-friendly format...\")\n",
    "    transformed_courses = [transform_course_to_text(c) for c in cleaned_courses]\n",
    "    total_tokens = sum(count_tokens(t) for t in transformed_courses)\n",
    "    print(\n",
    "        f\"   âœ… Transformed: {len(transformed_courses)} courses ({total_tokens:,} total tokens)\"\n",
    "    )\n",
    "\n",
    "    # Show before/after transformation\n",
    "    if cleaned_courses and transformed_courses:\n",
    "        print(f\"\\n   ðŸ“„ Transformation example:\")\n",
    "        print(f\"      Before: {cleaned_courses[0].course_code} (Course object)\")\n",
    "        print(f\"      After (LLM-friendly text):\")\n",
    "        preview = transformed_courses[0].replace('\\n', '\\n      ')\n",
    "        print(f\"      {preview[:250]}...\")\n",
    "\n",
    "    # Step 4: Create Structured Views\n",
    "    print(\"\\n[Step 4/5] Creating structured catalog view...\")\n",
    "    catalog_view = await create_catalog_view()\n",
    "    catalog_tokens = count_tokens(catalog_view)\n",
    "    redis_client.set(\"course_catalog_view\", catalog_view)\n",
    "    redis_client.set(\"course_catalog_view:updated\", \"2024-09-30\")\n",
    "    print(f\"   âœ… Created catalog view ({catalog_tokens:,} tokens)\")\n",
    "    print(f\"   âœ… Cached in Redis\")\n",
    "\n",
    "    # Show catalog structure\n",
    "    print(f\"\\n   ðŸ“„ Catalog view structure:\")\n",
    "    catalog_preview = catalog_view[:300].replace('\\n', '\\n      ')\n",
    "    print(f\"      {catalog_preview}...\")\n",
    "\n",
    "    # Step 5: Store (in production, would also create embeddings and store in vector DB)\n",
    "    print(\"\\n[Step 5/5] Storing processed data...\")\n",
    "    for i, (course, text) in enumerate(zip(cleaned_courses, transformed_courses)):\n",
    "        key = f\"course:processed:{course.course_code}\"\n",
    "        redis_client.set(key, text)\n",
    "    print(f\"   âœ… Stored {len(cleaned_courses)} processed courses in Redis\")\n",
    "\n",
    "    # Show storage example\n",
    "    if cleaned_courses:\n",
    "        print(f\"\\n   ðŸ“„ Storage example:\")\n",
    "        print(f\"      Key: course:processed:{cleaned_courses[0].course_code}\")\n",
    "        print(f\"      Value: {transformed_courses[0][:100]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BATCH PROCESSING COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"\"\"\n",
    "Summary:\n",
    "- Courses processed: {len(cleaned_courses)}\n",
    "- Total tokens: {total_tokens:,}\n",
    "- Catalog view tokens: {catalog_tokens:,}\n",
    "- Storage: Redis\n",
    "- Next run: 2024-10-07 (weekly)\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Run the batch pipeline\n",
    "await batch_process_courses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e27e9e5b1bf93",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. Context is Data - and Data Requires Engineering**\n",
    "- Context isn't just \"data you feed to an LLM\"\n",
    "- It requires systematic transformation: Raw â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- Engineering discipline: requirements analysis, design decisions, quality metrics, testing\n",
    "\n",
    "**2. The Data Engineering Pipeline**\n",
    "- Extract: Get raw data from sources\n",
    "- Clean: Remove noise, fix inconsistencies\n",
    "- Transform: Structure for LLM consumption\n",
    "- Optimize: Reduce tokens, improve clarity\n",
    "- Store: Choose storage strategy (RAG, Views, Hybrid)\n",
    "\n",
    "**3. Three Engineering Approaches**\n",
    "- **RAG:** Semantic search for relevant data (good for specific queries)\n",
    "- **Structured Views:** Pre-computed summaries (excellent for overviews)\n",
    "- **Hybrid:** Combine both (best for production)\n",
    "\n",
    "**4. Chunking is an Engineering Decision**\n",
    "- **Don't chunk** if data is already small and complete (< 500 tokens)\n",
    "- **Do chunk** if documents are long (> 1000 tokens) or multi-topic\n",
    "- Four strategies: Document-Based, Fixed-Size, Semantic, Hierarchical\n",
    "- Choose based on YOUR data characteristics, query patterns, and constraints\n",
    "\n",
    "**5. Production Pipeline Architectures**\n",
    "- **Request-Time:** Process on-the-fly (simple, always fresh, higher latency)\n",
    "- **Batch:** Pre-process in batches (fast queries, can be stale)\n",
    "- **Event-Driven:** Process on changes (real-time, complex infrastructure)\n",
    "\n",
    "### The Engineering Mindset\n",
    "\n",
    "Every decision should be based on **YOUR specific requirements:**\n",
    "\n",
    "1. **Analyze YOUR data:** Size, structure, update frequency, topic density\n",
    "2. **Analyze YOUR queries:** Specific vs. overview, single vs. cross-section\n",
    "3. **Analyze YOUR constraints:** Token budget, latency, quality requirements\n",
    "4. **Make informed decisions:** Choose approaches that match YOUR needs\n",
    "5. **Measure and iterate:** Test with real queries, measure quality, optimize\n",
    "\n",
    "**Remember:** There is no \"best practice\" that works for everyone. Context engineering is about making deliberate, informed choices based on YOUR domain, application, and constraints.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63994fc42bf9a188",
   "metadata": {},
   "source": [
    "## Part 6: Quality Optimization - Measuring and Improving Context\n",
    "\n",
    "### The Systematic Optimization Process\n",
    "\n",
    "Now that you understand data engineering and production pipelines, let's learn how to systematically optimize context quality.\n",
    "\n",
    "**The Process:**\n",
    "```\n",
    "1. Define Quality Metrics (domain-specific)\n",
    "   â†“\n",
    "2. Establish Baseline (measure current performance)\n",
    "   â†“\n",
    "3. Experiment (try different approaches)\n",
    "   â†“\n",
    "4. Measure (compare against metrics)\n",
    "   â†“\n",
    "5. Iterate (refine based on results)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c89199eb6b0330",
   "metadata": {},
   "source": [
    "### Step 1: Define Quality Metrics for YOUR Domain\n",
    "\n",
    "**The Problem with Generic Metrics:**\n",
    "\n",
    "Don't aim for \"95% accuracy on benchmark X\" - that benchmark wasn't designed for YOUR domain.\n",
    "\n",
    "**DO this instead:** Define what \"quality\" means for YOUR domain, then measure it.\n",
    "\n",
    "### The Four Quality Dimensions\n",
    "\n",
    "Every context engineering solution should be evaluated across four dimensions:\n",
    "\n",
    "1. **Relevance** - Does context include information needed to answer the query?\n",
    "2. **Completeness** - Does context include ALL necessary information?\n",
    "3. **Efficiency** - Is context optimized for token usage?\n",
    "4. **Accuracy** - Is context factually correct and up-to-date?\n",
    "\n",
    "Different domains prioritize these differently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e5e35147ec43d",
   "metadata": {},
   "source": [
    "### Example: Quality Metrics for Redis University Course Advisor\n",
    "\n",
    "Let's define specific, measurable quality metrics for our course advisor domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be4e1aeb26cfc100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:41.303962Z",
     "iopub.status.busy": "2025-11-05T02:56:41.303775Z",
     "iopub.status.idle": "2025-11-05T02:56:41.308800Z",
     "shell.execute_reply": "2025-11-05T02:56:41.308130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUALITY METRICS FOR REDIS UNIVERSITY COURSE ADVISOR\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "Relevance:\n",
      "  Definition: Does context include courses relevant to the user's query?\n",
      "  Metric: % of queries where retrieved courses match query intent\n",
      "  How to measure: Manual review of 50 sample queries\n",
      "  Target: >90%\n",
      "  Why important: Irrelevant courses waste tokens and confuse users\n",
      "\n",
      "Completeness:\n",
      "  Definition: Does context include all information needed to answer?\n",
      "  Metric: % of responses that mention all prerequisites when asked\n",
      "  How to measure: Automated check: parse response for prerequisite mentions\n",
      "  Target: 100%\n",
      "  Why important: Missing prerequisites leads to hallucinations\n",
      "\n",
      "Efficiency:\n",
      "  Definition: Is context optimized for token usage?\n",
      "  Metric: Average tokens per query\n",
      "  How to measure: Token counter on all context strings\n",
      "  Target: <5,000 tokens\n",
      "  Why important: Exceeding budget increases cost and latency\n",
      "\n",
      "Accuracy:\n",
      "  Definition: Is context factually correct and up-to-date?\n",
      "  Metric: % of responses with correct course information\n",
      "  How to measure: Manual review against course database\n",
      "  Target: >95%\n",
      "  Why important: Incorrect information damages trust\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
     ]
    }
   ],
   "source": [
    "# Define domain-specific quality metrics\n",
    "\n",
    "quality_metrics = {\n",
    "    \"Relevance\": {\n",
    "        \"definition\": \"Does context include courses relevant to the user's query?\",\n",
    "        \"metric\": \"% of queries where retrieved courses match query intent\",\n",
    "        \"measurement\": \"Manual review of 50 sample queries\",\n",
    "        \"target\": \">90%\",\n",
    "        \"why_important\": \"Irrelevant courses waste tokens and confuse users\",\n",
    "    },\n",
    "    \"Completeness\": {\n",
    "        \"definition\": \"Does context include all information needed to answer?\",\n",
    "        \"metric\": \"% of responses that mention all prerequisites when asked\",\n",
    "        \"measurement\": \"Automated check: parse response for prerequisite mentions\",\n",
    "        \"target\": \"100%\",\n",
    "        \"why_important\": \"Missing prerequisites leads to hallucinations\",\n",
    "    },\n",
    "    \"Efficiency\": {\n",
    "        \"definition\": \"Is context optimized for token usage?\",\n",
    "        \"metric\": \"Average tokens per query\",\n",
    "        \"measurement\": \"Token counter on all context strings\",\n",
    "        \"target\": \"<5,000 tokens\",\n",
    "        \"why_important\": \"Exceeding budget increases cost and latency\",\n",
    "    },\n",
    "    \"Accuracy\": {\n",
    "        \"definition\": \"Is context factually correct and up-to-date?\",\n",
    "        \"metric\": \"% of responses with correct course information\",\n",
    "        \"measurement\": \"Manual review against course database\",\n",
    "        \"target\": \">95%\",\n",
    "        \"why_important\": \"Incorrect information damages trust\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"\"\"QUALITY METRICS FOR REDIS UNIVERSITY COURSE ADVISOR\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\"\"\n",
    ")\n",
    "for dimension, details in quality_metrics.items():\n",
    "    print(\n",
    "        f\"\"\"\n",
    "{dimension}:\n",
    "  Definition: {details['definition']}\n",
    "  Metric: {details['metric']}\n",
    "  How to measure: {details['measurement']}\n",
    "  Target: {details['target']}\n",
    "  Why important: {details['why_important']}\"\"\"\n",
    "    )\n",
    "print(\"\\n\" + \"â”\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5331fe7c2b23131",
   "metadata": {},
   "source": [
    "### Key Insight: Metrics Must Be Domain-Specific\n",
    "\n",
    "Notice how these metrics are specific to the course advisor domain:\n",
    "\n",
    "**Relevance metric:**\n",
    "- âŒ Generic: \"Cosine similarity > 0.8\"\n",
    "- âœ… Domain-specific: \"Retrieved courses match query intent\"\n",
    "\n",
    "**Completeness metric:**\n",
    "- âŒ Generic: \"Context includes top-5 search results\"\n",
    "- âœ… Domain-specific: \"All prerequisites mentioned when asked\"\n",
    "\n",
    "**Efficiency metric:**\n",
    "- âŒ Generic: \"Minimize tokens\"\n",
    "- âœ… Domain-specific: \"<5,000 tokens (fits our budget)\"\n",
    "\n",
    "**Accuracy metric:**\n",
    "- âŒ Generic: \"95% on MMLU benchmark\"\n",
    "- âœ… Domain-specific: \"Correct course information vs. database\"\n",
    "\n",
    "**Your metrics should reflect YOUR domain's requirements, not generic benchmarks.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9936b76be5dd4f",
   "metadata": {},
   "source": [
    "### Step 2-5: Baseline â†’ Experiment â†’ Measure â†’ Iterate\n",
    "\n",
    "Let's demonstrate the optimization process with a concrete example.\n",
    "\n",
    "**Scenario:** We want to optimize our hybrid approach (catalog overview + RAG) to meet all quality targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ab33636bc9a2a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:41.310619Z",
     "iopub.status.busy": "2025-11-05T02:56:41.310476Z",
     "iopub.status.idle": "2025-11-05T02:56:41.525136Z",
     "shell.execute_reply": "2025-11-05T02:56:41.523440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:41 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE (Hybrid Approach):\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "Tokens: 177\n",
      "\n",
      "Context:\n",
      "Redis University Course Catalog Overview:\n",
      "\n",
      "Computer Science Department:\n",
      "- RU101: Introduction to Redis Data Structures (Beginner, 4-6 hours)\n",
      "- RU201: Redis for Python Developers (Intermediate, 6-8 hours)\n",
      "- RU301: Vector Similarity Search with Redis (Advanced, 8-10 hours)\n",
      "\n",
      "Data Science Department:\n",
      "- RU401: Machine Learning with Redis (Intermediate, 10-12 hours)\n",
      "- RU402: Real-Time Analytics with Redis (Advanced, 8-10 hours)\n",
      "\n",
      "\n",
      "Detailed Course Information:\n",
      "CS007: Machine Learning (advanced)\n",
      "Description: Introduction to machine learning algorithms and applications. Supervised and unsupervised learning, neural networks.\n",
      "Prerequisites: None\n",
      "\n",
      "MATH026: Linear Algebra (intermediate)\n",
      "Description: Vector spaces, matrices, eigenvalues, and linear transformations. Essential for data science and engineering.\n",
      "Prerequisites: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Establish Baseline (Hybrid Approach from Part 3)\n",
    "\n",
    "# Sample query\n",
    "test_query = \"What machine learning courses are available for beginners?\"\n",
    "\n",
    "# Hybrid approach: Catalog overview + RAG\n",
    "catalog_overview = \"\"\"Redis University Course Catalog Overview:\n",
    "\n",
    "Computer Science Department:\n",
    "- RU101: Introduction to Redis Data Structures (Beginner, 4-6 hours)\n",
    "- RU201: Redis for Python Developers (Intermediate, 6-8 hours)\n",
    "- RU301: Vector Similarity Search with Redis (Advanced, 8-10 hours)\n",
    "\n",
    "Data Science Department:\n",
    "- RU401: Machine Learning with Redis (Intermediate, 10-12 hours)\n",
    "- RU402: Real-Time Analytics with Redis (Advanced, 8-10 hours)\n",
    "\"\"\"\n",
    "\n",
    "# RAG: Get specific courses\n",
    "rag_results = await course_manager.search_courses(test_query, limit=2)\n",
    "rag_context = \"\\n\\n\".join(\n",
    "    [\n",
    "        f\"\"\"{course.course_code}: {course.title} ({course.difficulty_level.value})\n",
    "Description: {course.description}\n",
    "Prerequisites: {', '.join([p.course_code for p in course.prerequisites]) if course.prerequisites else 'None'}\"\"\"\n",
    "        for course in rag_results\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combined context\n",
    "baseline_context = f\"\"\"{catalog_overview}\n",
    "\n",
    "Detailed Course Information:\n",
    "{rag_context}\"\"\"\n",
    "\n",
    "baseline_tokens = count_tokens(baseline_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"BASELINE (Hybrid Approach):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Tokens: {baseline_tokens:,}\n",
    "\n",
    "Context:\n",
    "{baseline_context}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9835f424b4bba43e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:41.527738Z",
     "iopub.status.busy": "2025-11-05T02:56:41.527499Z",
     "iopub.status.idle": "2025-11-05T02:56:41.532019Z",
     "shell.execute_reply": "2025-11-05T02:56:41.531161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT (Optimized Hybrid):\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "Tokens: 111\n",
      "\n",
      "Context:\n",
      "Redis University - Relevant Departments:\n",
      "\n",
      "Data Science:\n",
      "- RU401: Machine Learning with Redis (Intermediate)\n",
      "- RU402: Real-Time Analytics (Advanced)\n",
      "\n",
      "Computer Science:\n",
      "- RU301: Vector Search (Advanced)\n",
      "\n",
      "\n",
      "CS007: Machine Learning (advanced)\n",
      "Description: Introduction to machine learning algorithms and applications. Supervised and unsupervised learning, neural networks.\n",
      "Prerequisites: None\n",
      "\n",
      "MATH026: Linear Algebra (intermediate)\n",
      "Description: Vector spaces, matrices, eigenvalues, and linear transformations. Essential for data science and engineering.\n",
      "Prerequisites: None\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "Token Reduction: 66 tokens (37.3% reduction)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Experiment - Try optimized version\n",
    "\n",
    "# Optimization: Reduce catalog overview to just relevant departments\n",
    "optimized_catalog = \"\"\"Redis University - Relevant Departments:\n",
    "\n",
    "Data Science:\n",
    "- RU401: Machine Learning with Redis (Intermediate)\n",
    "- RU402: Real-Time Analytics (Advanced)\n",
    "\n",
    "Computer Science:\n",
    "- RU301: Vector Search (Advanced)\n",
    "\"\"\"\n",
    "\n",
    "optimized_context = f\"\"\"{optimized_catalog}\n",
    "\n",
    "{rag_context}\"\"\"\n",
    "\n",
    "optimized_tokens = count_tokens(optimized_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"EXPERIMENT (Optimized Hybrid):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Tokens: {optimized_tokens:,}\n",
    "\n",
    "Context:\n",
    "{optimized_context}\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Token Reduction: {baseline_tokens - optimized_tokens:,} tokens ({((baseline_tokens - optimized_tokens) / baseline_tokens * 100):.1f}% reduction)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e9f6bf32872925d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T02:56:41.533884Z",
     "iopub.status.busy": "2025-11-05T02:56:41.533727Z",
     "iopub.status.idle": "2025-11-05T02:56:48.704484Z",
     "shell.execute_reply": "2025-11-05T02:56:48.702108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:56:48 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE RESPONSE:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "In the Redis University course catalog, there isn't a specific machine learning course labeled as \"beginner.\" However, if you're looking to start with foundational knowledge that could be beneficial for machine learning, you might consider taking courses that cover essential prerequisites or related topics. For example, MATH026: Linear Algebra (intermediate) could be a good starting point, as linear algebra is fundamental to understanding many machine learning algorithms. \n",
      "\n",
      "For a direct introduction to machine learning concepts, you might consider CS007: Machine Learning (advanced), although it is labeled as advanced, it covers introductory topics in machine learning algorithms and applications. If you're comfortable with the challenge, it could be a valuable course to take.\n",
      "\n",
      "OPTIMIZED RESPONSE:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "At Redis University, the available machine learning course is CS007: Machine Learning, which is an advanced course. Unfortunately, there are no beginner-level machine learning courses currently offered. However, if you're interested in building a foundation for machine learning, you might consider starting with MATH026: Linear Algebra, as it covers essential mathematical concepts that are fundamental to understanding machine learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Measure - Compare responses\n",
    "\n",
    "# Baseline response\n",
    "messages_baseline = [\n",
    "    SystemMessage(content=f\"You are a Redis University course advisor.\\n\\n{baseline_context}\"),\n",
    "    HumanMessage(content=test_query),\n",
    "]\n",
    "response_baseline = llm.invoke(messages_baseline)\n",
    "\n",
    "# Optimized response\n",
    "messages_optimized = [\n",
    "    SystemMessage(\n",
    "        content=f\"You are a Redis University course advisor.\\n\\n{optimized_context}\"\n",
    "    ),\n",
    "    HumanMessage(content=test_query),\n",
    "]\n",
    "response_optimized = llm.invoke(messages_optimized)\n",
    "\n",
    "print(\n",
    "    f\"\"\"BASELINE RESPONSE:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "{response_baseline.content}\n",
    "\n",
    "OPTIMIZED RESPONSE:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "{response_optimized.content}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73762d987cdf036d",
   "metadata": {},
   "source": [
    "### Step 5: Iterate - Refine Based on Results\n",
    "\n",
    "Based on the measurements:\n",
    "\n",
    "**Quality Assessment:**\n",
    "- âœ… **Relevance:** Both approaches retrieve relevant ML courses\n",
    "- âœ… **Completeness:** Both mention prerequisites and difficulty levels\n",
    "- âœ… **Efficiency:** Optimized version uses fewer tokens ({optimized_tokens} vs {baseline_tokens})\n",
    "- âœ… **Accuracy:** Both provide correct course information\n",
    "\n",
    "**Decision:** The optimized hybrid approach meets all quality targets while reducing token usage.\n",
    "\n",
    "**Next Iteration:** Test with more queries to ensure consistency across different query types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618ae42e227c5d1",
   "metadata": {},
   "source": [
    "### Key Takeaways: Quality Optimization\n",
    "\n",
    "1. **Define Domain-Specific Metrics** - Don't rely on generic benchmarks\n",
    "2. **Measure Systematically** - Baseline â†’ Experiment â†’ Measure â†’ Iterate\n",
    "3. **Balance Trade-offs** - Relevance vs. Efficiency, Completeness vs. Token Budget\n",
    "4. **Test Before Production** - Validate with real queries from your domain\n",
    "5. **Iterate Continuously** - Quality optimization is ongoing, not one-time\n",
    "\n",
    "**The Engineering Mindset:**\n",
    "- Context quality is measurable\n",
    "- Optimization is systematic, not guesswork\n",
    "- Domain-specific metrics matter more than generic benchmarks\n",
    "- Testing and iteration are essential\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76bb463b4f96e72",
   "metadata": {},
   "source": [
    "## ðŸ“ Summary\n",
    "\n",
    "You've mastered production-ready context engineering:\n",
    "\n",
    "**Part 1: The Engineering Mindset**\n",
    "- âœ… Context is data requiring engineering discipline\n",
    "- âœ… Naive approaches fail in production\n",
    "- âœ… Engineering mindset: Requirements â†’ Transformation â†’ Quality â†’ Testing\n",
    "\n",
    "**Part 2: Data Engineering Pipeline**\n",
    "- âœ… Extract â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- âœ… Concrete examples with course data\n",
    "- âœ… Token optimization techniques\n",
    "\n",
    "**Part 3: Engineering Approaches**\n",
    "- âœ… RAG (Semantic Search)\n",
    "- âœ… Structured Views (Pre-Computed Summaries)\n",
    "- âœ… Hybrid (Best of Both Worlds)\n",
    "- âœ… Decision framework for choosing approaches\n",
    "\n",
    "**Part 4: Chunking Strategies**\n",
    "- âœ… When to chunk (critical first question)\n",
    "- âœ… Four strategies with LangChain integration\n",
    "- âœ… Trade-offs and decision criteria\n",
    "\n",
    "**Part 5: Production Pipeline Architectures**\n",
    "- âœ… Request-Time, Batch, Event-Driven\n",
    "- âœ… Batch processing example with data\n",
    "- âœ… Decision framework for architecture selection\n",
    "\n",
    "**Part 6: Quality Optimization**\n",
    "- âœ… Domain-specific quality metrics\n",
    "- âœ… Systematic optimization process\n",
    "- âœ… Baseline â†’ Experiment â†’ Measure â†’ Iterate\n",
    "\n",
    "**You're now ready to engineer production-ready context for any domain!** ðŸŽ‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e80246eb4632a",
   "metadata": {},
   "source": [
    "## ðŸš€ What's Next?\n",
    "\n",
    "### Section 3: Memory Systems for Context Engineering\n",
    "\n",
    "Now that you can engineer high-quality retrieved context, you'll learn to manage conversation context:\n",
    "- **Working Memory:** Track conversation history within a session\n",
    "- **Long-term Memory:** Remember user preferences across sessions\n",
    "- **LangGraph Integration:** Manage stateful workflows with checkpointing\n",
    "- **Redis Agent Memory Server:** Automatic memory extraction and retrieval\n",
    "\n",
    "### Section 4: Tool Use and Agents\n",
    "\n",
    "After adding memory, you'll build complete autonomous agents:\n",
    "- **Tool Calling:** Let the AI use functions (search, enroll, check prerequisites)\n",
    "- **LangGraph State Management:** Orchestrate complex multi-step workflows\n",
    "- **Agent Reasoning:** Plan and execute multi-step tasks\n",
    "- **Production Patterns:** Error handling, retries, and monitoring\n",
    "\n",
    "```\n",
    "Section 1: Context Engineering Fundamentals\n",
    "    â†“\n",
    "Section 2, NB1: RAG Fundamentals\n",
    "    â†“\n",
    "Section 2, NB2: Engineering Context for Production â† You are here\n",
    "    â†“\n",
    "Section 3: Memory Systems for Context Engineering â† Next\n",
    "    â†“\n",
    "Section 4: Tool Use and Agents (Complete System)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "**Chunking Strategies:**\n",
    "- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- [LlamaIndex Node Parsers](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/)\n",
    "\n",
    "**Data Engineering for LLMs:**\n",
    "- [OpenAI Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "\n",
    "**Vector Databases:**\n",
    "- [Redis Vector Search Documentation](https://redis.io/docs/stack/search/reference/vectors/)\n",
    "- [RedisVL Python Library](https://github.com/RedisVentures/redisvl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682e2447c50f133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Context Validation & Health Monitoring\n",
    "\n",
    "## Learning Objectives (45 minutes)\n",
    "By the end of this notebook, you will be able to:\n",
    "1. **Understand** context quality issues that degrade agent performance\n",
    "2. **Implement** automated context validation and quality checks\n",
    "3. **Design** health monitoring systems for production context management\n",
    "4. **Create** alerting and remediation strategies for context problems\n",
    "5. **Measure** context quality metrics and performance indicators\n",
    "\n",
    "## Prerequisites\n",
    "- Completed all previous notebooks in Section 5\n",
    "- Understanding of production monitoring concepts\n",
    "- Familiarity with your complete Redis University system\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Context Validation & Health Monitoring** ensures your context engineering systems maintain high quality and performance in production. Just like monitoring application health, context health requires continuous validation and proactive maintenance.\n",
    "\n",
    "### Context Quality Problems\n",
    "\n",
    "**Common Issues in Production:**\n",
    "- **Context Drift**: Gradual degradation of context relevance\n",
    "- **Information Staleness**: Outdated information affecting decisions\n",
    "- **Contradiction Accumulation**: Unresolved conflicts building up\n",
    "- **Memory Bloat**: Excessive context causing performance issues\n",
    "- **Source Reliability Decay**: Previously reliable sources becoming unreliable\n",
    "- **Semantic Inconsistency**: Context that doesn't make logical sense\n",
    "\n",
    "### Our Solution: Comprehensive Health Monitoring\n",
    "\n",
    "We'll implement:\n",
    "1. **Quality metrics** for different aspects of context health\n",
    "2. **Automated validation** to detect problems early\n",
    "3. **Health dashboards** for monitoring context systems\n",
    "4. **Alerting systems** for proactive problem detection\n",
    "5. **Remediation strategies** for common context issues\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "import statistics\n",
    "import uuid\n",
    "from collections import defaultdict, deque\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "\n",
    "print(\"🔧 Environment Setup\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Redis URL: {REDIS_URL}\")\n",
    "print(f\"Agent Memory URL: {AGENT_MEMORY_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "try:\n",
    "    import redis\n",
    "    from redis_context_course.models import StudentProfile\n",
    "    from redis_context_course.course_manager import CourseManager\n",
    "    from redis_context_course.redis_config import redis_config\n",
    "    \n",
    "    # Redis connection\n",
    "    redis_client = redis.from_url(REDIS_URL)\n",
    "    if redis_config.health_check():\n",
    "        print(\"✅ Redis connection healthy\")\n",
    "    else:\n",
    "        print(\"❌ Redis connection failed\")\n",
    "    \n",
    "    print(\"✅ Core modules imported successfully\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    print(\"Please ensure you've completed the setup from previous sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Quality Framework\n",
    "\n",
    "Let's define a comprehensive framework for measuring context quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextQualityMetric(Enum):\n",
    "    \"\"\"Different aspects of context quality to measure.\"\"\"\n",
    "    RELEVANCE = \"relevance\"              # How relevant is context to current needs\n",
    "    FRESHNESS = \"freshness\"              # How recent/up-to-date is the information\n",
    "    CONSISTENCY = \"consistency\"          # Are there contradictions in context\n",
    "    COMPLETENESS = \"completeness\"        # Is sufficient context available\n",
    "    ACCURACY = \"accuracy\"                # Is the context information correct\n",
    "    COHERENCE = \"coherence\"              # Does context make logical sense together\n",
    "    EFFICIENCY = \"efficiency\"            # Context size vs. information value\n",
    "    DIVERSITY = \"diversity\"              # Variety of information sources\n",
    "\n",
    "class HealthStatus(Enum):\n",
    "    \"\"\"Overall health status levels.\"\"\"\n",
    "    EXCELLENT = \"excellent\"    # 90-100% quality\n",
    "    GOOD = \"good\"              # 75-89% quality\n",
    "    WARNING = \"warning\"        # 60-74% quality\n",
    "    CRITICAL = \"critical\"      # 40-59% quality\n",
    "    FAILING = \"failing\"        # 0-39% quality\n",
    "\n",
    "@dataclass\n",
    "class QualityMeasurement:\n",
    "    \"\"\"Individual quality measurement.\"\"\"\n",
    "    metric: ContextQualityMetric\n",
    "    score: float  # 0.0 to 1.0\n",
    "    timestamp: datetime\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "    issues_detected: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def get_status(self) -> HealthStatus:\n",
    "        \"\"\"Convert score to health status.\"\"\"\n",
    "        if self.score >= 0.9:\n",
    "            return HealthStatus.EXCELLENT\n",
    "        elif self.score >= 0.75:\n",
    "            return HealthStatus.GOOD\n",
    "        elif self.score >= 0.6:\n",
    "            return HealthStatus.WARNING\n",
    "        elif self.score >= 0.4:\n",
    "            return HealthStatus.CRITICAL\n",
    "        else:\n",
    "            return HealthStatus.FAILING\n",
    "\n",
    "@dataclass\n",
    "class ContextHealthReport:\n",
    "    \"\"\"Comprehensive context health report.\"\"\"\n",
    "    timestamp: datetime\n",
    "    student_id: str\n",
    "    overall_score: float\n",
    "    overall_status: HealthStatus\n",
    "    metric_scores: Dict[ContextQualityMetric, QualityMeasurement]\n",
    "    recommendations: List[str] = field(default_factory=list)\n",
    "    alerts: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        \"\"\"Get a human-readable summary.\"\"\"\n",
    "        status_emoji = {\n",
    "            HealthStatus.EXCELLENT: \"🟢\",\n",
    "            HealthStatus.GOOD: \"🟡\",\n",
    "            HealthStatus.WARNING: \"🟠\",\n",
    "            HealthStatus.CRITICAL: \"🔴\",\n",
    "            HealthStatus.FAILING: \"💀\"\n",
    "        }\n",
    "        \n",
    "        emoji = status_emoji.get(self.overall_status, \"❓\")\n",
    "        return f\"{emoji} Context Health: {self.overall_status.value.title()} ({self.overall_score:.1%})\"\n",
    "\n",
    "class ContextValidator:\n",
    "    \"\"\"Validates context quality across multiple dimensions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_history = deque(maxlen=100)  # Keep last 100 validations\n",
    "        self.quality_thresholds = {\n",
    "            ContextQualityMetric.RELEVANCE: 0.7,\n",
    "            ContextQualityMetric.FRESHNESS: 0.6,\n",
    "            ContextQualityMetric.CONSISTENCY: 0.8,\n",
    "            ContextQualityMetric.COMPLETENESS: 0.7,\n",
    "            ContextQualityMetric.ACCURACY: 0.9,\n",
    "            ContextQualityMetric.COHERENCE: 0.75,\n",
    "            ContextQualityMetric.EFFICIENCY: 0.6,\n",
    "            ContextQualityMetric.DIVERSITY: 0.5\n",
    "        }\n",
    "    \n",
    "    async def validate_context_health(self, \n",
    "                                    context_items: List[Any],\n",
    "                                    student_id: str,\n",
    "                                    query_context: str = \"\") -> ContextHealthReport:\n",
    "        \"\"\"Perform comprehensive context health validation.\"\"\"\n",
    "        \n",
    "        timestamp = datetime.now()\n",
    "        metric_scores = {}\n",
    "        \n",
    "        # Measure each quality metric\n",
    "        for metric in ContextQualityMetric:\n",
    "            measurement = await self._measure_quality_metric(\n",
    "                metric, context_items, query_context\n",
    "            )\n",
    "            metric_scores[metric] = measurement\n",
    "        \n",
    "        # Calculate overall score (weighted average)\n",
    "        weights = {\n",
    "            ContextQualityMetric.RELEVANCE: 0.2,\n",
    "            ContextQualityMetric.FRESHNESS: 0.15,\n",
    "            ContextQualityMetric.CONSISTENCY: 0.15,\n",
    "            ContextQualityMetric.COMPLETENESS: 0.15,\n",
    "            ContextQualityMetric.ACCURACY: 0.15,\n",
    "            ContextQualityMetric.COHERENCE: 0.1,\n",
    "            ContextQualityMetric.EFFICIENCY: 0.05,\n",
    "            ContextQualityMetric.DIVERSITY: 0.05\n",
    "        }\n",
    "        \n",
    "        overall_score = sum(\n",
    "            weights[metric] * measurement.score \n",
    "            for metric, measurement in metric_scores.items()\n",
    "        )\n",
    "        \n",
    "        # Determine overall status\n",
    "        overall_status = self._score_to_status(overall_score)\n",
    "        \n",
    "        # Generate recommendations and alerts\n",
    "        recommendations = self._generate_recommendations(metric_scores)\n",
    "        alerts = self._generate_alerts(metric_scores)\n",
    "        \n",
    "        # Create health report\n",
    "        report = ContextHealthReport(\n",
    "            timestamp=timestamp,\n",
    "            student_id=student_id,\n",
    "            overall_score=overall_score,\n",
    "            overall_status=overall_status,\n",
    "            metric_scores=metric_scores,\n",
    "            recommendations=recommendations,\n",
    "            alerts=alerts\n",
    "        )\n",
    "        \n",
    "        # Store in validation history\n",
    "        self.validation_history.append(report)\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    async def _measure_quality_metric(self, \n",
    "                                    metric: ContextQualityMetric,\n",
    "                                    context_items: List[Any],\n",
    "                                    query_context: str) -> QualityMeasurement:\n",
    "        \"\"\"Measure a specific quality metric.\"\"\"\n",
    "        \n",
    "        if metric == ContextQualityMetric.RELEVANCE:\n",
    "            return self._measure_relevance(context_items, query_context)\n",
    "        elif metric == ContextQualityMetric.FRESHNESS:\n",
    "            return self._measure_freshness(context_items)\n",
    "        elif metric == ContextQualityMetric.CONSISTENCY:\n",
    "            return self._measure_consistency(context_items)\n",
    "        elif metric == ContextQualityMetric.COMPLETENESS:\n",
    "            return self._measure_completeness(context_items, query_context)\n",
    "        elif metric == ContextQualityMetric.ACCURACY:\n",
    "            return self._measure_accuracy(context_items)\n",
    "        elif metric == ContextQualityMetric.COHERENCE:\n",
    "            return self._measure_coherence(context_items)\n",
    "        elif metric == ContextQualityMetric.EFFICIENCY:\n",
    "            return self._measure_efficiency(context_items)\n",
    "        elif metric == ContextQualityMetric.DIVERSITY:\n",
    "            return self._measure_diversity(context_items)\n",
    "        else:\n",
    "            # Default measurement\n",
    "            return QualityMeasurement(\n",
    "                metric=metric,\n",
    "                score=0.5,\n",
    "                timestamp=datetime.now(),\n",
    "                details={\"error\": \"Unknown metric\"}\n",
    "            )\n",
    "    \n",
    "    def _measure_relevance(self, context_items: List[Any], query_context: str) -> QualityMeasurement:\n",
    "        \"\"\"Measure how relevant context is to the current query.\"\"\"\n",
    "        if not context_items or not query_context:\n",
    "            return QualityMeasurement(\n",
    "                metric=ContextQualityMetric.RELEVANCE,\n",
    "                score=0.0,\n",
    "                timestamp=datetime.now(),\n",
    "                issues_detected=[\"No context or query provided\"]\n",
    "            )\n",
    "        \n",
    "        # Simple relevance scoring based on keyword overlap\n",
    "        query_words = set(query_context.lower().split())\n",
    "        relevance_scores = []\n",
    "        \n",
    "        for item in context_items:\n",
    "            # Handle different item types\n",
    "            if hasattr(item, 'content'):\n",
    "                content = item.content\n",
    "            elif isinstance(item, str):\n",
    "                content = item\n",
    "            else:\n",
    "                content = str(item)\n",
    "            \n",
    "            item_words = set(content.lower().split())\n",
    "            \n",
    "            if len(query_words) > 0:\n",
    "                overlap = len(query_words & item_words)\n",
    "                relevance = overlap / len(query_words)\n",
    "                relevance_scores.append(relevance)\n",
    "        \n",
    "        if relevance_scores:\n",
    "            avg_relevance = statistics.mean(relevance_scores)\n",
    "            max_relevance = max(relevance_scores)\n",
    "        else:\n",
    "            avg_relevance = 0.0\n",
    "            max_relevance = 0.0\n",
    "        \n",
    "        # Score is weighted average of mean and max relevance\n",
    "        score = (avg_relevance * 0.7) + (max_relevance * 0.3)\n",
    "        \n",
    "        issues = []\n",
    "        if score < 0.3:\n",
    "            issues.append(\"Low relevance to query context\")\n",
    "        if max_relevance < 0.5:\n",
    "            issues.append(\"No highly relevant context items found\")\n",
    "        \n",
    "        return QualityMeasurement(\n",
    "            metric=ContextQualityMetric.RELEVANCE,\n",
    "            score=min(score, 1.0),\n",
    "            timestamp=datetime.now(),\n",
    "            details={\n",
    "                \"avg_relevance\": avg_relevance,\n",
    "                \"max_relevance\": max_relevance,\n",
    "                \"items_analyzed\": len(context_items)\n",
    "            },\n",
    "            issues_detected=issues\n",
    "        )\n",
    "    \n",
    "    def _measure_freshness(self, context_items: List[Any]) -> QualityMeasurement:\n",
    "        \"\"\"Measure how fresh/recent the context information is.\"\"\"\n",
    "        if not context_items:\n",
    "            return QualityMeasurement(\n",
    "                metric=ContextQualityMetric.FRESHNESS,\n",
    "                score=0.0,\n",
    "                timestamp=datetime.now(),\n",
    "                issues_detected=[\"No context items to analyze\"]\n",
    "            )\n",
    "        \n",
    "        now = datetime.now()\n",
    "        freshness_scores = []\n",
    "        \n",
    "        for item in context_items:\n",
    "            # Try to get timestamp from item\n",
    "            if hasattr(item, 'timestamp'):\n",
    "                item_time = item.timestamp\n",
    "            elif hasattr(item, 'created_at'):\n",
    "                item_time = item.created_at\n",
    "            else:\n",
    "                # Assume recent if no timestamp\n",
    "                item_time = now - timedelta(hours=1)\n",
    "            \n",
    "            # Calculate age in hours\n",
    "            age_hours = (now - item_time).total_seconds() / 3600\n",
    "            \n",
    "            # Freshness score: exponential decay with 24-hour half-life\n",
    "            import math\n",
    "            freshness = math.exp(-age_hours / 24)\n",
    "            freshness_scores.append(freshness)\n",
    "        \n",
    "        avg_freshness = statistics.mean(freshness_scores)\n",
    "        oldest_age = max((now - (getattr(item, 'timestamp', now))).total_seconds() / 3600 \n",
    "                        for item in context_items)\n",
    "        \n",
    "        issues = []\n",
    "        if avg_freshness < 0.3:\n",
    "            issues.append(\"Context is generally stale\")\n",
    "        if oldest_age > 168:  # 1 week\n",
    "            issues.append(f\"Some context is very old ({oldest_age:.0f} hours)\")\n",
    "        \n",
    "        return QualityMeasurement(\n",
    "            metric=ContextQualityMetric.FRESHNESS,\n",
    "            score=avg_freshness,\n",
    "            timestamp=datetime.now(),\n",
    "            details={\n",
    "                \"avg_freshness\": avg_freshness,\n",
    "                \"oldest_age_hours\": oldest_age,\n",
    "                \"items_analyzed\": len(context_items)\n",
    "            },\n",
    "            issues_detected=issues\n",
    "        )\n",
    "    \n",
    "    def _score_to_status(self, score: float) -> HealthStatus:\n",
    "        \"\"\"Convert numeric score to health status.\"\"\"\n",
    "        if score >= 0.9:\n",
    "            return HealthStatus.EXCELLENT\n",
    "        elif score >= 0.75:\n",
    "            return HealthStatus.GOOD\n",
    "        elif score >= 0.6:\n",
    "            return HealthStatus.WARNING\n",
    "        elif score >= 0.4:\n",
    "            return HealthStatus.CRITICAL\n",
    "        else:\n",
    "            return HealthStatus.FAILING\n",
    "\n",
    "# Initialize the context validator\n",
    "context_validator = ContextValidator()\n",
    "\n",
    "print(\"✅ Context validation framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Quality Measurement Methods\n",
    "\n",
    "Let's implement the remaining quality measurement methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add remaining quality measurement methods to ContextValidator\n",
    "import math\n",
    "\n",
    "def _measure_consistency(self, context_items: List[Any]) -> QualityMeasurement:\n",
    "    \"\"\"Measure consistency - detect contradictions in context.\"\"\"\n",
    "    if len(context_items) < 2:\n",
    "        return QualityMeasurement(\n",
    "            metric=ContextQualityMetric.CONSISTENCY,\n",
    "            score=1.0,  # No contradictions possible with <2 items\n",
    "            timestamp=datetime.now(),\n",
    "            details={\"items_analyzed\": len(context_items)}\n",
    "        )\n",
    "    \n",
    "    # Simple contradiction detection based on opposing keywords\n",
    "    contradiction_patterns = [\n",
    "        ([\"online\", \"remote\"], [\"in-person\", \"on-campus\"]),\n",
    "        ([\"morning\", \"early\"], [\"evening\", \"late\", \"night\"]),\n",
    "        ([\"easy\", \"simple\"], [\"difficult\", \"challenging\", \"hard\"]),\n",
    "        ([\"prefer\", \"like\", \"want\"], [\"dislike\", \"avoid\", \"hate\"]),\n",
    "        ([\"completed\", \"finished\"], [\"failed\", \"dropped\", \"incomplete\"])\n",
    "    ]\n",
    "    \n",
    "    contradictions_found = 0\n",
    "    total_comparisons = 0\n",
    "    issues = []\n",
    "    \n",
    "    # Get content from items\n",
    "    contents = []\n",
    "    for item in context_items:\n",
    "        if hasattr(item, 'content'):\n",
    "            contents.append(item.content.lower())\n",
    "        elif isinstance(item, str):\n",
    "            contents.append(item.lower())\n",
    "        else:\n",
    "            contents.append(str(item).lower())\n",
    "    \n",
    "    # Check for contradictions\n",
    "    for positive_words, negative_words in contradiction_patterns:\n",
    "        positive_items = [content for content in contents \n",
    "                         if any(word in content for word in positive_words)]\n",
    "        negative_items = [content for content in contents \n",
    "                         if any(word in content for word in negative_words)]\n",
    "        \n",
    "        if positive_items and negative_items:\n",
    "            contradictions_found += 1\n",
    "            issues.append(f\"Contradiction: {positive_words} vs {negative_words}\")\n",
    "        \n",
    "        total_comparisons += 1\n",
    "    \n",
    "    # Calculate consistency score\n",
    "    if total_comparisons > 0:\n",
    "        consistency_score = 1.0 - (contradictions_found / total_comparisons)\n",
    "    else:\n",
    "        consistency_score = 1.0\n",
    "    \n",
    "    return QualityMeasurement(\n",
    "        metric=ContextQualityMetric.CONSISTENCY,\n",
    "        score=consistency_score,\n",
    "        timestamp=datetime.now(),\n",
    "        details={\n",
    "            \"contradictions_found\": contradictions_found,\n",
    "            \"total_comparisons\": total_comparisons,\n",
    "            \"items_analyzed\": len(context_items)\n",
    "        },\n",
    "        issues_detected=issues\n",
    "    )\n",
    "\n",
    "def _measure_completeness(self, context_items: List[Any], query_context: str) -> QualityMeasurement:\n",
    "    \"\"\"Measure completeness - is sufficient context available.\"\"\"\n",
    "    # Define expected context categories for academic queries\n",
    "    expected_categories = {\n",
    "        \"academic_progress\": [\"completed\", \"grade\", \"gpa\", \"credit\"],\n",
    "        \"preferences\": [\"prefer\", \"like\", \"want\", \"format\"],\n",
    "        \"schedule\": [\"time\", \"schedule\", \"morning\", \"evening\"],\n",
    "        \"career_goals\": [\"career\", \"job\", \"goal\", \"industry\"],\n",
    "        \"course_info\": [\"course\", \"class\", \"prerequisite\", \"requirement\"]\n",
    "    }\n",
    "    \n",
    "    # Check which categories are present\n",
    "    categories_present = set()\n",
    "    \n",
    "    for item in context_items:\n",
    "        content = getattr(item, 'content', str(item)).lower()\n",
    "        \n",
    "        for category, keywords in expected_categories.items():\n",
    "            if any(keyword in content for keyword in keywords):\n",
    "                categories_present.add(category)\n",
    "    \n",
    "    # Calculate completeness based on query type\n",
    "    query_lower = query_context.lower()\n",
    "    required_categories = set()\n",
    "    \n",
    "    if any(word in query_lower for word in [\"course\", \"class\", \"take\"]):\n",
    "        required_categories.update([\"academic_progress\", \"preferences\", \"course_info\"])\n",
    "    if any(word in query_lower for word in [\"schedule\", \"time\", \"when\"]):\n",
    "        required_categories.add(\"schedule\")\n",
    "    if any(word in query_lower for word in [\"career\", \"job\", \"future\"]):\n",
    "        required_categories.add(\"career_goals\")\n",
    "    \n",
    "    if not required_categories:\n",
    "        required_categories = {\"academic_progress\", \"preferences\"}  # Default minimum\n",
    "    \n",
    "    # Calculate completeness score\n",
    "    if required_categories:\n",
    "        completeness_score = len(categories_present & required_categories) / len(required_categories)\n",
    "    else:\n",
    "        completeness_score = 1.0\n",
    "    \n",
    "    missing_categories = required_categories - categories_present\n",
    "    issues = [f\"Missing {category} context\" for category in missing_categories]\n",
    "    \n",
    "    return QualityMeasurement(\n",
    "        metric=ContextQualityMetric.COMPLETENESS,\n",
    "        score=completeness_score,\n",
    "        timestamp=datetime.now(),\n",
    "        details={\n",
    "            \"categories_present\": list(categories_present),\n",
    "            \"required_categories\": list(required_categories),\n",
    "            \"missing_categories\": list(missing_categories)\n",
    "        },\n",
    "        issues_detected=issues\n",
    "    )\n",
    "\n",
    "def _measure_accuracy(self, context_items: List[Any]) -> QualityMeasurement:\n",
    "    \"\"\"Measure accuracy - detect potentially incorrect information.\"\"\"\n",
    "    # Simple accuracy checks for academic context\n",
    "    accuracy_issues = []\n",
    "    total_checks = 0\n",
    "    failed_checks = 0\n",
    "    \n",
    "    for item in context_items:\n",
    "        content = getattr(item, 'content', str(item)).lower()\n",
    "        \n",
    "        # Check for impossible GPA values\n",
    "        if \"gpa\" in content:\n",
    "            total_checks += 1\n",
    "            import re\n",
    "            gpa_matches = re.findall(r'gpa[:\\s]*([0-9.]+)', content)\n",
    "            for gpa_str in gpa_matches:\n",
    "                try:\n",
    "                    gpa = float(gpa_str)\n",
    "                    if gpa > 4.0 or gpa < 0.0:\n",
    "                        failed_checks += 1\n",
    "                        accuracy_issues.append(f\"Invalid GPA value: {gpa}\")\n",
    "                except ValueError:\n",
    "                    failed_checks += 1\n",
    "                    accuracy_issues.append(f\"Invalid GPA format: {gpa_str}\")\n",
    "        \n",
    "        # Check for impossible course codes\n",
    "        course_matches = re.findall(r'[A-Z]{2,4}\\d{3,4}', content.upper())\n",
    "        if course_matches:\n",
    "            total_checks += 1\n",
    "            for course_code in course_matches:\n",
    "                # Basic validation - course numbers should be reasonable\n",
    "                number_part = re.findall(r'\\d+', course_code)\n",
    "                if number_part:\n",
    "                    course_num = int(number_part[0])\n",
    "                    if course_num > 999 or course_num < 100:\n",
    "                        failed_checks += 1\n",
    "                        accuracy_issues.append(f\"Unusual course number: {course_code}\")\n",
    "    \n",
    "    # Calculate accuracy score\n",
    "    if total_checks > 0:\n",
    "        accuracy_score = 1.0 - (failed_checks / total_checks)\n",
    "    else:\n",
    "        accuracy_score = 0.9  # Assume good if no specific checks possible\n",
    "    \n",
    "    return QualityMeasurement(\n",
    "        metric=ContextQualityMetric.ACCURACY,\n",
    "        score=accuracy_score,\n",
    "        timestamp=datetime.now(),\n",
    "        details={\n",
    "            \"total_checks\": total_checks,\n",
    "            \"failed_checks\": failed_checks,\n",
    "            \"items_analyzed\": len(context_items)\n",
    "        },\n",
    "        issues_detected=accuracy_issues\n",
    "    )\n",
    "\n",
    "def _measure_coherence(self, context_items: List[Any]) -> QualityMeasurement:\n",
    "    \"\"\"Measure coherence - does context make logical sense together.\"\"\"\n",
    "    if len(context_items) < 2:\n",
    "        return QualityMeasurement(\n",
    "            metric=ContextQualityMetric.COHERENCE,\n",
    "            score=1.0,\n",
    "            timestamp=datetime.now(),\n",
    "            details={\"items_analyzed\": len(context_items)}\n",
    "        )\n",
    "    \n",
    "    # Simple coherence checks\n",
    "    coherence_issues = []\n",
    "    \n",
    "    # Check for temporal coherence (events in logical order)\n",
    "    academic_events = []\n",
    "    for item in context_items:\n",
    "        content = getattr(item, 'content', str(item)).lower()\n",
    "        timestamp = getattr(item, 'timestamp', datetime.now())\n",
    "        \n",
    "        if \"completed\" in content:\n",
    "            academic_events.append((\"completed\", timestamp, content))\n",
    "        elif \"enrolled\" in content or \"taking\" in content:\n",
    "            academic_events.append((\"enrolled\", timestamp, content))\n",
    "        elif \"planning\" in content or \"will take\" in content:\n",
    "            academic_events.append((\"planning\", timestamp, content))\n",
    "    \n",
    "    # Check for logical progression\n",
    "    event_order = {\"completed\": 1, \"enrolled\": 2, \"planning\": 3}\n",
    "    coherence_score = 1.0\n",
    "    \n",
    "    for i in range(len(academic_events) - 1):\n",
    "        current_event = academic_events[i]\n",
    "        next_event = academic_events[i + 1]\n",
    "        \n",
    "        current_order = event_order.get(current_event[0], 2)\n",
    "        next_order = event_order.get(next_event[0], 2)\n",
    "        \n",
    "        # If later event has earlier logical order, it's incoherent\n",
    "        if current_event[1] < next_event[1] and current_order > next_order:\n",
    "            coherence_score -= 0.2\n",
    "            coherence_issues.append(f\"Temporal incoherence: {current_event[0]} after {next_event[0]}\")\n",
    "    \n",
    "    coherence_score = max(coherence_score, 0.0)\n",
    "    \n",
    "    return QualityMeasurement(\n",
    "        metric=ContextQualityMetric.COHERENCE,\n",
    "        score=coherence_score,\n",
    "        timestamp=datetime.now(),\n",
    "        details={\n",
    "            \"academic_events_found\": len(academic_events),\n",
    "            \"items_analyzed\": len(context_items)\n",
    "        },\n",
    "        issues_detected=coherence_issues\n",
    "    )\n",
    "\n",
    "def _measure_efficiency(self, context_items: List[Any]) -> QualityMeasurement:\n",
    "    \"\"\"Measure efficiency - context size vs information value.\"\"\"\n",
    "    if not context_items:\n",
    "        return QualityMeasurement(\n",
    "            metric=ContextQualityMetric.EFFICIENCY,\n",
    "            score=1.0,\n",
    "            timestamp=datetime.now(),\n",
    "            details={\"items_analyzed\": 0}\n",
    "        )\n",
    "    \n",
    "    # Calculate total content size\n",
    "    total_chars = 0\n",
    "    unique_info_pieces = set()\n",
    "    \n",
    "    for item in context_items:\n",
    "        content = getattr(item, 'content', str(item))\n",
    "        total_chars += len(content)\n",
    "        \n",
    "        # Extract key information pieces (simplified)\n",
    "        words = content.lower().split()\n",
    "        for word in words:\n",
    "            if len(word) > 3 and word.isalpha():  # Meaningful words\n",
    "                unique_info_pieces.add(word)\n",
    "    \n",
    "    # Calculate efficiency: unique information per character\n",
    "    if total_chars > 0:\n",
    "        efficiency = len(unique_info_pieces) / total_chars * 100  # Scale up\n",
    "        efficiency = min(efficiency, 1.0)  # Cap at 1.0\n",
    "    else:\n",
    "        efficiency = 0.0\n",
    "    \n",
    "    issues = []\n",
    "    if total_chars > 5000:  # Large context\n",
    "        issues.append(\"Context size is very large\")\n",
    "    if efficiency < 0.1:\n",
    "        issues.append(\"Low information density\")\n",
    "    \n",
    "    return QualityMeasurement(\n",
    "        metric=ContextQualityMetric.EFFICIENCY,\n",
    "        score=efficiency,\n",
    "        timestamp=datetime.now(),\n",
    "        details={\n",
    "            \"total_chars\": total_chars,\n",
    "            \"unique_info_pieces\": len(unique_info_pieces),\n",
    "            \"items_analyzed\": len(context_items)\n",
    "        },\n",
    "        issues_detected=issues\n",
    "    )\n",
    "\n",
    "def _measure_diversity(self, context_items: List[Any]) -> QualityMeasurement:\n",
    "    \"\"\"Measure diversity - variety of information sources and types.\"\"\"\n",
    "    if not context_items:\n",
    "        return QualityMeasurement(\n",
    "            metric=ContextQualityMetric.DIVERSITY,\n",
    "            score=0.0,\n",
    "            timestamp=datetime.now(),\n",
    "            details={\"items_analyzed\": 0}\n",
    "        )\n",
    "    \n",
    "    # Count different source types\n",
    "    source_types = set()\n",
    "    content_types = set()\n",
    "    \n",
    "    for item in context_items:\n",
    "        # Source type\n",
    "        if hasattr(item, 'source'):\n",
    "            source_types.add(getattr(item.source, 'source_type', 'unknown'))\n",
    "        else:\n",
    "            source_types.add('unknown')\n",
    "        \n",
    "        # Content type (academic, preference, etc.)\n",
    "        content = getattr(item, 'content', str(item)).lower()\n",
    "        if any(word in content for word in [\"completed\", \"grade\", \"gpa\"]):\n",
    "            content_types.add(\"academic\")\n",
    "        if any(word in content for word in [\"prefer\", \"like\", \"want\"]):\n",
    "            content_types.add(\"preference\")\n",
    "        if any(word in content for word in [\"career\", \"job\", \"goal\"]):\n",
    "            content_types.add(\"career\")\n",
    "        if any(word in content for word in [\"schedule\", \"time\"]):\n",
    "            content_types.add(\"schedule\")\n",
    "    \n",
    "    # Calculate diversity score\n",
    "    max_source_types = 5  # Expected maximum variety\n",
    "    max_content_types = 4\n",
    "    \n",
    "    source_diversity = min(len(source_types) / max_source_types, 1.0)\n",
    "    content_diversity = min(len(content_types) / max_content_types, 1.0)\n",
    "    \n",
    "    diversity_score = (source_diversity + content_diversity) / 2\n",
    "    \n",
    "    issues = []\n",
    "    if len(source_types) <= 1:\n",
    "        issues.append(\"Limited source diversity\")\n",
    "    if len(content_types) <= 1:\n",
    "        issues.append(\"Limited content type diversity\")\n",
    "    \n",
    "    return QualityMeasurement(\n",
    "        metric=ContextQualityMetric.DIVERSITY,\n",
    "        score=diversity_score,\n",
    "        timestamp=datetime.now(),\n",
    "        details={\n",
    "            \"source_types\": list(source_types),\n",
    "            \"content_types\": list(content_types),\n",
    "            \"source_diversity\": source_diversity,\n",
    "            \"content_diversity\": content_diversity\n",
    "        },\n",
    "        issues_detected=issues\n",
    "    )\n",
    "\n",
    "# Add methods to ContextValidator class\n",
    "ContextValidator._measure_consistency = _measure_consistency\n",
    "ContextValidator._measure_completeness = _measure_completeness\n",
    "ContextValidator._measure_accuracy = _measure_accuracy\n",
    "ContextValidator._measure_coherence = _measure_coherence\n",
    "ContextValidator._measure_efficiency = _measure_efficiency\n",
    "ContextValidator._measure_diversity = _measure_diversity\n",
    "\n",
    "print(\"✅ Additional quality measurement methods added\")"
   ]
  ,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Monitoring and Alerting System\n",
    "\n",
    "Let's create a comprehensive health monitoring system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the ContextValidator with recommendation and alert generation\n",
    "def _generate_recommendations(self, metric_scores: Dict[ContextQualityMetric, QualityMeasurement]) -> List[str]:\n",
    "    \"\"\"Generate recommendations based on quality measurements.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    for metric, measurement in metric_scores.items():\n",
    "        if measurement.score < self.quality_thresholds[metric]:\n",
    "            if metric == ContextQualityMetric.RELEVANCE:\n",
    "                recommendations.append(\"Improve context retrieval to better match query intent\")\n",
    "            elif metric == ContextQualityMetric.FRESHNESS:\n",
    "                recommendations.append(\"Update stale context information or implement time-based pruning\")\n",
    "            elif metric == ContextQualityMetric.CONSISTENCY:\n",
    "                recommendations.append(\"Resolve contradictions in context using conflict resolution strategies\")\n",
    "            elif metric == ContextQualityMetric.COMPLETENESS:\n",
    "                recommendations.append(\"Gather additional context to provide complete information\")\n",
    "            elif metric == ContextQualityMetric.ACCURACY:\n",
    "                recommendations.append(\"Validate context information for accuracy and correct errors\")\n",
    "            elif metric == ContextQualityMetric.COHERENCE:\n",
    "                recommendations.append(\"Improve context ordering and logical flow\")\n",
    "            elif metric == ContextQualityMetric.EFFICIENCY:\n",
    "                recommendations.append(\"Optimize context size and remove redundant information\")\n",
    "            elif metric == ContextQualityMetric.DIVERSITY:\n",
    "                recommendations.append(\"Include context from more diverse sources and types\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def _generate_alerts(self, metric_scores: Dict[ContextQualityMetric, QualityMeasurement]) -> List[str]:\n",
    "    \"\"\"Generate alerts for critical quality issues.\"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    for metric, measurement in metric_scores.items():\n",
    "        status = measurement.get_status()\n",
    "        \n",
    "        if status == HealthStatus.CRITICAL:\n",
    "            alerts.append(f\"CRITICAL: {metric.value} quality is critically low ({measurement.score:.1%})\")\n",
    "        elif status == HealthStatus.FAILING:\n",
    "            alerts.append(f\"FAILING: {metric.value} quality is failing ({measurement.score:.1%})\")\n",
    "        \n",
    "        # Specific issue alerts\n",
    "        for issue in measurement.issues_detected:\n",
    "            if \"critical\" in issue.lower() or \"error\" in issue.lower():\n",
    "                alerts.append(f\"ISSUE: {issue}\")\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# Add methods to ContextValidator\n",
    "ContextValidator._generate_recommendations = _generate_recommendations\n",
    "ContextValidator._generate_alerts = _generate_alerts\n",
    "\n",
    "class ContextHealthMonitor:\n",
    "    \"\"\"Continuous monitoring system for context health.\"\"\"\n",
    "    \n",
    "    def __init__(self, validator: ContextValidator):\n",
    "        self.validator = validator\n",
    "        self.monitoring_history = deque(maxlen=1000)\n",
    "        self.alert_thresholds = {\n",
    "            \"consecutive_warnings\": 3,\n",
    "            \"critical_score_threshold\": 0.4,\n",
    "            \"trend_degradation_threshold\": 0.1  # 10% degradation\n",
    "        }\n",
    "        self.active_alerts = set()\n",
    "    \n",
    "    async def monitor_context_health(self, \n",
    "                                   context_items: List[Any],\n",
    "                                   student_id: str,\n",
    "                                   query_context: str = \"\") -> Dict[str, Any]:\n",
    "        \"\"\"Perform health monitoring and return comprehensive status.\"\"\"\n",
    "        \n",
    "        # Get current health report\n",
    "        health_report = await self.validator.validate_context_health(\n",
    "            context_items, student_id, query_context\n",
    "        )\n",
    "        \n",
    "        # Store in monitoring history\n",
    "        self.monitoring_history.append(health_report)\n",
    "        \n",
    "        # Analyze trends\n",
    "        trend_analysis = self._analyze_trends()\n",
    "        \n",
    "        # Check for alert conditions\n",
    "        new_alerts = self._check_alert_conditions(health_report, trend_analysis)\n",
    "        \n",
    "        # Update active alerts\n",
    "        self.active_alerts.update(new_alerts)\n",
    "        \n",
    "        return {\n",
    "            \"current_health\": health_report,\n",
    "            \"trend_analysis\": trend_analysis,\n",
    "            \"new_alerts\": new_alerts,\n",
    "            \"active_alerts\": list(self.active_alerts),\n",
    "            \"monitoring_summary\": self._create_monitoring_summary()\n",
    "        }\n",
    "    \n",
    "    def _analyze_trends(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze trends in context health over time.\"\"\"\n",
    "        if len(self.monitoring_history) < 2:\n",
    "            return {\"trend\": \"insufficient_data\", \"details\": \"Need more data points\"}\n",
    "        \n",
    "        # Get recent scores\n",
    "        recent_scores = [report.overall_score for report in list(self.monitoring_history)[-10:]]\n",
    "        \n",
    "        if len(recent_scores) >= 3:\n",
    "            # Calculate trend\n",
    "            early_avg = statistics.mean(recent_scores[:len(recent_scores)//2])\n",
    "            late_avg = statistics.mean(recent_scores[len(recent_scores)//2:])\n",
    "            \n",
    "            trend_change = late_avg - early_avg\n",
    "            \n",
    "            if trend_change > 0.05:\n",
    "                trend = \"improving\"\n",
    "            elif trend_change < -0.05:\n",
    "                trend = \"degrading\"\n",
    "            else:\n",
    "                trend = \"stable\"\n",
    "            \n",
    "            return {\n",
    "                \"trend\": trend,\n",
    "                \"trend_change\": trend_change,\n",
    "                \"recent_average\": late_avg,\n",
    "                \"previous_average\": early_avg,\n",
    "                \"data_points\": len(recent_scores)\n",
    "            }\n",
    "        \n",
    "        return {\"trend\": \"insufficient_data\", \"details\": \"Need more data points\"}\n",
    "    \n",
    "    def _check_alert_conditions(self, \n",
    "                               health_report: ContextHealthReport, \n",
    "                               trend_analysis: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Check for conditions that should trigger alerts.\"\"\"\n",
    "        new_alerts = []\n",
    "        \n",
    "        # Critical overall score\n",
    "        if health_report.overall_score < self.alert_thresholds[\"critical_score_threshold\"]:\n",
    "            new_alerts.append(f\"CRITICAL: Overall context health is critically low ({health_report.overall_score:.1%})\")\n",
    "        \n",
    "        # Degrading trend\n",
    "        if (trend_analysis.get(\"trend\") == \"degrading\" and \n",
    "            abs(trend_analysis.get(\"trend_change\", 0)) > self.alert_thresholds[\"trend_degradation_threshold\"]):\n",
    "            new_alerts.append(f\"WARNING: Context health is degrading (trend: {trend_analysis['trend_change']:.1%})\")\n",
    "        \n",
    "        # Consecutive warnings\n",
    "        if len(self.monitoring_history) >= self.alert_thresholds[\"consecutive_warnings\"]:\n",
    "            recent_statuses = [report.overall_status for report in list(self.monitoring_history)[-3:]]\n",
    "            if all(status in [HealthStatus.WARNING, HealthStatus.CRITICAL, HealthStatus.FAILING] \n",
    "                   for status in recent_statuses):\n",
    "                new_alerts.append(\"WARNING: Context health has been poor for multiple consecutive checks\")\n",
    "        \n",
    "        # Metric-specific alerts\n",
    "        for metric, measurement in health_report.metric_scores.items():\n",
    "            if measurement.get_status() == HealthStatus.FAILING:\n",
    "                new_alerts.append(f\"FAILING: {metric.value} metric is failing ({measurement.score:.1%})\")\n",
    "        \n",
    "        return new_alerts\n",
    "    \n",
    "    def _create_monitoring_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Create summary of monitoring status.\"\"\"\n",
    "        if not self.monitoring_history:\n",
    "            return {\"status\": \"no_data\"}\n",
    "        \n",
    "        latest_report = self.monitoring_history[-1]\n",
    "        \n",
    "        # Calculate averages over recent history\n",
    "        recent_reports = list(self.monitoring_history)[-10:]\n",
    "        avg_score = statistics.mean([r.overall_score for r in recent_reports])\n",
    "        \n",
    "        # Count status distribution\n",
    "        status_counts = defaultdict(int)\n",
    "        for report in recent_reports:\n",
    "            status_counts[report.overall_status.value] += 1\n",
    "        \n",
    "        return {\n",
    "            \"latest_score\": latest_report.overall_score,\n",
    "            \"latest_status\": latest_report.overall_status.value,\n",
    "            \"recent_average\": avg_score,\n",
    "            \"status_distribution\": dict(status_counts),\n",
    "            \"total_checks\": len(self.monitoring_history),\n",
    "            \"active_alert_count\": len(self.active_alerts)\n",
    "        }\n",
    "    \n",
    "    def get_health_dashboard(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive health dashboard data.\"\"\"\n",
    "        if not self.monitoring_history:\n",
    "            return {\"status\": \"no_data\", \"message\": \"No monitoring data available\"}\n",
    "        \n",
    "        latest_report = self.monitoring_history[-1]\n",
    "        \n",
    "        # Metric breakdown\n",
    "        metric_breakdown = {}\n",
    "        for metric, measurement in latest_report.metric_scores.items():\n",
    "            metric_breakdown[metric.value] = {\n",
    "                \"score\": measurement.score,\n",
    "                \"status\": measurement.get_status().value,\n",
    "                \"issues\": measurement.issues_detected\n",
    "            }\n",
    "        \n",
    "        # Historical trend\n",
    "        if len(self.monitoring_history) >= 5:\n",
    "            scores = [r.overall_score for r in list(self.monitoring_history)[-20:]]\n",
    "            trend_data = {\n",
    "                \"scores\": scores,\n",
    "                \"timestamps\": [r.timestamp.isoformat() for r in list(self.monitoring_history)[-20:]]\n",
    "            }\n",
    "        else:\n",
    "            trend_data = {\"message\": \"Insufficient data for trend analysis\"}\n",
    "        \n",
    "        return {\n",
    "            \"overall_health\": {\n",
    "                \"score\": latest_report.overall_score,\n",
    "                \"status\": latest_report.overall_status.value,\n",
    "                \"summary\": latest_report.get_summary()\n",
    "            },\n",
    "            \"metric_breakdown\": metric_breakdown,\n",
    "            \"active_alerts\": list(self.active_alerts),\n",
    "            \"recommendations\": latest_report.recommendations,\n",
    "            \"trend_data\": trend_data,\n",
    "            \"monitoring_stats\": self._create_monitoring_summary()\n",
    "        }\n",
    "    \n",
    "    def clear_alert(self, alert_message: str):\n",
    "        \"\"\"Clear a specific alert.\"\"\"\n",
    "        self.active_alerts.discard(alert_message)\n",
    "    \n",
    "    def clear_all_alerts(self):\n",
    "        \"\"\"Clear all active alerts.\"\"\"\n",
    "        self.active_alerts.clear()\n",
    "\n",
    "# Initialize the health monitor\n",
    "health_monitor = ContextHealthMonitor(context_validator)\n",
    "\n",
    "print(\"✅ Context health monitoring system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Context Health Validation\n",
    "\n",
    "Let's create sample context with various quality issues and see how validation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample context items with various quality issues\n",
    "@dataclass\n",
    "class MockContextItem:\n",
    "    \"\"\"Mock context item for testing.\"\"\"\n",
    "    content: str\n",
    "    timestamp: datetime\n",
    "    source: Optional[Any] = None\n",
    "\n",
    "def create_test_context_scenarios() -> Dict[str, List[MockContextItem]]:\n",
    "    \"\"\"Create different context scenarios for testing.\"\"\"\n",
    "    \n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    scenarios = {\n",
    "        \"healthy_context\": [\n",
    "            MockContextItem(\n",
    "                content=\"Student completed CS201 with grade A in Spring 2024\",\n",
    "                timestamp=base_time - timedelta(days=30)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student prefers online courses due to work schedule\",\n",
    "                timestamp=base_time - timedelta(days=5)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student interested in machine learning career path\",\n",
    "                timestamp=base_time - timedelta(days=10)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Current GPA: 3.7, planning to take CS401 next semester\",\n",
    "                timestamp=base_time - timedelta(days=2)\n",
    "            )\n",
    "        ],\n",
    "        \n",
    "        \"stale_context\": [\n",
    "            MockContextItem(\n",
    "                content=\"Student prefers morning classes\",\n",
    "                timestamp=base_time - timedelta(days=180)  # Very old\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student completed CS101 with grade B\",\n",
    "                timestamp=base_time - timedelta(days=365)  # Very old\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student interested in web development\",\n",
    "                timestamp=base_time - timedelta(days=200)  # Old\n",
    "            )\n",
    "        ],\n",
    "        \n",
    "        \"contradictory_context\": [\n",
    "            MockContextItem(\n",
    "                content=\"Student prefers online courses for flexibility\",\n",
    "                timestamp=base_time - timedelta(days=5)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student prefers in-person classes for better interaction\",\n",
    "                timestamp=base_time - timedelta(days=3)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student likes challenging courses\",\n",
    "                timestamp=base_time - timedelta(days=7)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student prefers easy courses to maintain GPA\",\n",
    "                timestamp=base_time - timedelta(days=4)\n",
    "            )\n",
    "        ],\n",
    "        \n",
    "        \"inaccurate_context\": [\n",
    "            MockContextItem(\n",
    "                content=\"Student has GPA of 5.2\",  # Impossible GPA\n",
    "                timestamp=base_time - timedelta(days=10)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student completed CS9999 advanced quantum computing\",  # Invalid course code\n",
    "                timestamp=base_time - timedelta(days=15)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student graduated in 2025 but is taking courses in 2024\",  # Temporal inconsistency\n",
    "                timestamp=base_time - timedelta(days=5)\n",
    "            )\n",
    "        ],\n",
    "        \n",
    "        \"incomplete_context\": [\n",
    "            MockContextItem(\n",
    "                content=\"Student wants to take advanced courses\",  # Vague\n",
    "                timestamp=base_time - timedelta(days=2)\n",
    "            ),\n",
    "            MockContextItem(\n",
    "                content=\"Student has some programming experience\",  # Vague\n",
    "                timestamp=base_time - timedelta(days=5)\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "# Test different context scenarios\n",
    "test_scenarios = create_test_context_scenarios()\n",
    "\n",
    "print(\"🧪 Testing Context Health Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for scenario_name, context_items in test_scenarios.items():\n",
    "    print(f\"\\n🎯 Scenario: {scenario_name.replace('_', ' ').title()}\")\n",
    "    print(f\"📚 Context Items: {len(context_items)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Validate context health\n",
    "    health_report = await context_validator.validate_context_health(\n",
    "        context_items=context_items,\n",
    "        student_id=\"test_student\",\n",
    "        query_context=\"Help me plan my computer science courses\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"📊 {health_report.get_summary()}\")\n",
    "    print(f\"📈 Overall Score: {health_report.overall_score:.1%}\")\n",
    "    \n",
    "    # Show metric breakdown\n",
    "    print(\"\\n📋 Metric Breakdown:\")\n",
    "    for metric, measurement in health_report.metric_scores.items():\n",
    "        status_emoji = {\n",
    "            HealthStatus.EXCELLENT: \"🟢\",\n",
    "            HealthStatus.GOOD: \"🟡\", \n",
    "            HealthStatus.WARNING: \"🟠\",\n",
    "            HealthStatus.CRITICAL: \"🔴\",\n",
    "            HealthStatus.FAILING: \"💀\"\n",
    "        }\n",
    "        emoji = status_emoji.get(measurement.get_status(), \"❓\")\n",
    "        print(f\"   {emoji} {metric.value}: {measurement.score:.1%}\")\n",
    "        \n",
    "        # Show issues if any\n",
    "        if measurement.issues_detected:\n",
    "            for issue in measurement.issues_detected[:2]:  # Show first 2 issues\n",
    "                print(f\"      ⚠️  {issue}\")\n",
    "    \n",
    "    # Show recommendations\n",
    "    if health_report.recommendations:\n",
    "        print(f\"\\n💡 Recommendations:\")\n",
    "        for rec in health_report.recommendations[:3]:  # Show first 3\n",
    "            print(f\"   • {rec}\")\n",
    "    \n",
    "    # Show alerts\n",
    "    if health_report.alerts:\n",
    "        print(f\"\\n🚨 Alerts:\")\n",
    "        for alert in health_report.alerts:\n",
    "            print(f\"   • {alert}\")\n",
    "    \n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Monitoring Dashboard\n",
    "\n",
    "Let's test the continuous monitoring system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test continuous health monitoring\n",
    "print(\"📊 Testing Continuous Health Monitoring\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate monitoring over time with different context quality\n",
    "monitoring_scenarios = [\n",
    "    (\"healthy_context\", \"Initial healthy state\"),\n",
    "    (\"healthy_context\", \"Maintaining good health\"),\n",
    "    (\"stale_context\", \"Context becoming stale\"),\n",
    "    (\"contradictory_context\", \"Contradictions appearing\"),\n",
    "    (\"inaccurate_context\", \"Accuracy issues detected\"),\n",
    "    (\"incomplete_context\", \"Context becoming incomplete\")\n",
    "]\n",
    "\n",
    "print(\"🔄 Simulating monitoring over time...\\n\")\n",
    "\n",
    "for i, (scenario_name, description) in enumerate(monitoring_scenarios, 1):\n",
    "    context_items = test_scenarios[scenario_name]\n",
    "    \n",
    "    print(f\"📅 Check {i}: {description}\")\n",
    "    \n",
    "    # Perform monitoring\n",
    "    monitoring_result = await health_monitor.monitor_context_health(\n",
    "        context_items=context_items,\n",
    "        student_id=\"test_student\",\n",
    "        query_context=\"Help me plan my courses for next semester\"\n",
    "    )\n",
    "    \n",
    "    current_health = monitoring_result[\"current_health\"]\n",
    "    trend_analysis = monitoring_result[\"trend_analysis\"]\n",
    "    new_alerts = monitoring_result[\"new_alerts\"]\n",
    "    \n",
    "    print(f\"   {current_health.get_summary()}\")\n",
    "    \n",
    "    if trend_analysis.get(\"trend\") != \"insufficient_data\":\n",
    "        trend = trend_analysis[\"trend\"]\n",
    "        change = trend_analysis.get(\"trend_change\", 0)\n",
    "        print(f\"   📈 Trend: {trend} ({change:+.1%})\")\n",
    "    \n",
    "    if new_alerts:\n",
    "        print(f\"   🚨 New Alerts: {len(new_alerts)}\")\n",
    "        for alert in new_alerts[:2]:  # Show first 2 alerts\n",
    "            print(f\"      • {alert}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Get comprehensive dashboard\n",
    "print(\"\\n📊 Health Dashboard Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "dashboard = health_monitor.get_health_dashboard()\n",
    "\n",
    "if dashboard.get(\"status\") != \"no_data\":\n",
    "    overall_health = dashboard[\"overall_health\"]\n",
    "    print(f\"🎯 {overall_health['summary']}\")\n",
    "    \n",
    "    # Show metric breakdown\n",
    "    print(\"\\n📋 Current Metric Status:\")\n",
    "    for metric_name, metric_data in dashboard[\"metric_breakdown\"].items():\n",
    "        status_emoji = {\n",
    "            \"excellent\": \"🟢\", \"good\": \"🟡\", \"warning\": \"🟠\", \n",
    "            \"critical\": \"🔴\", \"failing\": \"💀\"\n",
    "        }\n",
    "        emoji = status_emoji.get(metric_data[\"status\"], \"❓\")\n",
    "        print(f\"   {emoji} {metric_name}: {metric_data['score']:.1%}\")\n",
    "    \n",
    "    # Show active alerts\n",
    "    if dashboard[\"active_alerts\"]:\n",
    "        print(f\"\\n🚨 Active Alerts ({len(dashboard['active_alerts'])}):\")\n",
    "        for alert in dashboard[\"active_alerts\"][:3]:\n",
    "            print(f\"   • {alert}\")\n",
    "    \n",
    "    # Show recommendations\n",
    "    if dashboard[\"recommendations\"]:\n",
    "        print(f\"\\n💡 Top Recommendations:\")\n",
    "        for rec in dashboard[\"recommendations\"][:3]:\n",
    "            print(f\"   • {rec}\")\n",
    "    \n",
    "    # Show monitoring stats\n",
    "    stats = dashboard[\"monitoring_stats\"]\n",
    "    print(f\"\\n📈 Monitoring Statistics:\")\n",
    "    print(f\"   • Total Checks: {stats['total_checks']}\")\n",
    "    print(f\"   • Recent Average: {stats['recent_average']:.1%}\")\n",
    "    print(f\"   • Active Alerts: {stats['active_alert_count']}\")\n",
    "    \n",
    "    if \"status_distribution\" in stats:\n",
    "        print(f\"   • Status Distribution: {stats['status_distribution']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Hands-on Exercise: Design Your Validation Strategy\n",
    "\n",
    "Now it's your turn to create custom validation rules for your domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create domain-specific validation rules\n",
    "print(\"🧪 Exercise: Design Your Context Validation Strategy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Create custom validation rules for academic advising\n",
    "class AcademicAdvisingValidator(ContextValidator):\n",
    "    \"\"\"Specialized validator for academic advising context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Academic-specific quality thresholds\n",
    "        self.quality_thresholds.update({\n",
    "            ContextQualityMetric.ACCURACY: 0.95,  # Higher accuracy requirement\n",
    "            ContextQualityMetric.COMPLETENESS: 0.8,  # Higher completeness requirement\n",
    "            ContextQualityMetric.CONSISTENCY: 0.85  # Higher consistency requirement\n",
    "        })\n",
    "        \n",
    "        # Academic-specific validation rules\n",
    "        self.academic_validation_rules = {\n",
    "            \"gpa_range\": (0.0, 4.0),\n",
    "            \"valid_course_prefixes\": [\"CS\", \"MATH\", \"PHYS\", \"CHEM\", \"ENGL\", \"HIST\"],\n",
    "            \"valid_course_numbers\": (100, 999),\n",
    "            \"valid_grades\": [\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"F\"],\n",
    "            \"max_credits_per_semester\": 18,\n",
    "            \"graduation_credit_requirement\": 120\n",
    "        }\n",
    "    \n",
    "    def validate_academic_progression(self, context_items: List[Any]) -> QualityMeasurement:\n",
    "        \"\"\"Validate logical academic progression.\"\"\"\n",
    "        progression_issues = []\n",
    "        progression_score = 1.0\n",
    "        \n",
    "        # Extract academic events\n",
    "        academic_events = []\n",
    "        for item in context_items:\n",
    "            content = getattr(item, 'content', str(item))\n",
    "            timestamp = getattr(item, 'timestamp', datetime.now())\n",
    "            \n",
    "            # Look for course completions\n",
    "            import re\n",
    "            course_completions = re.findall(r'completed ([A-Z]{2,4}\\d{3})', content.upper())\n",
    "            for course in course_completions:\n",
    "                academic_events.append((\"completed\", course, timestamp))\n",
    "            \n",
    "            # Look for current enrollments\n",
    "            current_courses = re.findall(r'enrolled in ([A-Z]{2,4}\\d{3})', content.upper())\n",
    "            for course in current_courses:\n",
    "                academic_events.append((\"enrolled\", course, timestamp))\n",
    "        \n",
    "        # Check for prerequisite violations\n",
    "        prerequisite_map = {\n",
    "            \"CS201\": [\"CS101\"],\n",
    "            \"CS301\": [\"CS201\"],\n",
    "            \"CS401\": [\"CS301\", \"MATH201\"],\n",
    "            \"CS402\": [\"CS401\"]\n",
    "        }\n",
    "        \n",
    "        completed_courses = set()\n",
    "        for event_type, course, timestamp in sorted(academic_events, key=lambda x: x[2]):\n",
    "            if event_type == \"completed\":\n",
    "                completed_courses.add(course)\n",
    "            elif event_type == \"enrolled\":\n",
    "                # Check if prerequisites are met\n",
    "                required_prereqs = prerequisite_map.get(course, [])\n",
    "                missing_prereqs = set(required_prereqs) - completed_courses\n",
    "                \n",
    "                if missing_prereqs:\n",
    "                    progression_score -= 0.3\n",
    "                    progression_issues.append(f\"Missing prerequisites for {course}: {list(missing_prereqs)}\")\n",
    "        \n",
    "        return QualityMeasurement(\n",
    "            metric=ContextQualityMetric.COHERENCE,  # Using coherence for academic progression\n",
    "            score=max(progression_score, 0.0),\n",
    "            timestamp=datetime.now(),\n",
    "            details={\n",
    "                \"academic_events\": len(academic_events),\n",
    "                \"completed_courses\": list(completed_courses),\n",
    "                \"prerequisite_violations\": len(progression_issues)\n",
    "            },\n",
    "            issues_detected=progression_issues\n",
    "        )\n",
    "    \n",
    "    def validate_graduation_feasibility(self, context_items: List[Any]) -> QualityMeasurement:\n",
    "        \"\"\"Validate if graduation plan is feasible.\"\"\"\n",
    "        feasibility_issues = []\n",
    "        feasibility_score = 1.0\n",
    "        \n",
    "        # Extract graduation timeline and credit information\n",
    "        total_credits = 0\n",
    "        graduation_timeline = None\n",
    "        current_semester = \"Fall 2024\"  # Assume current\n",
    "        \n",
    "        for item in context_items:\n",
    "            content = getattr(item, 'content', str(item)).lower()\n",
    "            \n",
    "            # Look for credit information\n",
    "            import re\n",
    "            credit_matches = re.findall(r'(\\d+)\\s*credits?', content)\n",
    "            for credit_str in credit_matches:\n",
    "                total_credits += int(credit_str)\n",
    "            \n",
    "            # Look for graduation timeline\n",
    "            if \"graduation\" in content or \"graduate\" in content:\n",
    "                timeline_matches = re.findall(r'(spring|fall)\\s*(\\d{4})', content)\n",
    "                if timeline_matches:\n",
    "                    semester, year = timeline_matches[0]\n",
    "                    graduation_timeline = f\"{semester.title()} {year}\"\n",
    "        \n",
    "        # Check credit requirements\n",
    "        required_credits = self.academic_validation_rules[\"graduation_credit_requirement\"]\n",
    "        if total_credits < required_credits:\n",
    "            remaining_credits = required_credits - total_credits\n",
    "            \n",
    "            if graduation_timeline:\n",
    "                # Calculate if timeline is feasible\n",
    "                # Simplified calculation\n",
    "                semesters_remaining = 4  # Assume 4 semesters remaining\n",
    "                credits_per_semester = remaining_credits / semesters_remaining\n",
    "                \n",
    "                max_credits = self.academic_validation_rules[\"max_credits_per_semester\"]\n",
    "                if credits_per_semester > max_credits:\n",
    "                    feasibility_score -= 0.4\n",
    "                    feasibility_issues.append(f\"Graduation timeline requires {credits_per_semester:.1f} credits/semester (max: {max_credits})\")\n",
    "            \n",
    "            if remaining_credits > 60:  # More than 2 years of work\n",
    "                feasibility_score -= 0.2\n",
    "                feasibility_issues.append(f\"Significant credits remaining: {remaining_credits}\")\n",
    "        \n",
    "        return QualityMeasurement(\n",
    "            metric=ContextQualityMetric.COMPLETENESS,  # Using completeness for graduation feasibility\n",
    "            score=max(feasibility_score, 0.0),\n",
    "            timestamp=datetime.now(),\n",
    "            details={\n",
    "                \"total_credits\": total_credits,\n",
    "                \"required_credits\": required_credits,\n",
    "                \"graduation_timeline\": graduation_timeline,\n",
    "                \"remaining_credits\": max(required_credits - total_credits, 0)\n",
    "            },\n",
    "            issues_detected=feasibility_issues\n",
    "        )\n",
    "    \n",
    "    async def validate_context_health(self, \n",
    "                                    context_items: List[Any],\n",
    "                                    student_id: str,\n",
    "                                    query_context: str = \"\") -> ContextHealthReport:\n",
    "        \"\"\"Enhanced validation with academic-specific checks.\"\"\"\n",
    "        \n",
    "        # Get standard validation\n",
    "        standard_report = await super().validate_context_health(context_items, student_id, query_context)\n",
    "        \n",
    "        # Add academic-specific validations\n",
    "        progression_check = self.validate_academic_progression(context_items)\n",
    "        feasibility_check = self.validate_graduation_feasibility(context_items)\n",
    "        \n",
    "        # Update metric scores with academic checks\n",
    "        standard_report.metric_scores[ContextQualityMetric.COHERENCE] = progression_check\n",
    "        \n",
    "        # Add academic-specific recommendations\n",
    "        if progression_check.score < 0.7:\n",
    "            standard_report.recommendations.append(\"Review course prerequisites and academic progression\")\n",
    "        \n",
    "        if feasibility_check.score < 0.7:\n",
    "            standard_report.recommendations.append(\"Reassess graduation timeline and credit requirements\")\n",
    "        \n",
    "        # Recalculate overall score\n",
    "        weights = {\n",
    "            ContextQualityMetric.RELEVANCE: 0.15,\n",
    "            ContextQualityMetric.FRESHNESS: 0.1,\n",
    "            ContextQualityMetric.CONSISTENCY: 0.2,\n",
    "            ContextQualityMetric.COMPLETENESS: 0.2,\n",
    "            ContextQualityMetric.ACCURACY: 0.25,  # Higher weight for academic accuracy\n",
    "            ContextQualityMetric.COHERENCE: 0.1   # Academic progression\n",
    "        }\n",
    "        \n",
    "        standard_report.overall_score = sum(\n",
    "            weights.get(metric, 0.05) * measurement.score \n",
    "            for metric, measurement in standard_report.metric_scores.items()\n",
    "        )\n",
    "        \n",
    "        standard_report.overall_status = self._score_to_status(standard_report.overall_score)\n",
    "        \n",
    "        return standard_report\n",
    "\n",
    "# Test the academic validator\n",
    "academic_validator = AcademicAdvisingValidator()\n",
    "\n",
    "# Create academic-specific test context\n",
    "academic_test_context = [\n",
    "    MockContextItem(\n",
    "        content=\"Student completed CS101 with grade A in Fall 2023\",\n",
    "        timestamp=datetime.now() - timedelta(days=120)\n",
    "    ),\n",
    "    MockContextItem(\n",
    "        content=\"Student is enrolled in CS301 but has not completed CS201\",  # Prerequisite violation\n",
    "        timestamp=datetime.now() - timedelta(days=5)\n",
    "    ),\n",
    "    MockContextItem(\n",
    "        content=\"Student has 45 credits and wants to graduate in Spring 2025\",  # Feasibility issue\n",
    "        timestamp=datetime.now() - timedelta(days=10)\n",
    "    ),\n",
    "    MockContextItem(\n",
    "        content=\"Current GPA: 3.7, planning advanced courses\",\n",
    "        timestamp=datetime.now() - timedelta(days=2)\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\n🎯 Testing Academic-Specific Validation:\")\n",
    "\n",
    "academic_report = await academic_validator.validate_context_health(\n",
    "    context_items=academic_test_context,\n",
    "    student_id=\"academic_test_student\",\n",
    "    query_context=\"Help me plan my remaining courses for graduation\"\n",
    ")\n",
    "\n",
    "print(f\"📊 {academic_report.get_summary()}\")\n",
    "print(f\"📈 Overall Score: {academic_report.overall_score:.1%}\")\n",
    "\n",
    "if academic_report.recommendations:\n",
    "    print(f\"\\n💡 Academic Recommendations:\")\n",
    "    for rec in academic_report.recommendations:\n",
    "        print(f\"   • {rec}\")\n",
    "\n",
    "print(\"\\n🤔 Reflection Questions:\")\n",
    "print(\"1. How do domain-specific validation rules improve context quality?\")\n",
    "print(\"2. What other academic validation rules would be valuable?\")\n",
    "print(\"3. How would you balance strict validation with user experience?\")\n",
    "print(\"4. What metrics would you track for production context health?\")\n",
    "\n",
    "print(\"\\n🔧 Your Turn: Try These Modifications:\")\n",
    "print(\"   • Add validation for course scheduling conflicts\")\n",
    "print(\"   • Create alerts for academic policy violations\")\n",
    "print(\"   • Implement semester-specific validation rules\")\n",
    "print(\"   • Add validation for financial aid requirements\")\n",
    "print(\"   • Create student-type specific validation (part-time, transfer, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "From this exploration of context validation and health monitoring, you've learned:\n",
    "\n",
    "### 🎯 **Core Concepts**\n",
    "- **Context quality** has multiple dimensions that must be measured and monitored\n",
    "- **Automated validation** can detect issues before they impact user experience\n",
    "- **Health monitoring** provides continuous oversight of context systems\n",
    "- **Domain-specific validation** improves accuracy for specialized use cases\n",
    "\n",
    "### 🛠️ **Implementation Patterns**\n",
    "- **Multi-dimensional quality metrics** for comprehensive assessment\n",
    "- **Threshold-based alerting** for proactive issue detection\n",
    "- **Trend analysis** for identifying degradation patterns\n",
    "- **Automated recommendations** for context improvement\n",
    "\n",
    "### 📊 **Quality Dimensions**\n",
    "- **Relevance**: How well context matches current needs\n",
    "- **Freshness**: How recent and up-to-date information is\n",
    "- **Consistency**: Absence of contradictions in context\n",
    "- **Completeness**: Sufficient information for decision-making\n",
    "- **Accuracy**: Correctness of context information\n",
    "- **Coherence**: Logical flow and sense-making\n",
    "- **Efficiency**: Information density and context size optimization\n",
    "- **Diversity**: Variety of sources and information types\n",
    "\n",
    "### 🔄 **Monitoring Benefits**\n",
    "- **Early problem detection** before user impact\n",
    "- **Performance optimization** through quality insights\n",
    "- **Automated remediation** for common issues\n",
    "- **Production reliability** through continuous oversight\n",
    "\n",
    "### 🎓 **Academic Applications**\n",
    "- **Prerequisite validation** for course planning\n",
    "- **Graduation feasibility** checking\n",
    "- **Academic progression** logic validation\n",
    "- **Policy compliance** monitoring\n",
    "\n",
    "### 🚀 **Production Readiness**\n",
    "You now have the complete toolkit for advanced context engineering:\n",
    "1. **Dynamic Tool Selection** - Optimize tool availability\n",
    "2. **Context Isolation** - Prevent contamination between domains\n",
    "3. **Context Pruning** - Intelligent memory cleanup\n",
    "4. **Context Summarization** - Compress information while preserving value\n",
    "5. **Context Fusion** - Intelligently combine multiple information sources\n",
    "6. **Context Validation** - Ensure quality and detect issues\n",
    "\n",
    "These techniques work together to create robust, scalable, and reliable context management systems for production AI agents.\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Congratulations!** You've completed Section 5: Advanced Context Engineering. Your Redis University Class Agent now has enterprise-grade context management capabilities that can handle real-world complexity and scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d06c497fe3df20b",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# üß† Section 3, Notebook 3: Memory Management - Handling Long Conversations\n",
    "\n",
    "**‚è±Ô∏è Estimated Time:** 50-60 minutes\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand** why long conversations need management (token limits, cost, performance)\n",
    "2. **Implement** conversation summarization to preserve key information\n",
    "3. **Build** context compression strategies (truncation, priority-based, summarization)\n",
    "4. **Configure** automatic memory management with Agent Memory Server\n",
    "5. **Decide** when to apply each technique based on conversation characteristics\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where We Are\n",
    "\n",
    "### **Your Journey So Far:**\n",
    "\n",
    "**Section 3, Notebook 1:** Memory Fundamentals\n",
    "- ‚úÖ Working memory for conversation continuity\n",
    "- ‚úÖ Long-term memory for persistent knowledge\n",
    "- ‚úÖ The grounding problem and reference resolution\n",
    "- ‚úÖ Memory types (semantic, episodic, message)\n",
    "\n",
    "**Section 3, Notebook 2:** Memory-Enhanced RAG\n",
    "- ‚úÖ Integrated all four context types\n",
    "- ‚úÖ Built complete memory-enhanced RAG system\n",
    "- ‚úÖ Demonstrated benefits of stateful conversations\n",
    "\n",
    "**Your memory system works!** It can:\n",
    "- Remember conversation history across turns\n",
    "- Store and retrieve long-term facts\n",
    "- Resolve references (\"it\", \"that course\")\n",
    "- Provide personalized recommendations\n",
    "\n",
    "### **But... What About Long Conversations?**\n",
    "\n",
    "**Questions we can't answer yet:**\n",
    "- ‚ùì What happens when conversations get really long?\n",
    "- ‚ùì How do we handle token limits?\n",
    "- ‚ùì How much does a 50-turn conversation cost?\n",
    "- ‚ùì Can we preserve important context while reducing tokens?\n",
    "- ‚ùì When should we summarize vs. truncate vs. keep everything?\n",
    "\n",
    "---\n",
    "\n",
    "## üö® The Long Conversation Problem\n",
    "\n",
    "Before diving into solutions, let's understand the fundamental problem.\n",
    "\n",
    "### **The Problem: Unbounded Growth**\n",
    "\n",
    "Every conversation turn adds messages to working memory:\n",
    "\n",
    "```\n",
    "Turn 1:  System (500) + Messages (200) = 700 tokens ‚úÖ\n",
    "Turn 5:  System (500) + Messages (1,000) = 1,500 tokens ‚úÖ\n",
    "Turn 20: System (500) + Messages (4,000) = 4,500 tokens ‚úÖ\n",
    "Turn 50: System (500) + Messages (10,000) = 10,500 tokens ‚ö†Ô∏è\n",
    "Turn 100: System (500) + Messages (20,000) = 20,500 tokens ‚ö†Ô∏è\n",
    "Turn 200: System (500) + Messages (40,000) = 40,500 tokens ‚ùå\n",
    "```\n",
    "\n",
    "**Without management, conversations grow unbounded!**\n",
    "\n",
    "### **Why This Matters**\n",
    "\n",
    "**1. Token Limits (Hard Constraint)**\n",
    "- GPT-4o: 128K tokens (~96,000 words)\n",
    "- GPT-3.5: 16K tokens (~12,000 words)\n",
    "- Eventually, you'll hit the limit and conversations fail\n",
    "\n",
    "**2. Cost (Economic Constraint)**\n",
    "- Input tokens cost money  (e.g. $0.0025 /  1K  tokens for GPT-4o)\n",
    "\n",
    "- A 50-turn conversation = ~10,000 tokens = $0.025 per query\n",
    "\n",
    "- Over 1,000 conversations = $25 just for conversation history!\n",
    "\n",
    "**3. Performance (Quality Constraint)**\n",
    "- More tokens = longer processing time\n",
    "- Context Rot: LLMs struggle with very long contexts\n",
    "- Important information gets \"lost in the middle\"\n",
    "\n",
    "**4. User Experience**\n",
    "- Slow responses frustrate users\n",
    "- Expensive conversations aren't sustainable\n",
    "- Failed conversations due to token limits are unacceptable\n",
    "\n",
    "### **The Solution: Memory Management**\n",
    "\n",
    "We need strategies to:\n",
    "- ‚úÖ Keep conversations within token budgets\n",
    "- ‚úÖ Preserve important information\n",
    "- ‚úÖ Maintain conversation quality\n",
    "- ‚úÖ Control costs\n",
    "- ‚úÖ Enable indefinite conversations\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Part 0: Setup and Environment\n",
    "\n",
    "Let's set up our environment and create tools for measuring conversation growth.\n",
    "\n",
    "### ‚ö†Ô∏è Prerequisites\n",
    "\n",
    "**Before running this notebook, make sure you have:**\n",
    "\n",
    "1. **Docker Desktop running** - Required for Redis and Agent Memory Server\n",
    "\n",
    "2. **Environment variables** - Create a `.env` file in the `reference-agent` directory:\n",
    "   ```bash\n",
    "   # Copy the example file\n",
    "   cd ../../reference-agent\n",
    "   cp .env.example .env\n",
    "\n",
    "   # Edit .env and add your OpenAI API key\n",
    "   # OPENAI_API_KEY=your_actual_openai_api_key_here\n",
    "   ```\n",
    "\n",
    "3. **Run the setup script** - This will automatically start Redis and Agent Memory Server:\n",
    "   ```bash\n",
    "   cd ../../reference-agent\n",
    "   python setup_agent_memory_server.py\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c59ecc51d30c3",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10e48e57f1431e",
   "metadata": {},
   "source": [
    "### Automated Setup Check\n",
    "\n",
    "Let's run the setup script to ensure all services are running properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808cea2af3f4f118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running automated setup check...\n",
      "\n",
      "\n",
      "üîß Agent Memory Server Setup\n",
      "===========================\n",
      "üìä Checking Redis...\n",
      "‚úÖ Redis is running\n",
      "üìä Checking Agent Memory Server...\n",
      "üîç Agent Memory Server container exists. Checking health...\n",
      "‚úÖ Agent Memory Server is running and healthy\n",
      "‚úÖ No Redis connection issues detected\n",
      "\n",
      "‚úÖ Setup Complete!\n",
      "=================\n",
      "üìä Services Status:\n",
      "   ‚Ä¢ Redis: Running on port 6379\n",
      "   ‚Ä¢ Agent Memory Server: Running on port 8088\n",
      "\n",
      "üéØ You can now run the notebooks!\n",
      "\n",
      "\n",
      "‚úÖ All services are ready!\n"
     ]
    }
   ],
   "source": [
    "# Run the setup script to ensure Redis and Agent Memory Server are running\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to setup script\n",
    "setup_script = Path(\"../../reference-agent/setup_agent_memory_server.py\")\n",
    "\n",
    "if setup_script.exists():\n",
    "    print(\"Running automated setup check...\\n\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(setup_script)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"‚ö†Ô∏è  Setup check failed. Please review the output above.\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All services are ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Setup script not found. Please ensure services are running manually.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ab2a448dd08fc",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8400bfed20f64",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already installed the reference-agent package, uncomment and run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ad9f5d109351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install reference-agent package\n",
    "# %pip install -q -e ../../reference-agent\n",
    "\n",
    "# Uncomment to install agent-memory-client\n",
    "# %pip install -q agent-memory-client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bf6b02f73fdb9",
   "metadata": {},
   "source": [
    "### Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00247fc4bb718d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AgentMemoryClient' from 'agent_memory_client' (/Users/nitin.kanukolanu/workspace/redis-ai-resources/python-recipes/context-engineering/venv/lib/python3.12/site-packages/agent_memory_client/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMessage, HumanMessage, AIMessage, SystemMessage\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Redis and Agent Memory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magent_memory_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentMemoryClient\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magent_memory_client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ClientMemoryRecord\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Token counting\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AgentMemoryClient' from 'agent_memory_client' (/Users/nitin.kanukolanu/workspace/redis-ai-resources/python-recipes/context-engineering/venv/lib/python3.12/site-packages/agent_memory_client/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Redis and Agent Memory\n",
    "from agent_memory_client import AgentMemoryClient\n",
    "from agent_memory_client.models import ClientMemoryRecord\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "# For visualization\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38946d91e830639a",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a3192aacee6dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables configured\n",
      "   Redis URL: redis://localhost:6379\n",
      "   Agent Memory URL: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from reference-agent directory\n",
    "env_path = Path(\"../../reference-agent/.env\")\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(f\"\"\"‚ùå OPENAI_API_KEY not found!\n",
    "\n",
    "Please create a .env file at: {env_path.absolute()}\n",
    "\n",
    "With the following content:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "REDIS_URL=redis://localhost:6379\n",
    "AGENT_MEMORY_URL=http://localhost:8088\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"‚úÖ Environment variables configured\")\n",
    "    print(f\"   Redis URL: {REDIS_URL}\")\n",
    "    print(f\"   Agent Memory URL: {AGENT_MEMORY_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42157025d92c5",
   "metadata": {},
   "source": [
    "### Initialize Clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6acdabe9f826582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AgentMemoryClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m embeddings = OpenAIEmbeddings(\n\u001b[32m      9\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Initialize Agent Memory Client\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m memory_client = \u001b[43mAgentMemoryClient\u001b[49m(\n\u001b[32m     14\u001b[39m     base_url=AGENT_MEMORY_URL\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Initialize tokenizer for counting\u001b[39;00m\n\u001b[32m     18\u001b[39m tokenizer = tiktoken.encoding_for_model(\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'AgentMemoryClient' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Initialize Agent Memory Client\n",
    "memory_client = AgentMemoryClient(\n",
    "    base_url=AGENT_MEMORY_URL\n",
    ")\n",
    "\n",
    "# Initialize tokenizer for counting\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "print(\"‚úÖ Clients initialized\")\n",
    "print(f\"   LLM: {llm.model_name}\")\n",
    "print(f\"   Embeddings: text-embedding-3-small\")\n",
    "print(f\"   Memory Server: {AGENT_MEMORY_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c6e2d8cee7f21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Understanding Conversation Growth\n",
    "\n",
    "Let's visualize how conversations grow and understand the implications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4a48ea4fee96b",
   "metadata": {},
   "source": [
    "### Demo 1: Token Growth Over Time\n",
    "\n",
    "Let's simulate how token counts grow as conversations progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7e262cad76878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt (constant across all turns)\n",
    "system_prompt = \"\"\"You are a helpful course advisor for Redis University.\n",
    "Help students find courses, check prerequisites, and plan their schedule.\n",
    "Be friendly, concise, and accurate.\"\"\"\n",
    "\n",
    "system_tokens = count_tokens(system_prompt)\n",
    "\n",
    "print(f\"System prompt: {system_tokens} tokens\\n\")\n",
    "\n",
    "# Simulate conversation growth\n",
    "# Assume average message pair (user + assistant) = 100 tokens\n",
    "avg_message_pair_tokens = 100\n",
    "\n",
    "print(\"Conversation Growth Simulation:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Turn':<8} {'Messages':<10} {'Conv Tokens':<15} {'Total Tokens':<15} {'Cost ($)':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for turn in [1, 5, 10, 20, 30, 50, 75, 100, 150, 200]:\n",
    "    # Each turn = user message + assistant message\n",
    "    num_messages = turn * 2\n",
    "    conversation_tokens = num_messages * (avg_message_pair_tokens // 2)\n",
    "    total_tokens = system_tokens + conversation_tokens\n",
    "    \n",
    "    # Cost calculation (GPT-4o input: $0.0025 per 1K tokens)\n",
    "    cost_per_query = (total_tokens / 1000) * 0.0025\n",
    "    \n",
    "    # Visual indicator\n",
    "    if total_tokens < 5000:\n",
    "        indicator = \"‚úÖ\"\n",
    "    elif total_tokens < 20000:\n",
    "        indicator = \"‚ö†Ô∏è\"\n",
    "    else:\n",
    "        indicator = \"‚ùå\"\n",
    "    \n",
    "    print(f\"{turn:<8} {num_messages:<10} {conversation_tokens:<15,} {total_tokens:<15,} ${cost_per_query:<11.4f} {indicator}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Without management, conversations become expensive and slow!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99edd1b0325093b",
   "metadata": {},
   "source": [
    "### Demo 2: Cost Analysis\n",
    "\n",
    "Let's calculate the cumulative cost of long conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e0cfece6beaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conversation_cost(num_turns: int, avg_tokens_per_turn: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"Calculate cost metrics for a conversation.\"\"\"\n",
    "    system_tokens = 50  # Simplified\n",
    "    \n",
    "    # Cumulative cost (each turn includes all previous messages)\n",
    "    cumulative_tokens = 0\n",
    "    cumulative_cost = 0.0\n",
    "    \n",
    "    for turn in range(1, num_turns + 1):\n",
    "        # Total tokens for this turn\n",
    "        conversation_tokens = turn * avg_tokens_per_turn\n",
    "        total_tokens = system_tokens + conversation_tokens\n",
    "        \n",
    "        # Cost for this turn (input tokens)\n",
    "        turn_cost = (total_tokens / 1000) * 0.0025\n",
    "        cumulative_cost += turn_cost\n",
    "        cumulative_tokens += total_tokens\n",
    "    \n",
    "    return {\n",
    "        \"num_turns\": num_turns,\n",
    "        \"final_tokens\": system_tokens + (num_turns * avg_tokens_per_turn),\n",
    "        \"cumulative_tokens\": cumulative_tokens,\n",
    "        \"cumulative_cost\": cumulative_cost,\n",
    "        \"avg_cost_per_turn\": cumulative_cost / num_turns\n",
    "    }\n",
    "\n",
    "# Compare different conversation lengths\n",
    "print(\"Cost Analysis for Different Conversation Lengths:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Turns':<10} {'Final Tokens':<15} {'Cumulative Tokens':<20} {'Total Cost':<15} {'Avg/Turn'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for num_turns in [10, 25, 50, 100, 200]:\n",
    "    metrics = calculate_conversation_cost(num_turns)\n",
    "    print(f\"{metrics['num_turns']:<10} \"\n",
    "          f\"{metrics['final_tokens']:<15,} \"\n",
    "          f\"{metrics['cumulative_tokens']:<20,} \"\n",
    "          f\"${metrics['cumulative_cost']:<14.2f} \"\n",
    "          f\"${metrics['avg_cost_per_turn']:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Costs grow quadratically without memory management!\")\n",
    "print(\"   A 100-turn conversation costs ~$1.50 in total\")\n",
    "print(\"   A 200-turn conversation costs ~$6.00 in total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ca757272caef3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 2: Conversation Summarization\n",
    "\n",
    "Now let's implement intelligent summarization to manage long conversations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c9c59a8e344be",
   "metadata": {},
   "source": [
    "### Theory: What to Preserve vs. Compress\n",
    "\n",
    "**What to Preserve:**\n",
    "- ‚úÖ Key facts and decisions\n",
    "- ‚úÖ Student preferences and goals\n",
    "- ‚úÖ Important course recommendations\n",
    "- ‚úÖ Prerequisites and requirements\n",
    "- ‚úÖ Recent context (last few messages)\n",
    "\n",
    "**What to Compress:**\n",
    "- üì¶ Small talk and greetings\n",
    "- üì¶ Redundant information\n",
    "- üì¶ Old conversation details\n",
    "- üì¶ Resolved questions\n",
    "\n",
    "**When to Summarize:**\n",
    "- Token threshold exceeded (e.g., > 2000 tokens)\n",
    "- Message count threshold exceeded (e.g., > 10 messages)\n",
    "- Time-based (e.g., after 1 hour)\n",
    "- Manual trigger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998184e76d362bf3",
   "metadata": {},
   "source": [
    "### Implementation: ConversationSummarizer Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710bd8b0268c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConversationMessage:\n",
    "    \"\"\"Represents a single conversation message.\"\"\"\n",
    "    role: str  # \"user\", \"assistant\", \"system\"\n",
    "    content: str\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "    token_count: Optional[int] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.token_count is None:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "class ConversationSummarizer:\n",
    "    \"\"\"Manages conversation summarization to keep token counts manageable.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        token_threshold: int = 2000,\n",
    "        message_threshold: int = 10,\n",
    "        keep_recent: int = 4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the summarizer.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model for generating summaries\n",
    "            token_threshold: Summarize when total tokens exceed this\n",
    "            message_threshold: Summarize when message count exceeds this\n",
    "            keep_recent: Number of recent messages to keep unsummarized\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.token_threshold = token_threshold\n",
    "        self.message_threshold = message_threshold\n",
    "        self.keep_recent = keep_recent\n",
    "        \n",
    "        self.summarization_prompt = \"\"\"You are summarizing a conversation between a student and a course advisor.\n",
    "\n",
    "Create a concise summary that preserves:\n",
    "1. Key decisions made\n",
    "2. Important requirements or prerequisites discussed\n",
    "3. Student's goals, preferences, and constraints\n",
    "4. Specific courses mentioned and recommendations given\n",
    "5. Any problems or issues that need follow-up\n",
    "\n",
    "Format as bullet points. Be specific and actionable.\n",
    "\n",
    "Conversation to summarize:\n",
    "{conversation}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    def should_summarize(self, messages: List[ConversationMessage]) -> bool:\n",
    "        \"\"\"Determine if conversation needs summarization.\"\"\"\n",
    "        if len(messages) <= self.keep_recent:\n",
    "            return False\n",
    "        \n",
    "        total_tokens = sum(msg.token_count for msg in messages)\n",
    "        \n",
    "        return (total_tokens > self.token_threshold or \n",
    "                len(messages) > self.message_threshold)\n",
    "    \n",
    "    async def summarize_conversation(\n",
    "        self,\n",
    "        messages: List[ConversationMessage]\n",
    "    ) -> ConversationMessage:\n",
    "        \"\"\"Create intelligent summary of conversation messages.\"\"\"\n",
    "        # Format conversation for summarization\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{msg.role.title()}: {msg.content}\" \n",
    "            for msg in messages\n",
    "        ])\n",
    "        \n",
    "        # Generate summary using LLM\n",
    "        prompt = self.summarization_prompt.format(conversation=conversation_text)\n",
    "        response = await self.llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        summary_content = f\"[CONVERSATION SUMMARY]\\n{response.content}\"\n",
    "        \n",
    "        # Create summary message\n",
    "        summary_msg = ConversationMessage(\n",
    "            role=\"system\",\n",
    "            content=summary_content,\n",
    "            timestamp=messages[-1].timestamp\n",
    "        )\n",
    "        \n",
    "        return summary_msg\n",
    "    \n",
    "    async def compress_conversation(\n",
    "        self,\n",
    "        messages: List[ConversationMessage]\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"\n",
    "        Compress conversation by summarizing old messages and keeping recent ones.\n",
    "        \n",
    "        Returns:\n",
    "            List of messages: [summary] + [recent messages]\n",
    "        \"\"\"\n",
    "        if not self.should_summarize(messages):\n",
    "            return messages\n",
    "        \n",
    "        # Split into old and recent\n",
    "        old_messages = messages[:-self.keep_recent]\n",
    "        recent_messages = messages[-self.keep_recent:]\n",
    "        \n",
    "        if not old_messages:\n",
    "            return messages\n",
    "        \n",
    "        # Summarize old messages\n",
    "        summary = await self.summarize_conversation(old_messages)\n",
    "        \n",
    "        # Return summary + recent messages\n",
    "        return [summary] + recent_messages\n",
    "\n",
    "print(\"‚úÖ ConversationSummarizer class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441a3298bd38af8",
   "metadata": {},
   "source": [
    "### Demo 3: Test Summarization\n",
    "\n",
    "Let's test the summarizer with a sample conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5840eedf4a9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample long conversation\n",
    "sample_conversation = [\n",
    "    ConversationMessage(\"user\", \"Hi, I'm interested in learning about machine learning courses\"),\n",
    "    ConversationMessage(\"assistant\", \"Great! Redis University offers several ML courses. CS401 Machine Learning is our flagship course. It covers supervised learning, neural networks, and practical applications.\"),\n",
    "    ConversationMessage(\"user\", \"What are the prerequisites for CS401?\"),\n",
    "    ConversationMessage(\"assistant\", \"CS401 requires CS201 Data Structures and MATH301 Linear Algebra. Have you completed these courses?\"),\n",
    "    ConversationMessage(\"user\", \"I've completed CS101 but not CS201 yet\"),\n",
    "    ConversationMessage(\"assistant\", \"Perfect! CS201 is the next logical step. It covers algorithms and data structures essential for ML. It's offered every semester.\"),\n",
    "    ConversationMessage(\"user\", \"How difficult is MATH301?\"),\n",
    "    ConversationMessage(\"assistant\", \"MATH301 is moderately challenging. It covers vectors, matrices, and eigenvalues used in ML algorithms. Most students find it manageable with consistent practice.\"),\n",
    "    ConversationMessage(\"user\", \"Can I take both CS201 and MATH301 together?\"),\n",
    "    ConversationMessage(\"assistant\", \"Yes, that's a good combination! They complement each other well. Many students take them concurrently.\"),\n",
    "    ConversationMessage(\"user\", \"What about CS401 after that?\"),\n",
    "    ConversationMessage(\"assistant\", \"CS401 is perfect after completing both prerequisites. It's our most popular AI course with hands-on projects.\"),\n",
    "    ConversationMessage(\"user\", \"When is CS401 offered?\"),\n",
    "    ConversationMessage(\"assistant\", \"CS401 is offered in Fall and Spring semesters. The Fall section typically fills up quickly, so register early!\"),\n",
    "    ConversationMessage(\"user\", \"Great! What's the workload like?\"),\n",
    "    ConversationMessage(\"assistant\", \"CS401 requires about 10-12 hours per week including lectures, assignments, and projects. There are 4 major projects throughout the semester.\"),\n",
    "]\n",
    "\n",
    "# Calculate original metrics\n",
    "original_token_count = sum(msg.token_count for msg in sample_conversation)\n",
    "print(f\"Original conversation:\")\n",
    "print(f\"  Messages: {len(sample_conversation)}\")\n",
    "print(f\"  Total tokens: {original_token_count}\")\n",
    "print(f\"  Average tokens per message: {original_token_count / len(sample_conversation):.1f}\")\n",
    "\n",
    "# Test summarization\n",
    "summarizer = ConversationSummarizer(\n",
    "    llm=llm,\n",
    "    token_threshold=500,  # Low threshold for demo\n",
    "    message_threshold=10,\n",
    "    keep_recent=4\n",
    ")\n",
    "\n",
    "print(f\"\\nSummarizer configuration:\")\n",
    "print(f\"  Token threshold: {summarizer.token_threshold}\")\n",
    "print(f\"  Message threshold: {summarizer.message_threshold}\")\n",
    "print(f\"  Keep recent: {summarizer.keep_recent}\")\n",
    "\n",
    "# Check if summarization is needed\n",
    "should_summarize = summarizer.should_summarize(sample_conversation)\n",
    "print(f\"\\nShould summarize? {should_summarize}\")\n",
    "\n",
    "if should_summarize:\n",
    "    # Compress the conversation\n",
    "    compressed = await summarizer.compress_conversation(sample_conversation)\n",
    "    \n",
    "    compressed_token_count = sum(msg.token_count for msg in compressed)\n",
    "    token_savings = original_token_count - compressed_token_count\n",
    "    savings_percentage = (token_savings / original_token_count) * 100\n",
    "    \n",
    "    print(f\"\\nAfter summarization:\")\n",
    "    print(f\"  Messages: {len(compressed)}\")\n",
    "    print(f\"  Total tokens: {compressed_token_count}\")\n",
    "    print(f\"  Token savings: {token_savings} ({savings_percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nCompressed conversation structure:\")\n",
    "    for i, msg in enumerate(compressed):\n",
    "        role_icon = \"üìã\" if msg.role == \"system\" else \"üë§\" if msg.role == \"user\" else \"ü§ñ\"\n",
    "        content_preview = msg.content[:80].replace('\\n', ' ')\n",
    "        print(f\"  {i+1}. {role_icon} [{msg.role}] {content_preview}...\")\n",
    "        print(f\"     Tokens: {msg.token_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f1c4414f6d2a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Part 3: Context Compression Strategies\n",
    "\n",
    "Beyond summarization, there are other compression strategies. Let's implement and compare them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a9c3a31a589d0",
   "metadata": {},
   "source": [
    "### Theory: Three Compression Approaches\n",
    "\n",
    "**1. Truncation (Fast, Simple)**\n",
    "- Keep only the most recent N messages\n",
    "- ‚úÖ Pros: Fast, no LLM calls, predictable\n",
    "- ‚ùå Cons: Loses all old context, no intelligence\n",
    "\n",
    "**2. Priority-Based (Balanced)**\n",
    "- Score messages by importance, keep highest-scoring\n",
    "- ‚úÖ Pros: Preserves important context, no LLM calls\n",
    "- ‚ùå Cons: Requires good scoring logic, may lose temporal flow\n",
    "\n",
    "**3. Summarization (High Quality)**\n",
    "- Use LLM to create intelligent summaries\n",
    "- ‚úÖ Pros: Preserves meaning, high quality\n",
    "- ‚ùå Cons: Slower, costs tokens, requires LLM call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbd6185d7e1fd4",
   "metadata": {},
   "source": [
    "### Implementation: Three Compression Strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8486d8bc89f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressionStrategy:\n",
    "    \"\"\"Base class for compression strategies.\"\"\"\n",
    "    \n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress messages to fit within max_tokens.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TruncationStrategy(CompressionStrategy):\n",
    "    \"\"\"Keep only the most recent messages within token budget.\"\"\"\n",
    "    \n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Keep most recent messages within token budget.\"\"\"\n",
    "        compressed = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        # Work backwards from most recent\n",
    "        for msg in reversed(messages):\n",
    "            if total_tokens + msg.token_count <= max_tokens:\n",
    "                compressed.insert(0, msg)\n",
    "                total_tokens += msg.token_count\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return compressed\n",
    "\n",
    "class PriorityBasedStrategy(CompressionStrategy):\n",
    "    \"\"\"Keep highest-priority messages within token budget.\"\"\"\n",
    "    \n",
    "    def calculate_importance(self, msg: ConversationMessage) -> float:\n",
    "        \"\"\"\n",
    "        Calculate importance score for a message.\n",
    "        \n",
    "        Higher scores = more important.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        content_lower = msg.content.lower()\n",
    "        \n",
    "        # Course codes are important (CS401, MATH301, etc.)\n",
    "        if any(code in content_lower for code in ['cs', 'math', 'eng']):\n",
    "            score += 2.0\n",
    "        \n",
    "        # Questions are important\n",
    "        if '?' in msg.content:\n",
    "            score += 1.5\n",
    "        \n",
    "        # Prerequisites and requirements are important\n",
    "        if any(word in content_lower for word in ['prerequisite', 'require', 'need']):\n",
    "            score += 1.5\n",
    "        \n",
    "        # Preferences and goals are important\n",
    "        if any(word in content_lower for word in ['prefer', 'want', 'goal', 'interested']):\n",
    "            score += 1.0\n",
    "        \n",
    "        # User messages slightly more important (their needs)\n",
    "        if msg.role == 'user':\n",
    "            score += 0.5\n",
    "        \n",
    "        # Longer messages often have more content\n",
    "        if msg.token_count > 50:\n",
    "            score += 0.5\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Keep highest-priority messages within token budget.\"\"\"\n",
    "        # Score each message\n",
    "        scored_messages = [\n",
    "            (self.calculate_importance(msg), i, msg)\n",
    "            for i, msg in enumerate(messages)\n",
    "        ]\n",
    "        \n",
    "        # Sort by score (descending), then by index to maintain some order\n",
    "        scored_messages.sort(key=lambda x: (-x[0], x[1]))\n",
    "        \n",
    "        # Select messages within budget\n",
    "        selected = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for score, idx, msg in scored_messages:\n",
    "            if total_tokens + msg.token_count <= max_tokens:\n",
    "                selected.append((idx, msg))\n",
    "                total_tokens += msg.token_count\n",
    "        \n",
    "        # Sort by original index to maintain conversation flow\n",
    "        selected.sort(key=lambda x: x[0])\n",
    "        \n",
    "        return [msg for idx, msg in selected]\n",
    "\n",
    "class SummarizationStrategy(CompressionStrategy):\n",
    "    \"\"\"Use LLM to create intelligent summaries.\"\"\"\n",
    "    \n",
    "    def __init__(self, summarizer: ConversationSummarizer):\n",
    "        self.summarizer = summarizer\n",
    "    \n",
    "    async def compress_async(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress using summarization (async).\"\"\"\n",
    "        # Use the summarizer's logic\n",
    "        return await self.summarizer.compress_conversation(messages)\n",
    "    \n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Synchronous wrapper (not recommended, use compress_async).\"\"\"\n",
    "        raise NotImplementedError(\"Use compress_async for summarization strategy\")\n",
    "\n",
    "print(\"‚úÖ Compression strategies defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db188fb9f01d750",
   "metadata": {},
   "source": [
    "### Demo 4: Compare Compression Strategies\n",
    "\n",
    "Let's compare all three strategies on the same conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49f8f61e276661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same sample conversation from before\n",
    "test_conversation = sample_conversation.copy()\n",
    "max_tokens = 800  # Target token budget\n",
    "\n",
    "print(f\"Original conversation: {len(test_conversation)} messages, {sum(msg.token_count for msg in test_conversation)} tokens\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Strategy 1: Truncation\n",
    "truncation = TruncationStrategy()\n",
    "truncated = truncation.compress(test_conversation, max_tokens)\n",
    "truncated_tokens = sum(msg.token_count for msg in truncated)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  TRUNCATION STRATEGY\")\n",
    "print(f\"   Result: {len(truncated)} messages, {truncated_tokens} tokens\")\n",
    "print(f\"   Savings: {sum(msg.token_count for msg in test_conversation) - truncated_tokens} tokens\")\n",
    "print(f\"   Kept messages: {[i for i, msg in enumerate(test_conversation) if msg in truncated]}\")\n",
    "\n",
    "# Strategy 2: Priority-Based\n",
    "priority = PriorityBasedStrategy()\n",
    "prioritized = priority.compress(test_conversation, max_tokens)\n",
    "prioritized_tokens = sum(msg.token_count for msg in prioritized)\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  PRIORITY-BASED STRATEGY\")\n",
    "print(f\"   Result: {len(prioritized)} messages, {prioritized_tokens} tokens\")\n",
    "print(f\"   Savings: {sum(msg.token_count for msg in test_conversation) - prioritized_tokens} tokens\")\n",
    "print(f\"   Kept messages: {[i for i, msg in enumerate(test_conversation) if msg in prioritized]}\")\n",
    "\n",
    "# Show importance scores for a few messages\n",
    "print(f\"\\n   Sample importance scores:\")\n",
    "for i in [0, 2, 4, 6]:\n",
    "    if i < len(test_conversation):\n",
    "        score = priority.calculate_importance(test_conversation[i])\n",
    "        preview = test_conversation[i].content[:50]\n",
    "        print(f\"     Message {i}: {score:.1f} - \\\"{preview}...\\\"\")\n",
    "\n",
    "# Strategy 3: Summarization\n",
    "summarization = SummarizationStrategy(summarizer)\n",
    "summarized = await summarization.compress_async(test_conversation, max_tokens)\n",
    "summarized_tokens = sum(msg.token_count for msg in summarized)\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  SUMMARIZATION STRATEGY\")\n",
    "print(f\"   Result: {len(summarized)} messages, {summarized_tokens} tokens\")\n",
    "print(f\"   Savings: {sum(msg.token_count for msg in test_conversation) - summarized_tokens} tokens\")\n",
    "print(f\"   Structure: 1 summary + {len(summarized) - 1} recent messages\")\n",
    "\n",
    "# Comparison table\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nüìä COMPARISON SUMMARY\")\n",
    "print(f\"{'Strategy':<20} {'Messages':<12} {'Tokens':<12} {'Savings':<12} {'Quality'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "original_tokens = sum(msg.token_count for msg in test_conversation)\n",
    "\n",
    "strategies = [\n",
    "    (\"Original\", len(test_conversation), original_tokens, 0, \"N/A\"),\n",
    "    (\"Truncation\", len(truncated), truncated_tokens, original_tokens - truncated_tokens, \"Low\"),\n",
    "    (\"Priority-Based\", len(prioritized), prioritized_tokens, original_tokens - prioritized_tokens, \"Medium\"),\n",
    "    (\"Summarization\", len(summarized), summarized_tokens, original_tokens - summarized_tokens, \"High\"),\n",
    "]\n",
    "\n",
    "for name, msgs, tokens, savings, quality in strategies:\n",
    "    savings_pct = f\"({savings/original_tokens*100:.0f}%)\" if savings > 0 else \"\"\n",
    "    print(f\"{name:<20} {msgs:<12} {tokens:<12} {savings:<5} {savings_pct:<6} {quality}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Choose strategy based on your quality/speed requirements!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290935fa536cb8aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 4: Agent Memory Server Integration\n",
    "\n",
    "The Agent Memory Server provides automatic summarization. Let's configure and test it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37993b003426e127",
   "metadata": {},
   "source": [
    "### Theory: Automatic Memory Management\n",
    "\n",
    "**Agent Memory Server Features:**\n",
    "- ‚úÖ Automatic summarization when thresholds are exceeded\n",
    "- ‚úÖ Configurable strategies (recent + summary, sliding window, full summary)\n",
    "- ‚úÖ Transparent to your application code\n",
    "- ‚úÖ Production-ready and scalable\n",
    "\n",
    "**How It Works:**\n",
    "1. You add messages to working memory normally\n",
    "2. Server monitors message count and token count\n",
    "3. When threshold is exceeded, server automatically summarizes\n",
    "4. Old messages are replaced with summary\n",
    "5. Recent messages are kept for context\n",
    "6. Your application retrieves the compressed memory\n",
    "\n",
    "**Configuration Options:**\n",
    "- `message_threshold`: Summarize after N messages (default: 20)\n",
    "- `token_threshold`: Summarize after N tokens (default: 4000)\n",
    "- `keep_recent`: Number of recent messages to keep (default: 4)\n",
    "- `strategy`: \"recent_plus_summary\", \"sliding_window\", or \"full_summary\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39408752c4a504",
   "metadata": {},
   "source": [
    "### Demo 5: Test Automatic Summarization\n",
    "\n",
    "Let's test the Agent Memory Server's automatic summarization with a long conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca0c3b7f31459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test session\n",
    "test_session_id = f\"long_conversation_test_{int(time.time())}\"\n",
    "test_student_id = \"student_memory_test\"\n",
    "\n",
    "print(f\"Testing automatic summarization\")\n",
    "print(f\"Session ID: {test_session_id}\")\n",
    "print(f\"Student ID: {test_student_id}\\n\")\n",
    "\n",
    "# Simulate a long conversation (25 turns = 50 messages)\n",
    "print(\"Simulating 25-turn conversation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "conversation_turns = [\n",
    "    (\"I'm interested in machine learning\", \"Great! CS401 Machine Learning is perfect for you.\"),\n",
    "    (\"What are the prerequisites?\", \"You'll need CS201 Data Structures and MATH301 Linear Algebra.\"),\n",
    "    (\"I've completed CS101\", \"Perfect! CS201 is your next step.\"),\n",
    "    (\"How difficult is CS201?\", \"It's moderately challenging but very rewarding.\"),\n",
    "    (\"When is it offered?\", \"CS201 is offered every semester - Fall, Spring, and Summer.\"),\n",
    "    (\"What about MATH301?\", \"MATH301 covers linear algebra essentials for ML.\"),\n",
    "    (\"Can I take both together?\", \"Yes, many students take CS201 and MATH301 concurrently.\"),\n",
    "    (\"How long will it take?\", \"If you take both, you can start CS401 in about 4-6 months.\"),\n",
    "    (\"What's the workload?\", \"Expect 10-12 hours per week for each course.\"),\n",
    "    (\"Are there online options?\", \"Yes, both courses have online and in-person sections.\"),\n",
    "    (\"Which format is better?\", \"Online offers flexibility, in-person offers more interaction.\"),\n",
    "    (\"What about CS401 after that?\", \"CS401 is our flagship ML course with hands-on projects.\"),\n",
    "    (\"How many projects?\", \"CS401 has 4 major projects throughout the semester.\"),\n",
    "    (\"What topics are covered?\", \"Supervised learning, neural networks, deep learning, and NLP.\"),\n",
    "    (\"Is there a final exam?\", \"Yes, there's a comprehensive final exam worth 30% of your grade.\"),\n",
    "    (\"What's the pass rate?\", \"About 85% of students pass CS401 on their first attempt.\"),\n",
    "    (\"Are there TAs available?\", \"Yes, we have 3 TAs for CS401 with office hours daily.\"),\n",
    "    (\"What programming language?\", \"CS401 uses Python with TensorFlow and PyTorch.\"),\n",
    "    (\"Do I need a GPU?\", \"Recommended but not required. We provide cloud GPU access.\"),\n",
    "    (\"What's the class size?\", \"CS401 typically has 30-40 students per section.\"),\n",
    "    (\"Can I audit the course?\", \"Yes, auditing is available but you won't get credit.\"),\n",
    "    (\"What's the cost?\", \"CS401 is $1,200 for credit, $300 for audit.\"),\n",
    "    (\"Are there scholarships?\", \"Yes, we offer merit-based scholarships. Apply early!\"),\n",
    "    (\"When should I apply?\", \"Applications open 2 months before each semester starts.\"),\n",
    "    (\"Thanks for the help!\", \"You're welcome! Feel free to reach out with more questions.\"),\n",
    "]\n",
    "\n",
    "# Add messages to working memory\n",
    "for i, (user_msg, assistant_msg) in enumerate(conversation_turns, 1):\n",
    "    # Add user message\n",
    "    await memory_client.add_messages(\n",
    "        session_id=test_session_id,\n",
    "        user_id=test_student_id,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_msg}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Show progress every 5 turns\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Turn {i:2d}: Added messages (total: {i*2} messages)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Added {len(conversation_turns)} turns ({len(conversation_turns)*2} messages)\")\n",
    "\n",
    "# Retrieve working memory to see if summarization occurred\n",
    "working_memory = await memory_client.get_messages(\n",
    "    session_id=test_session_id,\n",
    "    user_id=test_student_id\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Working Memory Status:\")\n",
    "print(f\"   Messages in memory: {len(working_memory)}\")\n",
    "print(f\"   Original messages added: {len(conversation_turns)*2}\")\n",
    "\n",
    "if len(working_memory) < len(conversation_turns)*2:\n",
    "    print(f\"   ‚úÖ Automatic summarization occurred!\")\n",
    "    print(f\"   Compression: {len(conversation_turns)*2} ‚Üí {len(working_memory)} messages\")\n",
    "\n",
    "    # Check for summary message\n",
    "    summary_messages = [msg for msg in working_memory if '[SUMMARY]' in msg.get('content', '') or msg.get('role') == 'system']\n",
    "    if summary_messages:\n",
    "        print(f\"   Summary messages found: {len(summary_messages)}\")\n",
    "        print(f\"\\n   Summary preview:\")\n",
    "        for msg in summary_messages[:1]:  # Show first summary\n",
    "            content_preview = msg.get('content', '')[:200].replace('\\n', ' ')\n",
    "            print(f\"   {content_preview}...\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è  No summarization yet (threshold not reached)\")\n",
    "\n",
    "# Calculate token savings\n",
    "original_tokens = sum(count_tokens(user_msg) + count_tokens(assistant_msg) for user_msg, assistant_msg in conversation_turns)\n",
    "current_tokens = sum(count_tokens(msg.get('content', '')) for msg in working_memory)\n",
    "\n",
    "print(f\"\\nüí∞ Token Analysis:\")\n",
    "print(f\"   Original tokens: {original_tokens}\")\n",
    "print(f\"   Current tokens: {current_tokens}\")\n",
    "if current_tokens < original_tokens:\n",
    "    savings = original_tokens - current_tokens\n",
    "    savings_pct = (savings / original_tokens) * 100\n",
    "    print(f\"   Token savings: {savings} ({savings_pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41ae7eb2d88f5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 5: Decision Framework\n",
    "\n",
    "How do you choose which compression strategy to use? Let's build a decision framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb87c914424cd",
   "metadata": {},
   "source": [
    "### Theory: Choosing the Right Strategy\n",
    "\n",
    "**Decision Factors:**\n",
    "\n",
    "1. **Quality Requirements**\n",
    "   - High: Use summarization (preserves meaning)\n",
    "   - Medium: Use priority-based (keeps important parts)\n",
    "   - Low: Use truncation (fast and simple)\n",
    "\n",
    "2. **Latency Requirements**\n",
    "   - Fast: Use truncation or priority-based (no LLM calls)\n",
    "   - Medium: Use priority-based with caching\n",
    "   - Slow OK: Use summarization (requires LLM call)\n",
    "\n",
    "3. **Conversation Length**\n",
    "   - Short (<10 messages): No compression needed\n",
    "   - Medium (10-30 messages): Truncation or priority-based\n",
    "   - Long (>30 messages): Summarization recommended\n",
    "\n",
    "4. **Cost Sensitivity**\n",
    "   - High: Use truncation or priority-based (no LLM costs)\n",
    "   - Medium: Use summarization with caching\n",
    "   - Low: Use summarization freely\n",
    "\n",
    "5. **Context Importance**\n",
    "   - Critical: Use summarization (preserves all important info)\n",
    "   - Important: Use priority-based (keeps high-value messages)\n",
    "   - Less critical: Use truncation (simple and fast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b904a38b1bad2b9",
   "metadata": {},
   "source": [
    "### Implementation: Decision Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fce6b8d81c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Literal\n",
    "\n",
    "class CompressionChoice(Enum):\n",
    "    \"\"\"Available compression strategies.\"\"\"\n",
    "    NONE = \"none\"\n",
    "    TRUNCATION = \"truncation\"\n",
    "    PRIORITY = \"priority\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "\n",
    "def choose_compression_strategy(\n",
    "    conversation_length: int,\n",
    "    token_count: int,\n",
    "    quality_requirement: Literal[\"high\", \"medium\", \"low\"],\n",
    "    latency_requirement: Literal[\"fast\", \"medium\", \"slow_ok\"],\n",
    "    cost_sensitivity: Literal[\"high\", \"medium\", \"low\"] = \"medium\"\n",
    ") -> CompressionChoice:\n",
    "    \"\"\"\n",
    "    Decision framework for choosing compression strategy.\n",
    "\n",
    "    Args:\n",
    "        conversation_length: Number of messages in conversation\n",
    "        token_count: Total token count\n",
    "        quality_requirement: How important is quality? (\"high\", \"medium\", \"low\")\n",
    "        latency_requirement: How fast must it be? (\"fast\", \"medium\", \"slow_ok\")\n",
    "        cost_sensitivity: How sensitive to costs? (\"high\", \"medium\", \"low\")\n",
    "\n",
    "    Returns:\n",
    "        CompressionChoice: Recommended strategy\n",
    "    \"\"\"\n",
    "    # No compression needed for short conversations\n",
    "    if token_count < 2000 and conversation_length < 10:\n",
    "        return CompressionChoice.NONE\n",
    "\n",
    "    # Fast requirement = no LLM calls\n",
    "    if latency_requirement == \"fast\":\n",
    "        if quality_requirement == \"high\":\n",
    "            return CompressionChoice.PRIORITY\n",
    "        else:\n",
    "            return CompressionChoice.TRUNCATION\n",
    "\n",
    "    # High cost sensitivity = avoid LLM calls\n",
    "    if cost_sensitivity == \"high\":\n",
    "        return CompressionChoice.PRIORITY if quality_requirement != \"low\" else CompressionChoice.TRUNCATION\n",
    "\n",
    "    # High quality + willing to wait = summarization\n",
    "    if quality_requirement == \"high\" and latency_requirement == \"slow_ok\":\n",
    "        return CompressionChoice.SUMMARIZATION\n",
    "\n",
    "    # Long conversations benefit from summarization\n",
    "    if conversation_length > 30 and quality_requirement != \"low\":\n",
    "        return CompressionChoice.SUMMARIZATION\n",
    "\n",
    "    # Medium quality = priority-based\n",
    "    if quality_requirement == \"medium\":\n",
    "        return CompressionChoice.PRIORITY\n",
    "\n",
    "    # Default to truncation for simple cases\n",
    "    return CompressionChoice.TRUNCATION\n",
    "\n",
    "print(\"‚úÖ Decision framework defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324715c96096689",
   "metadata": {},
   "source": [
    "### Demo 6: Test Decision Framework\n",
    "\n",
    "Let's test the decision framework with various scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb98376eb2b00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test scenarios\n",
    "scenarios = [\n",
    "    # (length, tokens, quality, latency, cost, description)\n",
    "    (5, 1000, \"high\", \"fast\", \"medium\", \"Short conversation, high quality needed\"),\n",
    "    (15, 3000, \"high\", \"slow_ok\", \"low\", \"Medium conversation, quality critical\"),\n",
    "    (30, 8000, \"medium\", \"medium\", \"medium\", \"Long conversation, balanced needs\"),\n",
    "    (50, 15000, \"high\", \"slow_ok\", \"medium\", \"Very long, quality important\"),\n",
    "    (100, 30000, \"low\", \"fast\", \"high\", \"Extremely long, cost-sensitive\"),\n",
    "    (20, 5000, \"medium\", \"fast\", \"high\", \"Medium length, fast and cheap\"),\n",
    "    (40, 12000, \"high\", \"medium\", \"low\", \"Long conversation, quality focus\"),\n",
    "    (8, 1500, \"low\", \"fast\", \"high\", \"Short, simple case\"),\n",
    "]\n",
    "\n",
    "print(\"Decision Framework Test Scenarios:\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Scenario':<45} {'Length':<8} {'Tokens':<10} {'Quality':<10} {'Latency':<10} {'Cost':<8} {'Strategy'}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for length, tokens, quality, latency, cost, description in scenarios:\n",
    "    strategy = choose_compression_strategy(length, tokens, quality, latency, cost)\n",
    "    print(f\"{description:<45} {length:<8} {tokens:<10,} {quality:<10} {latency:<10} {cost:<8} {strategy.value}\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Short conversations (<10 messages, <2000 tokens) ‚Üí No compression\")\n",
    "print(\"   ‚Ä¢ Fast requirement ‚Üí Truncation or Priority-based (no LLM calls)\")\n",
    "print(\"   ‚Ä¢ High quality + willing to wait ‚Üí Summarization\")\n",
    "print(\"   ‚Ä¢ Long conversations (>30 messages) ‚Üí Summarization recommended\")\n",
    "print(\"   ‚Ä¢ Cost-sensitive ‚Üí Avoid summarization, use Priority-based\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63fdaf5a2a2587",
   "metadata": {},
   "source": [
    "### Production Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824592502d5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè≠ PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  FOR MOST APPLICATIONS (Balanced)\")\n",
    "print(\"   Strategy: Agent Memory Server with automatic summarization\")\n",
    "print(\"   Configuration:\")\n",
    "print(\"     ‚Ä¢ message_threshold: 20 messages\")\n",
    "print(\"     ‚Ä¢ token_threshold: 4000 tokens\")\n",
    "print(\"     ‚Ä¢ keep_recent: 4 messages\")\n",
    "print(\"     ‚Ä¢ strategy: 'recent_plus_summary'\")\n",
    "print(\"   Why: Automatic, transparent, production-ready\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  FOR HIGH-VOLUME, COST-SENSITIVE (Efficient)\")\n",
    "print(\"   Strategy: Priority-based compression\")\n",
    "print(\"   Configuration:\")\n",
    "print(\"     ‚Ä¢ max_tokens: 2000\")\n",
    "print(\"     ‚Ä¢ Custom importance scoring\")\n",
    "print(\"     ‚Ä¢ No LLM calls\")\n",
    "print(\"   Why: Fast, cheap, no external dependencies\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  FOR CRITICAL CONVERSATIONS (Quality)\")\n",
    "print(\"   Strategy: Manual summarization with review\")\n",
    "print(\"   Configuration:\")\n",
    "print(\"     ‚Ä¢ token_threshold: 5000\")\n",
    "print(\"     ‚Ä¢ Human review of summaries\")\n",
    "print(\"     ‚Ä¢ Store full conversation separately\")\n",
    "print(\"   Why: Maximum quality, human oversight\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  FOR REAL-TIME CHAT (Speed)\")\n",
    "print(\"   Strategy: Truncation with sliding window\")\n",
    "print(\"   Configuration:\")\n",
    "print(\"     ‚Ä¢ keep_recent: 10 messages\")\n",
    "print(\"     ‚Ä¢ No summarization\")\n",
    "print(\"     ‚Ä¢ Fast response required\")\n",
    "print(\"   Why: Minimal latency, simple implementation\")\n",
    "\n",
    "print(\"\\nüí° General Guidelines:\")\n",
    "print(\"   ‚Ä¢ Start with Agent Memory Server automatic summarization\")\n",
    "print(\"   ‚Ä¢ Monitor token usage and costs in production\")\n",
    "print(\"   ‚Ä¢ Adjust thresholds based on your use case\")\n",
    "print(\"   ‚Ä¢ Consider hybrid approaches (truncation + summarization)\")\n",
    "print(\"   ‚Ä¢ Always preserve critical information in long-term memory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1cd42e5cb65a39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí™ Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to reinforce your learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b283d8917e353",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Sliding Window Compression\n",
    "\n",
    "Create a sliding window compression that keeps only the last N messages:\n",
    "\n",
    "```python\n",
    "def compress_sliding_window(\n",
    "    messages: List[ConversationMessage],\n",
    "    window_size: int = 10\n",
    ") -> List[ConversationMessage]:\n",
    "    \"\"\"\n",
    "    Keep only the last N messages (sliding window).\n",
    "\n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        window_size: Number of recent messages to keep\n",
    "\n",
    "    Returns:\n",
    "        List of messages (last N messages)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "test_messages = sample_conversation.copy()\n",
    "windowed = compress_sliding_window(test_messages, window_size=6)\n",
    "print(f\"Original: {len(test_messages)} messages\")\n",
    "print(f\"After sliding window: {len(windowed)} messages\")\n",
    "```\n",
    "\n",
    "**Hint:** This is simpler than truncation - just return the last N messages!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d60c07d558dbe2",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Hybrid Compression\n",
    "\n",
    "Combine summarization + truncation for optimal results:\n",
    "\n",
    "```python\n",
    "async def compress_hybrid(\n",
    "    messages: List[ConversationMessage],\n",
    "    summarizer: ConversationSummarizer,\n",
    "    max_tokens: int = 2000\n",
    ") -> List[ConversationMessage]:\n",
    "    \"\"\"\n",
    "    Hybrid compression: Summarize old messages, truncate if still too large.\n",
    "\n",
    "    Steps:\n",
    "    1. First, try summarization\n",
    "    2. If still over budget, apply truncation to summary + recent messages\n",
    "    3. Ensure we stay within max_tokens\n",
    "\n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        summarizer: ConversationSummarizer instance\n",
    "        max_tokens: Maximum token budget\n",
    "\n",
    "    Returns:\n",
    "        Compressed messages within token budget\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "hybrid_result = await compress_hybrid(sample_conversation, summarizer, max_tokens=1000)\n",
    "print(f\"Hybrid compression: {len(hybrid_result)} messages, {sum(m.token_count for m in hybrid_result)} tokens\")\n",
    "```\n",
    "\n",
    "**Hint:** Use `summarizer.compress_conversation()` first, then apply truncation if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956554c8c979d1a4",
   "metadata": {},
   "source": [
    "### Exercise 3: Quality Comparison\n",
    "\n",
    "Test all compression strategies and compare quality:\n",
    "\n",
    "```python\n",
    "async def compare_compression_quality(\n",
    "    messages: List[ConversationMessage],\n",
    "    test_query: str = \"What courses did we discuss?\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare compression strategies by testing reference resolution.\n",
    "\n",
    "    Steps:\n",
    "    1. Compress using each strategy\n",
    "    2. Try to answer test_query using compressed context\n",
    "    3. Compare quality of responses\n",
    "    4. Measure token savings\n",
    "\n",
    "    Args:\n",
    "        messages: Original conversation\n",
    "        test_query: Question to test reference resolution\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    # Test if the agent can still answer questions after compression\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "quality_results = await compare_compression_quality(sample_conversation)\n",
    "print(\"Quality Comparison Results:\")\n",
    "for strategy, results in quality_results.items():\n",
    "    print(f\"{strategy}: {results}\")\n",
    "```\n",
    "\n",
    "**Hint:** Use the LLM to answer the test query with each compressed context and compare responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566e3ee779cc9b6",
   "metadata": {},
   "source": [
    "### Exercise 4: Custom Importance Scoring\n",
    "\n",
    "Improve the `calculate_importance()` function with domain-specific logic:\n",
    "\n",
    "```python\n",
    "def calculate_importance_enhanced(msg: ConversationMessage) -> float:\n",
    "    \"\"\"\n",
    "    Enhanced importance scoring for course advisor conversations.\n",
    "\n",
    "    Add scoring for:\n",
    "    - Specific course codes (CS401, MATH301, etc.) - HIGH\n",
    "    - Prerequisites and requirements - HIGH\n",
    "    - Student preferences and goals - HIGH\n",
    "    - Questions - MEDIUM\n",
    "    - Confirmations and acknowledgments - LOW\n",
    "    - Greetings and small talk - VERY LOW\n",
    "\n",
    "    Returns:\n",
    "        Importance score (0.0 to 5.0)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "for msg in sample_conversation[:5]:\n",
    "    score = calculate_importance_enhanced(msg)\n",
    "    print(f\"Score: {score:.1f} - {msg.content[:60]}...\")\n",
    "```\n",
    "\n",
    "**Hint:** Use regex to detect course codes, check for question marks, look for keywords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85f81eedf9cae1",
   "metadata": {},
   "source": [
    "### Exercise 5: Production Configuration\n",
    "\n",
    "Configure Agent Memory Server for your specific use case:\n",
    "\n",
    "```python\n",
    "# Scenario: High-volume customer support chatbot\n",
    "# Requirements:\n",
    "# - Handle 1000+ conversations per day\n",
    "# - Average conversation: 15-20 turns\n",
    "# - Cost-sensitive but quality important\n",
    "# - Response time: <2 seconds\n",
    "\n",
    "# Your task: Choose appropriate configuration\n",
    "production_config = {\n",
    "    \"message_threshold\": ???,  # When to trigger summarization\n",
    "    \"token_threshold\": ???,    # Token limit before summarization\n",
    "    \"keep_recent\": ???,        # How many recent messages to keep\n",
    "    \"strategy\": ???,           # Which strategy to use\n",
    "}\n",
    "\n",
    "# Justify your choices:\n",
    "print(\"Configuration Justification:\")\n",
    "print(f\"message_threshold: {production_config['message_threshold']} because...\")\n",
    "print(f\"token_threshold: {production_config['token_threshold']} because...\")\n",
    "print(f\"keep_recent: {production_config['keep_recent']} because...\")\n",
    "print(f\"strategy: {production_config['strategy']} because...\")\n",
    "```\n",
    "\n",
    "**Hint:** Consider the trade-offs between cost, quality, and latency for this specific scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6fb297080ad8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### **What You Learned:**\n",
    "\n",
    "1. ‚úÖ **The Long Conversation Problem**\n",
    "   - Token limits, cost implications, performance degradation\n",
    "   - Why unbounded growth is unsustainable\n",
    "   - Quadratic cost growth without management\n",
    "\n",
    "2. ‚úÖ **Conversation Summarization**\n",
    "   - What to preserve vs. compress\n",
    "   - When to trigger summarization (token/message thresholds)\n",
    "   - Implementation with `ConversationSummarizer` class\n",
    "   - LLM-based intelligent summarization\n",
    "\n",
    "3. ‚úÖ **Three Compression Strategies**\n",
    "   - **Truncation:** Fast, simple, loses context\n",
    "   - **Priority-based:** Balanced, intelligent, no LLM calls\n",
    "   - **Summarization:** High quality, preserves meaning, requires LLM\n",
    "   - Trade-offs between speed, quality, and cost\n",
    "\n",
    "4. ‚úÖ **Agent Memory Server Integration**\n",
    "   - Automatic summarization configuration\n",
    "   - Transparent memory management\n",
    "   - Production-ready solution\n",
    "   - Configurable thresholds and strategies\n",
    "\n",
    "5. ‚úÖ **Decision Framework**\n",
    "   - How to choose the right strategy\n",
    "   - Factors: quality, latency, cost, conversation length\n",
    "   - Production recommendations for different scenarios\n",
    "   - Hybrid approaches for optimal results\n",
    "\n",
    "### **What You Built:**\n",
    "\n",
    "- ‚úÖ `ConversationSummarizer` class for intelligent summarization\n",
    "- ‚úÖ Three compression strategy implementations (Truncation, Priority, Summarization)\n",
    "- ‚úÖ Decision framework for strategy selection\n",
    "- ‚úÖ Production configuration examples\n",
    "- ‚úÖ Comparison tools for evaluating strategies\n",
    "- ‚úÖ Token counting and cost analysis tools\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "üí° **\"Conversations grow unbounded without management\"**\n",
    "- Every turn adds tokens and cost\n",
    "- Eventually you'll hit limits\n",
    "- Costs grow quadratically (each turn includes all previous messages)\n",
    "\n",
    "üí° **\"Summarization preserves meaning while reducing tokens\"**\n",
    "- Use LLM to create intelligent summaries\n",
    "- Keep recent messages for immediate context\n",
    "- Store important facts in long-term memory\n",
    "\n",
    "üí° **\"Choose strategy based on requirements\"**\n",
    "- Quality-critical ‚Üí Summarization\n",
    "- Speed-critical ‚Üí Truncation or Priority-based\n",
    "- Balanced ‚Üí Agent Memory Server automatic\n",
    "- Cost-sensitive ‚Üí Priority-based\n",
    "\n",
    "üí° **\"Agent Memory Server handles this automatically\"**\n",
    "- Production-ready solution\n",
    "- Transparent to your application\n",
    "- Configurable for your needs\n",
    "- No manual intervention required\n",
    "\n",
    "### **Connection to Context Engineering:**\n",
    "\n",
    "This notebook completes the **Conversation Context** story from Section 1:\n",
    "\n",
    "1. **Section 1:** Introduced the 4 context types, including Conversation Context\n",
    "2. **Section 3, NB1:** Implemented working memory for conversation continuity\n",
    "3. **Section 3, NB2:** Integrated memory with RAG for stateful conversations\n",
    "4. **Section 3, NB3:** Managed long conversations with summarization and compression ‚Üê You are here\n",
    "\n",
    "**Next:** Section 4 will show how agents can actively manage their own memory using tools!\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "**Section 4: Tools and Agents**\n",
    "- Build agents that actively manage their own memory\n",
    "- Implement memory tools (store, search, retrieve)\n",
    "- Use LangGraph for agent workflows\n",
    "- Let the LLM decide when to summarize\n",
    "\n",
    "**Section 5: Production Optimization**\n",
    "- Performance measurement and monitoring\n",
    "- Hybrid retrieval strategies\n",
    "- Semantic tool selection\n",
    "- Quality assurance and validation\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Resources\n",
    "\n",
    "### **Documentation:**\n",
    "- [Agent Memory Server](https://github.com/redis/agent-memory-server) - Production memory management\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client library\n",
    "- [LangChain Memory](https://python.langchain.com/docs/modules/memory/) - Memory patterns\n",
    "- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Token counting tool\n",
    "- [tiktoken](https://github.com/openai/tiktoken) - Fast token counting library\n",
    "\n",
    "### **Research Papers:**\n",
    "- [Lost in the Middle](https://arxiv.org/abs/2307.03172) - Context Rot research showing performance degradation\n",
    "- [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401) - RAG fundamentals\n",
    "- [MemGPT](https://arxiv.org/abs/2310.08560) - Memory management for LLMs\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer architecture and context windows\n",
    "\n",
    "### **Related Notebooks:**\n",
    "- **Section 1, NB1:** Introduction to Context Engineering\n",
    "- **Section 1, NB2:** The Four Context Types\n",
    "- **Section 2, NB1:** RAG and Retrieved Context\n",
    "- **Section 3, NB1:** Memory Fundamentals and Integration\n",
    "- **Section 3, NB2:** Memory-Enhanced RAG and Agents\n",
    "- **Section 4, NB1:** Tools and LangGraph Fundamentals\n",
    "- **Section 4, NB2:** Redis University Course Advisor Agent\n",
    "- **Section 5, NB1:** Measuring and Optimizing Performance\n",
    "\n",
    "### **Tools and Libraries:**\n",
    "- **Redis:** Vector storage and memory backend\n",
    "- **Agent Memory Server:** Dual-memory architecture with automatic summarization\n",
    "- **LangChain:** LLM interaction framework\n",
    "- **LangGraph:** State management and agent workflows\n",
    "- **OpenAI:** GPT-4o for generation and summarization\n",
    "- **tiktoken:** Token counting for cost estimation\n",
    "\n",
    "---\n",
    "\n",
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "**Redis University - Context Engineering Course**\n",
    "\n",
    "**üéâ Congratulations!** You've completed Section 3: Memory Architecture!\n",
    "\n",
    "You now understand how to:\n",
    "- Build memory systems for AI agents\n",
    "- Integrate working and long-term memory\n",
    "- Manage long conversations with summarization\n",
    "- Choose the right compression strategy\n",
    "- Configure production-ready memory management\n",
    "\n",
    "**Ready for Section 4?** Let's build agents that actively manage their own memory using tools!\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

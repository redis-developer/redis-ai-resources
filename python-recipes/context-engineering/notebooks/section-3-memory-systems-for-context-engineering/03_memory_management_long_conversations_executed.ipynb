{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d06c497fe3df20b",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# üß† Section 3, Notebook 3: Memory Management - Handling Long Conversations\n",
    "\n",
    "**‚è±Ô∏è Estimated Time:** 50-60 minutes\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand** why long conversations need management (token limits, cost, performance)\n",
    "2. **Implement** conversation summarization to preserve key information\n",
    "3. **Build** context compression strategies (truncation, priority-based, summarization)\n",
    "4. **Configure** automatic memory management with Agent Memory Server\n",
    "5. **Decide** when to apply each technique based on conversation characteristics\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where We Are\n",
    "\n",
    "### **Your Journey So Far:**\n",
    "\n",
    "**Section 3, Notebook 1:** Memory Fundamentals\n",
    "- ‚úÖ Working memory for conversation continuity\n",
    "- ‚úÖ Long-term memory for persistent knowledge\n",
    "- ‚úÖ The grounding problem and reference resolution\n",
    "- ‚úÖ Memory types (semantic, episodic, message)\n",
    "\n",
    "**Section 3, Notebook 2:** Memory-Enhanced RAG\n",
    "- ‚úÖ Integrated all four context types\n",
    "- ‚úÖ Built complete memory-enhanced RAG system\n",
    "- ‚úÖ Demonstrated benefits of stateful conversations\n",
    "\n",
    "**Your memory system works!** It can:\n",
    "- Remember conversation history across turns\n",
    "- Store and retrieve long-term facts\n",
    "- Resolve references (\"it\", \"that course\")\n",
    "- Provide personalized recommendations\n",
    "\n",
    "### **But... What About Long Conversations?**\n",
    "\n",
    "**Questions we can't answer yet:**\n",
    "- ‚ùì What happens when conversations get really long?\n",
    "- ‚ùì How do we handle token limits?\n",
    "- ‚ùì How much does a 50-turn conversation cost?\n",
    "- ‚ùì Can we preserve important context while reducing tokens?\n",
    "- ‚ùì When should we summarize vs. truncate vs. keep everything?\n",
    "\n",
    "---\n",
    "\n",
    "## üö® The Long Conversation Problem\n",
    "\n",
    "Before diving into solutions, let's understand the fundamental problem.\n",
    "\n",
    "### **The Problem: Unbounded Growth**\n",
    "\n",
    "Every conversation turn adds messages to working memory:\n",
    "\n",
    "```\n",
    "Turn 1:  System (500) + Messages (200) = 700 tokens ‚úÖ\n",
    "Turn 5:  System (500) + Messages (1,000) = 1,500 tokens ‚úÖ\n",
    "Turn 20: System (500) + Messages (4,000) = 4,500 tokens ‚úÖ\n",
    "Turn 50: System (500) + Messages (10,000) = 10,500 tokens ‚ö†Ô∏è\n",
    "Turn 100: System (500) + Messages (20,000) = 20,500 tokens ‚ö†Ô∏è\n",
    "Turn 200: System (500) + Messages (40,000) = 40,500 tokens ‚ùå\n",
    "```\n",
    "\n",
    "**Without management, conversations grow unbounded!**\n",
    "\n",
    "### **Why This Matters**\n",
    "\n",
    "**1. Token Limits (Hard Constraint)**\n",
    "- GPT-4o: 128K tokens (~96,000 words)\n",
    "- GPT-3.5: 16K tokens (~12,000 words)\n",
    "- Eventually, you'll hit the limit and conversations fail\n",
    "\n",
    "**2. Cost (Economic Constraint)**\n",
    "- Input tokens cost money  (e.g. $0.0025 /  1K  tokens for GPT-4o)\n",
    "\n",
    "- A 50-turn conversation = ~10,000 tokens = $0.025 per query\n",
    "\n",
    "- Over 1,000 conversations = $25 just for conversation history!\n",
    "\n",
    "**3. Performance (Quality Constraint)**\n",
    "- More tokens = longer processing time\n",
    "- Context Rot: LLMs struggle with very long contexts\n",
    "- Important information gets \"lost in the middle\"\n",
    "\n",
    "**4. User Experience**\n",
    "- Slow responses frustrate users\n",
    "- Expensive conversations aren't sustainable\n",
    "- Failed conversations due to token limits are unacceptable\n",
    "\n",
    "### **The Solution: Memory Management**\n",
    "\n",
    "We need strategies to:\n",
    "- ‚úÖ Keep conversations within token budgets\n",
    "- ‚úÖ Preserve important information\n",
    "- ‚úÖ Maintain conversation quality\n",
    "- ‚úÖ Control costs\n",
    "- ‚úÖ Enable indefinite conversations\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Part 0: Setup and Environment\n",
    "\n",
    "Let's set up our environment and create tools for measuring conversation growth.\n",
    "\n",
    "### ‚ö†Ô∏è Prerequisites\n",
    "\n",
    "**Before running this notebook, make sure you have:**\n",
    "\n",
    "1. **Docker Desktop running** - Required for Redis and Agent Memory Server\n",
    "\n",
    "2. **Environment variables** - Create a `.env` file in the `reference-agent` directory:\n",
    "   ```bash\n",
    "   # Copy the example file\n",
    "   cd ../../reference-agent\n",
    "   cp .env.example .env\n",
    "\n",
    "   # Edit .env and add your OpenAI API key\n",
    "   # OPENAI_API_KEY=your_actual_openai_api_key_here\n",
    "   ```\n",
    "\n",
    "3. **Run the setup script** - This will automatically start Redis and Agent Memory Server:\n",
    "   ```bash\n",
    "   cd ../../reference-agent\n",
    "   python setup_agent_memory_server.py\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c59ecc51d30c3",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10e48e57f1431e",
   "metadata": {},
   "source": [
    "### Automated Setup Check\n",
    "\n",
    "Let's run the setup script to ensure all services are running properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808cea2af3f4f118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:12.149354Z",
     "iopub.status.busy": "2025-11-02T01:09:12.149256Z",
     "iopub.status.idle": "2025-11-02T01:09:12.404028Z",
     "shell.execute_reply": "2025-11-02T01:09:12.403476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running automated setup check...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Agent Memory Server Setup\n",
      "===========================\n",
      "üìä Checking Redis...\n",
      "‚úÖ Redis is running\n",
      "üìä Checking Agent Memory Server...\n",
      "üîç Agent Memory Server container exists. Checking health...\n",
      "‚úÖ Agent Memory Server is running and healthy\n",
      "‚úÖ No Redis connection issues detected\n",
      "\n",
      "‚úÖ Setup Complete!\n",
      "=================\n",
      "üìä Services Status:\n",
      "   ‚Ä¢ Redis: Running on port 6379\n",
      "   ‚Ä¢ Agent Memory Server: Running on port 8088\n",
      "\n",
      "üéØ You can now run the notebooks!\n",
      "\n",
      "\n",
      "‚úÖ All services are ready!\n"
     ]
    }
   ],
   "source": [
    "# Run the setup script to ensure Redis and Agent Memory Server are running\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to setup script\n",
    "setup_script = Path(\"../../reference-agent/setup_agent_memory_server.py\")\n",
    "\n",
    "if setup_script.exists():\n",
    "    print(\"Running automated setup check...\\n\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(setup_script)], capture_output=True, text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"‚ö†Ô∏è  Setup check failed. Please review the output above.\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All services are ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Setup script not found. Please ensure services are running manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ab2a448dd08fc",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8400bfed20f64",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already installed the reference-agent package, uncomment and run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ad9f5d109351a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:12.405399Z",
     "iopub.status.busy": "2025-11-02T01:09:12.405297Z",
     "iopub.status.idle": "2025-11-02T01:09:12.406937Z",
     "shell.execute_reply": "2025-11-02T01:09:12.406610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install reference-agent package\n",
    "# %pip install -q -e ../../reference-agent\n",
    "\n",
    "# Uncomment to install agent-memory-client\n",
    "# %pip install -q agent-memory-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bf6b02f73fdb9",
   "metadata": {},
   "source": [
    "### Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00247fc4bb718d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:12.408080Z",
     "iopub.status.busy": "2025-11-02T01:09:12.408022Z",
     "iopub.status.idle": "2025-11-02T01:09:14.659616Z",
     "shell.execute_reply": "2025-11-02T01:09:14.659086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "# For visualization\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "# Redis and Agent Memory\n",
    "from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "from agent_memory_client.models import ClientMemoryRecord, MemoryMessage, WorkingMemory\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# LangChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38946d91e830639a",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a3192aacee6dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.660925Z",
     "iopub.status.busy": "2025-11-02T01:09:14.660805Z",
     "iopub.status.idle": "2025-11-02T01:09:14.665197Z",
     "shell.execute_reply": "2025-11-02T01:09:14.664758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables configured\n",
      "   Redis URL: redis://localhost:6379\n",
      "   Agent Memory URL: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from reference-agent directory\n",
    "env_path = Path(\"../../reference-agent/.env\")\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\n",
    "        f\"\"\"‚ùå OPENAI_API_KEY not found!\n",
    "\n",
    "Please create a .env file at: {env_path.absolute()}\n",
    "\n",
    "With the following content:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "REDIS_URL=redis://localhost:6379\n",
    "AGENT_MEMORY_URL=http://localhost:8088\n",
    "\"\"\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úÖ Environment variables configured\")\n",
    "    print(f\"   Redis URL: {REDIS_URL}\")\n",
    "    print(f\"   Agent Memory URL: {AGENT_MEMORY_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42157025d92c5",
   "metadata": {},
   "source": [
    "### Initialize Clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6acdabe9f826582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.666265Z",
     "iopub.status.busy": "2025-11-02T01:09:14.666205Z",
     "iopub.status.idle": "2025-11-02T01:09:14.922557Z",
     "shell.execute_reply": "2025-11-02T01:09:14.922092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clients initialized\n",
      "   LLM: gpt-4o\n",
      "   Embeddings: text-embedding-3-small\n",
      "   Memory Server: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize Agent Memory Client\n",
    "memory_config = MemoryClientConfig(base_url=AGENT_MEMORY_URL)\n",
    "memory_client = MemoryAPIClient(config=memory_config)\n",
    "\n",
    "# Initialize tokenizer for counting\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Clients initialized\")\n",
    "print(f\"   LLM: {llm.model_name}\")\n",
    "print(f\"   Embeddings: text-embedding-3-small\")\n",
    "print(f\"   Memory Server: {AGENT_MEMORY_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c6e2d8cee7f21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Understanding Conversation Growth\n",
    "\n",
    "Let's visualize how conversations grow and understand the implications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4a48ea4fee96b",
   "metadata": {},
   "source": [
    "### üî¨ Research Context: Why Context Management Matters\n",
    "\n",
    "Modern LLMs have impressive context windows:\n",
    "- **GPT-4o**: 128K tokens (~96,000 words)\n",
    "- **Claude 3.5**: 200K tokens (~150,000 words)\n",
    "- **Gemini 1.5 Pro**: 1M tokens (~750,000 words)\n",
    "\n",
    "**But here's the problem:** Larger context windows don't guarantee better performance.\n",
    "\n",
    "#### The \"Lost in the Middle\" Problem\n",
    "\n",
    "Research by Liu et al. (2023) in their paper [\"Lost in the Middle: How Language Models Use Long Contexts\"](https://arxiv.org/abs/2307.03172) revealed critical findings:\n",
    "\n",
    "**Key Finding #1: U-Shaped Performance**\n",
    "- Models perform best when relevant information is at the **beginning** or **end** of context\n",
    "- Performance **significantly degrades** when information is in the **middle** of long contexts\n",
    "- This happens even with models explicitly designed for long contexts\n",
    "\n",
    "**Key Finding #2: Non-Uniform Degradation**\n",
    "- It's not just about hitting token limits\n",
    "- Quality degrades **even within the context window**\n",
    "- The longer the context, the worse the \"middle\" performance becomes\n",
    "\n",
    "**Key Finding #3: More Context ‚â† Better Results**\n",
    "- In some experiments, GPT-3.5 performed **worse** with retrieved documents than with no documents at all\n",
    "- Adding more context can actually **hurt** performance if not managed properly\n",
    "\n",
    "**Why This Matters for Memory Management:**\n",
    "- Simply storing all conversation history isn't optimal\n",
    "- We need **intelligent compression** to keep important information accessible\n",
    "- **Position matters**: Recent context (at the end) is naturally well-positioned\n",
    "- **Quality over quantity**: Better to have concise, relevant context than exhaustive history\n",
    "\n",
    "**References:**\n",
    "- Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). Lost in the Middle: How Language Models Use Long Contexts. *Transactions of the Association for Computational Linguistics (TACL)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7e262cad76878",
   "metadata": {},
   "source": [
    "### Demo 1: Token Growth Over Time\n",
    "\n",
    "Now let's see this problem in action by simulating conversation growth.\n",
    "\n",
    "#### Step 1: Define our system prompt and count its tokens\n",
    "\n",
    "**What:** Creating a system prompt and measuring its token count.\n",
    "\n",
    "**Why:** The system prompt is sent with EVERY request, so its size directly impacts costs. Understanding this baseline is crucial for budgeting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99edd1b0325093b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.923876Z",
     "iopub.status.busy": "2025-11-02T01:09:14.923775Z",
     "iopub.status.idle": "2025-11-02T01:09:14.926222Z",
     "shell.execute_reply": "2025-11-02T01:09:14.925827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: 31 tokens\n"
     ]
    }
   ],
   "source": [
    "# System prompt (constant across all turns)\n",
    "system_prompt = \"\"\"You are a helpful course advisor for Redis University.\n",
    "Help students find courses, check prerequisites, and plan their schedule.\n",
    "Be friendly, concise, and accurate.\"\"\"\n",
    "\n",
    "system_tokens = count_tokens(system_prompt)\n",
    "\n",
    "print(f\"System prompt: {system_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e0cfece6beaf5",
   "metadata": {},
   "source": [
    "#### Step 2: Simulate how tokens grow with each conversation turn\n",
    "\n",
    "**What:** Projecting token growth and costs across 1 to 200 conversation turns.\n",
    "\n",
    "**Why:** Visualizing the growth curve shows when conversations become expensive (>20K tokens) and helps you plan compression strategies. Notice how costs accelerate - this is the quadratic growth problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "117ca757272caef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.927323Z",
     "iopub.status.busy": "2025-11-02T01:09:14.927226Z",
     "iopub.status.idle": "2025-11-02T01:09:14.929730Z",
     "shell.execute_reply": "2025-11-02T01:09:14.929335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation Growth Simulation:\n",
      "================================================================================\n",
      "Turn     Messages   Conv Tokens     Total Tokens    Cost ($)    \n",
      "--------------------------------------------------------------------------------\n",
      "1        2          100             131             $0.0003      ‚úÖ\n",
      "5        10         500             531             $0.0013      ‚úÖ\n",
      "10       20         1,000           1,031           $0.0026      ‚úÖ\n",
      "20       40         2,000           2,031           $0.0051      ‚úÖ\n",
      "30       60         3,000           3,031           $0.0076      ‚úÖ\n",
      "50       100        5,000           5,031           $0.0126      ‚ö†Ô∏è\n",
      "75       150        7,500           7,531           $0.0188      ‚ö†Ô∏è\n",
      "100      200        10,000          10,031          $0.0251      ‚ö†Ô∏è\n",
      "150      300        15,000          15,031          $0.0376      ‚ö†Ô∏è\n",
      "200      400        20,000          20,031          $0.0501      ‚ùå\n"
     ]
    }
   ],
   "source": [
    "# Assume average message pair (user + assistant) = 100 tokens\n",
    "avg_message_pair_tokens = 100\n",
    "\n",
    "print(\"\\nConversation Growth Simulation:\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"{'Turn':<8} {'Messages':<10} {'Conv Tokens':<15} {'Total Tokens':<15} {'Cost ($)':<12}\"\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for turn in [1, 5, 10, 20, 30, 50, 75, 100, 150, 200]:\n",
    "    # Each turn = user message + assistant message\n",
    "    num_messages = turn * 2\n",
    "    conversation_tokens = num_messages * (avg_message_pair_tokens // 2)\n",
    "    total_tokens = system_tokens + conversation_tokens\n",
    "\n",
    "    # Cost calculation (GPT-4o input: $0.0025 per 1K tokens)\n",
    "    cost_per_query = (total_tokens / 1000) * 0.0025\n",
    "\n",
    "    # Visual indicator\n",
    "    if total_tokens < 5000:\n",
    "        indicator = \"‚úÖ\"\n",
    "    elif total_tokens < 20000:\n",
    "        indicator = \"‚ö†Ô∏è\"\n",
    "    else:\n",
    "        indicator = \"‚ùå\"\n",
    "\n",
    "    print(\n",
    "        f\"{turn:<8} {num_messages:<10} {conversation_tokens:<15,} {total_tokens:<15,} ${cost_per_query:<11.4f} {indicator}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c9c59a8e344be",
   "metadata": {},
   "source": [
    "### Demo 2: Cost Analysis\n",
    "\n",
    "Let's calculate the cumulative cost of long conversations.\n",
    "\n",
    "**Why costs grow quadratically:**\n",
    "- Turn 1: Process 100 tokens\n",
    "- Turn 2: Process 200 tokens (includes turn 1)\n",
    "- Turn 3: Process 300 tokens (includes turns 1 & 2)\n",
    "- Turn N: Process N√ó100 tokens\n",
    "\n",
    "Total cost = 100 + 200 + 300 + ... + N√ó100 = **O(N¬≤)** growth!\n",
    "\n",
    "#### Step 1: Create a function to calculate conversation costs\n",
    "\n",
    "**What:** Building a cost calculator that accounts for cumulative token processing.\n",
    "\n",
    "**Why:** Each turn processes ALL previous messages, so costs compound. This function reveals the true cost of long conversations - not just the final token count, but the sum of all API calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998184e76d362bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.930677Z",
     "iopub.status.busy": "2025-11-02T01:09:14.930598Z",
     "iopub.status.idle": "2025-11-02T01:09:14.932733Z",
     "shell.execute_reply": "2025-11-02T01:09:14.932377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cost calculation function defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_conversation_cost(\n",
    "    num_turns: int, avg_tokens_per_turn: int = 100\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate cost metrics for a conversation.\n",
    "\n",
    "    Args:\n",
    "        num_turns: Number of conversation turns\n",
    "        avg_tokens_per_turn: Average tokens per turn (user + assistant)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with cost metrics\n",
    "    \"\"\"\n",
    "    system_tokens = 50  # Simplified\n",
    "\n",
    "    # Cumulative cost (each turn includes all previous messages)\n",
    "    cumulative_tokens = 0\n",
    "    cumulative_cost = 0.0\n",
    "\n",
    "    for turn in range(1, num_turns + 1):\n",
    "        # Total tokens for this turn\n",
    "        conversation_tokens = turn * avg_tokens_per_turn\n",
    "        total_tokens = system_tokens + conversation_tokens\n",
    "\n",
    "        # Cost for this turn (input tokens)\n",
    "        turn_cost = (total_tokens / 1000) * 0.0025\n",
    "        cumulative_cost += turn_cost\n",
    "        cumulative_tokens += total_tokens\n",
    "\n",
    "    return {\n",
    "        \"num_turns\": num_turns,\n",
    "        \"final_tokens\": system_tokens + (num_turns * avg_tokens_per_turn),\n",
    "        \"cumulative_tokens\": cumulative_tokens,\n",
    "        \"cumulative_cost\": cumulative_cost,\n",
    "        \"avg_cost_per_turn\": cumulative_cost / num_turns,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Cost calculation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710bd8b0268c34d",
   "metadata": {},
   "source": [
    "#### Step 2: Compare costs across different conversation lengths\n",
    "\n",
    "**What:** Running cost projections for conversations from 10 to 200 turns.\n",
    "\n",
    "**Why:** Seeing the quadratic growth in action - a 200-turn conversation costs $1.26, but the cumulative cost across all turns is much higher. This motivates compression strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4441a3298bd38af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.935301Z",
     "iopub.status.busy": "2025-11-02T01:09:14.935202Z",
     "iopub.status.idle": "2025-11-02T01:09:14.937547Z",
     "shell.execute_reply": "2025-11-02T01:09:14.936972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Analysis for Different Conversation Lengths:\n",
      "================================================================================\n",
      "Turns      Final Tokens    Cumulative Tokens    Total Cost      Avg/Turn\n",
      "--------------------------------------------------------------------------------\n",
      "10         1,050           6,000                $0.02           $0.0015\n",
      "25         2,550           33,750               $0.08           $0.0034\n",
      "50         5,050           130,000              $0.33           $0.0065\n",
      "100        10,050          510,000              $1.27           $0.0127\n",
      "200        20,050          2,020,000            $5.05           $0.0253\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost Analysis for Different Conversation Lengths:\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"{'Turns':<10} {'Final Tokens':<15} {'Cumulative Tokens':<20} {'Total Cost':<15} {'Avg/Turn'}\"\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for num_turns in [10, 25, 50, 100, 200]:\n",
    "    metrics = calculate_conversation_cost(num_turns)\n",
    "    print(\n",
    "        f\"{metrics['num_turns']:<10} \"\n",
    "        f\"{metrics['final_tokens']:<15,} \"\n",
    "        f\"{metrics['cumulative_tokens']:<20,} \"\n",
    "        f\"${metrics['cumulative_cost']:<14.2f} \"\n",
    "        f\"${metrics['avg_cost_per_turn']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5840eedf4a9185",
   "metadata": {},
   "source": [
    "#### Key Takeaways\n",
    "\n",
    "**Without memory management:**\n",
    "- Costs grow **quadratically** (O(N¬≤))\n",
    "  \n",
    "- A 100-turn conversation costs ~$1.50 in total\n",
    "\n",
    "  \n",
    "- A 200-turn conversation costs ~$6.00 in total\n",
    "\n",
    "- At scale (1000s of users), this becomes unsustainable\n",
    "\n",
    "**The solution:** Intelligent memory management to keep conversations within budget while preserving quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f1c4414f6d2a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 2: Context Summarizaton\n",
    "\n",
    "**Context summarization** is the process of condensing conversation history into a compact representation that preserves essential information while dramatically reducing token count.\n",
    "\n",
    "Picture a chat assistant helping someone plan a wedding over 50 messages:\n",
    "- It captures the critical stuff: venue choice, budget, guest count, vendor decisions\n",
    "- It grabs the decisions and ditches the small talk\n",
    "- Later messages can reference \"the venue we picked\" without replaying the entire debate\n",
    "  \n",
    "**Same deal with LLM chats:**\n",
    "- Squash ancient messages into a tight little paragraph\n",
    "- Keep the gold (facts, choices, what the user loves/hates)\n",
    "- Leave fresh messages untouched (they're still doing work)\n",
    "- Slash token usage by 50-80% without lobotomizing the conversation\n",
    "\n",
    "### Why Should You Care About Summarization?\n",
    "\n",
    "Summarization tackles three gnarly problems:\n",
    "\n",
    "**1. Plays Nice With Token Caps (Callback to Part 1)**\n",
    "- Chats balloon up forever if you let them\n",
    "- Summarization keeps you from hitting the ceiling\n",
    "- **Real talk:** 50 messages (10K tokens) ‚Üí Compressed summary + 4 fresh messages (2.5K tokens)\n",
    "\n",
    "**2. Fixes the Context Rot Problem (Also From Part 1)**\n",
    "- Remember that \"Lost in the Middle\" mess? Old info gets buried and ignored\n",
    "- Summarization yanks that old stuff to the front in condensed form\n",
    "- Fresh messages chill at the end (where the model actually pays attention)\n",
    "- **Upshot:** Model performs better AND you save space‚Äîwin-win\n",
    "\n",
    "**3. Keeps Working Memory From Exploding (Throwback to Notebook 1)**\n",
    "- Working memory = your conversation backlog\n",
    "- Without summarization, it just keeps growing like a digital hoarder's closet\n",
    "- Summarization gives it a haircut regularly\n",
    "- **Payoff:** Conversations that can actually go the distance\n",
    "\n",
    "### When Should You Reach for This Tool?\n",
    "\n",
    "**Great for:**\n",
    "- ‚úÖ Marathon conversations (10+ back-and-forths)\n",
    "- ‚úÖ Chats that have a narrative arc (customer support, coaching sessions)\n",
    "- ‚úÖ Situations where you want history but not ALL the history\n",
    "- ‚úÖ When the recent stuff matters most\n",
    "\n",
    "**Skip it when:**\n",
    "- ‚ùå Quick exchanges (under 5 turns‚Äîdon't overthink it)\n",
    "- ‚ùå Every syllable counts (legal docs, medical consultations)\n",
    "- ‚ùå You might need verbatim quotes from way back\n",
    "- ‚ùå The extra LLM call for summarization costs too much time or money\n",
    "\n",
    "### Where Summarization Lives in Your Memory Stack\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                  Your LLM Agent Brain                   ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Context Window (128K tokens available)                 ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. System Prompt (500 tokens)                  ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. Long-term Memory Bank (1,000 tokens)        ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ 3. RAG Retrieval Stuff (2,000 tokens)          ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ 4. Working Memory Zone:                        ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ [COMPRESSED HISTORY] (500 tokens)    ‚îÇ    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ - Critical facts from rounds 1-20    ‚îÇ    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ - Decisions that were locked in      ‚îÇ    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ - User quirks and preferences        ‚îÇ    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Live Recent Messages (1,000 tokens)         ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round 21: User shot + Assistant reply     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round 22: User shot + Assistant reply     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round 23: User shot + Assistant reply     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round 24: User shot + Assistant reply     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ 5. Current Incoming Query (200 tokens)         ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Running total: ~5,200 tokens (instead of 15K‚Äînice!)    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "#### The Bottom Line: \n",
    "Summarization is a *compression technique* for working memory that maintains conversation continuity while keeping token counts manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a9c3a31a589d0",
   "metadata": {},
   "source": [
    "### üî¨ Research Foundation: Recursive Summarization\n",
    "\n",
    "Wang et al. (2023) in [\"Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models\"](https://arxiv.org/abs/2308.15022) demonstrated that:\n",
    "\n",
    "**Key Insight:** Recursive summarization enables LLMs to handle extremely long conversations by:\n",
    "1. Memorizing small dialogue contexts\n",
    "2. Recursively producing new memory using previous memory + new contexts\n",
    "3. Maintaining consistency across long conversations\n",
    "\n",
    "**Their findings:**\n",
    "- Improved response consistency in long-context conversations\n",
    "- Works well with both long-context models (8K, 16K) and retrieval-enhanced LLMs\n",
    "- Provides a practical solution for modeling extremely long contexts\n",
    "\n",
    "**Practical Application:**\n",
    "- Summarize old messages while keeping recent ones intact\n",
    "- Preserve key information (facts, decisions, preferences)\n",
    "- Compress redundant or less important information\n",
    "\n",
    "**References:**\n",
    "- Wang, Q., Fu, Y., Cao, Y., Wang, S., Tian, Z., & Ding, L. (2023). Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. *Neurocomputing* (Accepted).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbd6185d7e1fd4",
   "metadata": {},
   "source": [
    "### Theory: What to Preserve vs. Compress\n",
    "\n",
    "When summarizing conversations, we need to be strategic about what to keep and what to compress.\n",
    "\n",
    "**What to Preserve:**\n",
    "- ‚úÖ Key facts and decisions\n",
    "- ‚úÖ Student preferences and goals\n",
    "- ‚úÖ Important course recommendations\n",
    "- ‚úÖ Prerequisites and requirements\n",
    "- ‚úÖ Recent context (last few messages)\n",
    "\n",
    "**What to Compress:**\n",
    "- üì¶ Small talk and greetings\n",
    "- üì¶ Redundant information\n",
    "- üì¶ Old conversation details\n",
    "- üì¶ Resolved questions\n",
    "\n",
    "**When to Summarize:**\n",
    "- Token threshold exceeded (e.g., > 2000 tokens)\n",
    "- Message count threshold exceeded (e.g., > 10 messages)\n",
    "- Time-based (e.g., after 1 hour)\n",
    "- Manual trigger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8486d8bc89f7b",
   "metadata": {},
   "source": [
    "### Building Summarization Step-by-Step\n",
    "\n",
    "Let's build our summarization system incrementally, starting with simple components.\n",
    "\n",
    "#### Step 1: Create a data structure for conversation messages\n",
    "\n",
    "**What we're building:** A data structure to represent individual messages with metadata.\n",
    "\n",
    "**Why it's needed:** We need to track not just the message content, but also:\n",
    "- Who sent it (user, assistant, system)\n",
    "- When it was sent (timestamp)\n",
    "- How many tokens it uses (for threshold checks)\n",
    "\n",
    "**How it works:** Python's `@dataclass` decorator creates a clean, type-safe structure with automatic initialization and token counting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db188fb9f01d750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.938898Z",
     "iopub.status.busy": "2025-11-02T01:09:14.938801Z",
     "iopub.status.idle": "2025-11-02T01:09:14.941541Z",
     "shell.execute_reply": "2025-11-02T01:09:14.941043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConversationMessage dataclass defined\n",
      "   Example - Role: user, Tokens: 9\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ConversationMessage:\n",
    "    \"\"\"Represents a single conversation message.\"\"\"\n",
    "\n",
    "    role: str  # \"user\", \"assistant\", \"system\"\n",
    "    content: str\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "    token_count: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.token_count is None:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "\n",
    "# Test it\n",
    "test_msg = ConversationMessage(\n",
    "    role=\"user\", content=\"What courses do you recommend for machine learning?\"\n",
    ")\n",
    "print(f\"‚úÖ ConversationMessage dataclass defined\")\n",
    "print(f\"   Example - Role: {test_msg.role}, Tokens: {test_msg.token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f8f61e276661",
   "metadata": {},
   "source": [
    "#### Step 2: Create a function to check if summarization is needed\n",
    "\n",
    "**What we're building:** A decision function that determines when to trigger summarization.\n",
    "\n",
    "**Why it's needed:** We don't want to summarize too early (loses context) or too late (hits token limits). We need smart thresholds.\n",
    "\n",
    "**How it works:**\n",
    "- Checks if we have enough messages to make summarization worthwhile\n",
    "- Calculates total token count across all messages\n",
    "- Returns `True` if either threshold (tokens OR messages) is exceeded\n",
    "- Ensures we keep at least `keep_recent` messages unsummarized\n",
    "\n",
    "**When to summarize:**\n",
    "- Token threshold: Prevents hitting model limits (e.g., >2000 tokens)\n",
    "- Message threshold: Prevents conversation from getting too long (e.g., >10 messages)\n",
    "- Keep recent: Preserves the most relevant context (e.g., last 4 messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "290935fa536cb8aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.942848Z",
     "iopub.status.busy": "2025-11-02T01:09:14.942733Z",
     "iopub.status.idle": "2025-11-02T01:09:14.945144Z",
     "shell.execute_reply": "2025-11-02T01:09:14.944725Z"
    }
   },
   "outputs": [],
   "source": [
    "def should_summarize(\n",
    "    messages: List[ConversationMessage],\n",
    "    token_threshold: int = 2000,\n",
    "    message_threshold: int = 10,\n",
    "    keep_recent: int = 4,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Determine if conversation needs summarization.\n",
    "\n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        token_threshold: Summarize when total tokens exceed this\n",
    "        message_threshold: Summarize when message count exceeds this\n",
    "        keep_recent: Number of recent messages to keep unsummarized\n",
    "\n",
    "    Returns:\n",
    "        True if summarization is needed\n",
    "    \"\"\"\n",
    "    # Don't summarize if we have very few messages\n",
    "    if len(messages) <= keep_recent:\n",
    "        return False\n",
    "\n",
    "    # Calculate total tokens\n",
    "    total_tokens = sum(msg.token_count for msg in messages)\n",
    "\n",
    "    # Summarize if either threshold is exceeded\n",
    "    return total_tokens > token_threshold or len(messages) > message_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37993b003426e127",
   "metadata": {},
   "source": [
    "#### Step 3: Create a prompt template for summarization\n",
    "\n",
    "**What we're building:** A carefully crafted prompt that instructs the LLM on how to summarize conversations.\n",
    "\n",
    "**Why it's needed:** Generic summarization loses important details. We need domain-specific instructions that preserve what matters for course advisory conversations.\n",
    "\n",
    "**How it works:**\n",
    "- Specifies the context (student-advisor conversation)\n",
    "- Lists exactly what to preserve (decisions, requirements, goals, courses, issues)\n",
    "- Requests structured output (bullet points for clarity)\n",
    "- Emphasizes being \"specific and actionable\" (not vague summaries)\n",
    "\n",
    "**Design principle:** The prompt template is the \"instructions\" for the summarization LLM. Better instructions = better summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a39408752c4a504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.946915Z",
     "iopub.status.busy": "2025-11-02T01:09:14.946793Z",
     "iopub.status.idle": "2025-11-02T01:09:14.948854Z",
     "shell.execute_reply": "2025-11-02T01:09:14.948284Z"
    }
   },
   "outputs": [],
   "source": [
    "summarization_prompt_template = \"\"\"You are summarizing a conversation between a student and a course advisor.\n",
    "\n",
    "Create a concise summary that preserves:\n",
    "1. Key decisions made\n",
    "2. Important requirements or prerequisites discussed\n",
    "3. Student's goals, preferences, and constraints\n",
    "4. Specific courses mentioned and recommendations given\n",
    "5. Any problems or issues that need follow-up\n",
    "\n",
    "Format as bullet points. Be specific and actionable.\n",
    "\n",
    "Conversation to summarize:\n",
    "{conversation}\n",
    "\n",
    "Summary:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca0c3b7f31459f",
   "metadata": {},
   "source": [
    "#### Step 4: Create a function to generate summaries using the LLM\n",
    "\n",
    "**What we're building:** A function that takes messages and produces an intelligent summary using an LLM.\n",
    "\n",
    "**Why it's needed:** This is where the actual summarization happens. We need to:\n",
    "- Format the conversation for the LLM\n",
    "- Call the LLM with our prompt template\n",
    "- Package the summary as a system message\n",
    "\n",
    "**How it works:**\n",
    "1. Formats messages as \"User: ...\" and \"Assistant: ...\" text\n",
    "2. Inserts formatted conversation into the prompt template\n",
    "3. Calls the LLM asynchronously (non-blocking)\n",
    "4. Wraps the summary in `[CONVERSATION SUMMARY]` marker for easy identification\n",
    "5. Returns as a system message (distinguishes it from user/assistant messages)\n",
    "\n",
    "**Why async?** Summarization can take 1-3 seconds. Async allows other operations to continue while waiting for the LLM response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b41ae7eb2d88f5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.950203Z",
     "iopub.status.busy": "2025-11-02T01:09:14.950110Z",
     "iopub.status.idle": "2025-11-02T01:09:14.952595Z",
     "shell.execute_reply": "2025-11-02T01:09:14.952206Z"
    }
   },
   "outputs": [],
   "source": [
    "async def create_summary(\n",
    "    messages: List[ConversationMessage], llm: ChatOpenAI\n",
    ") -> ConversationMessage:\n",
    "    \"\"\"\n",
    "    Create intelligent summary of conversation messages.\n",
    "\n",
    "    Args:\n",
    "        messages: List of messages to summarize\n",
    "        llm: Language model for generating summary\n",
    "\n",
    "    Returns:\n",
    "        ConversationMessage containing the summary\n",
    "    \"\"\"\n",
    "    # Format conversation for summarization\n",
    "    conversation_text = \"\\n\".join(\n",
    "        [f\"{msg.role.title()}: {msg.content}\" for msg in messages]\n",
    "    )\n",
    "\n",
    "    # Generate summary using LLM\n",
    "    prompt = summarization_prompt_template.format(conversation=conversation_text)\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    summary_content = f\"[CONVERSATION SUMMARY]\\n{response.content}\"\n",
    "\n",
    "    # Create summary message\n",
    "    summary_msg = ConversationMessage(\n",
    "        role=\"system\", content=summary_content, timestamp=messages[-1].timestamp\n",
    "    )\n",
    "\n",
    "    return summary_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb87c914424cd",
   "metadata": {},
   "source": [
    "#### Step 5: Create a function to compress conversations\n",
    "\n",
    "**What we're building:** The main compression function that orchestrates the entire summarization process.\n",
    "\n",
    "**Why it's needed:** This ties together all the previous components into a single, easy-to-use function that:\n",
    "- Decides whether to summarize\n",
    "- Splits messages into old vs. recent\n",
    "- Generates the summary\n",
    "- Returns the compressed conversation\n",
    "\n",
    "**How it works:**\n",
    "1. **Check:** Calls `should_summarize()` to see if compression is needed\n",
    "2. **Split:** Divides messages into `old_messages` (to summarize) and `recent_messages` (to keep)\n",
    "3. **Summarize:** Calls `create_summary()` on old messages\n",
    "4. **Combine:** Returns `[summary] + recent_messages`\n",
    "\n",
    "**The result:** A conversation that's 50-80% smaller but preserves all essential information.\n",
    "\n",
    "**Example:**\n",
    "- Input: 20 messages (4,000 tokens)\n",
    "- Output: 1 summary + 4 recent messages (1,200 tokens)\n",
    "- Savings: 70% reduction in tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b904a38b1bad2b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.953876Z",
     "iopub.status.busy": "2025-11-02T01:09:14.953787Z",
     "iopub.status.idle": "2025-11-02T01:09:14.955880Z",
     "shell.execute_reply": "2025-11-02T01:09:14.955487Z"
    }
   },
   "outputs": [],
   "source": [
    "async def compress_conversation(\n",
    "    messages: List[ConversationMessage],\n",
    "    llm: ChatOpenAI,\n",
    "    token_threshold: int = 2000,\n",
    "    message_threshold: int = 10,\n",
    "    keep_recent: int = 4,\n",
    ") -> List[ConversationMessage]:\n",
    "    \"\"\"\n",
    "    Compress conversation by summarizing old messages and keeping recent ones.\n",
    "\n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        llm: Language model for generating summaries\n",
    "        token_threshold: Summarize when total tokens exceed this\n",
    "        message_threshold: Summarize when message count exceeds this\n",
    "        keep_recent: Number of recent messages to keep unsummarized\n",
    "\n",
    "    Returns:\n",
    "        List of messages: [summary] + [recent messages]\n",
    "    \"\"\"\n",
    "    # Check if summarization is needed\n",
    "    if not should_summarize(messages, token_threshold, message_threshold, keep_recent):\n",
    "        return messages\n",
    "\n",
    "    # Split into old and recent\n",
    "    old_messages = messages[:-keep_recent]\n",
    "    recent_messages = messages[-keep_recent:]\n",
    "\n",
    "    if not old_messages:\n",
    "        return messages\n",
    "\n",
    "    # Summarize old messages\n",
    "    summary = await create_summary(old_messages, llm)\n",
    "\n",
    "    # Return summary + recent messages\n",
    "    return [summary] + recent_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fce6b8d81c302",
   "metadata": {},
   "source": [
    "#### Step 6: Combine into a reusable class\n",
    "\n",
    "Now that we've built and tested each component, let's combine them into a reusable class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8324715c96096689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.957043Z",
     "iopub.status.busy": "2025-11-02T01:09:14.956964Z",
     "iopub.status.idle": "2025-11-02T01:09:14.959582Z",
     "shell.execute_reply": "2025-11-02T01:09:14.959215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summarization system built:\n",
      "   - ConversationMessage dataclass\n",
      "   - should_summarize() function\n",
      "   - Summarization prompt template\n",
      "   - create_summary() function\n",
      "   - compress_conversation() function\n",
      "   - ConversationSummarizer class\n"
     ]
    }
   ],
   "source": [
    "class ConversationSummarizer:\n",
    "    \"\"\"Manages conversation summarization to keep token counts manageable.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        token_threshold: int = 2000,\n",
    "        message_threshold: int = 10,\n",
    "        keep_recent: int = 4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the summarizer.\n",
    "\n",
    "        Args:\n",
    "            llm: Language model for generating summaries\n",
    "            token_threshold: Summarize when total tokens exceed this\n",
    "            message_threshold: Summarize when message count exceeds this\n",
    "            keep_recent: Number of recent messages to keep unsummarized\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.token_threshold = token_threshold\n",
    "        self.message_threshold = message_threshold\n",
    "        self.keep_recent = keep_recent\n",
    "        self.summarization_prompt = summarization_prompt_template\n",
    "\n",
    "    def should_summarize(self, messages: List[ConversationMessage]) -> bool:\n",
    "        \"\"\"Determine if conversation needs summarization.\"\"\"\n",
    "        return should_summarize(\n",
    "            messages, self.token_threshold, self.message_threshold, self.keep_recent\n",
    "        )\n",
    "\n",
    "    async def summarize_conversation(\n",
    "        self, messages: List[ConversationMessage]\n",
    "    ) -> ConversationMessage:\n",
    "        \"\"\"Create intelligent summary of conversation messages.\"\"\"\n",
    "        return await create_summary(messages, self.llm)\n",
    "\n",
    "    async def compress_conversation(\n",
    "        self, messages: List[ConversationMessage]\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress conversation by summarizing old messages and keeping recent ones.\"\"\"\n",
    "        return await compress_conversation(\n",
    "            messages,\n",
    "            self.llm,\n",
    "            self.token_threshold,\n",
    "            self.message_threshold,\n",
    "            self.keep_recent,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\"\"‚úÖ Summarization system built:\n",
    "   - ConversationMessage dataclass\n",
    "   - should_summarize() function\n",
    "   - Summarization prompt template\n",
    "   - create_summary() function\n",
    "   - compress_conversation() function\n",
    "   - ConversationSummarizer class\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb98376eb2b00b0",
   "metadata": {},
   "source": [
    "### Demo 3: Test Summarization\n",
    "\n",
    "Let's test the summarizer with a sample conversation.\n",
    "\n",
    "#### Step 1: Create a sample conversation\n",
    "\n",
    "**What:** Creating a realistic 14-message conversation about course planning.\n",
    "\n",
    "**Why:** We need a conversation long enough to trigger summarization (>10 messages, >500 tokens) so we can see the compression in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e63fdaf5a2a2587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.960594Z",
     "iopub.status.busy": "2025-11-02T01:09:14.960526Z",
     "iopub.status.idle": "2025-11-02T01:09:14.963210Z",
     "shell.execute_reply": "2025-11-02T01:09:14.962816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original conversation:\n",
      "  Messages: 16\n",
      "  Total tokens: 261\n",
      "  Average tokens per message: 16.3\n"
     ]
    }
   ],
   "source": [
    "# Create a sample long conversation\n",
    "sample_conversation = [\n",
    "    ConversationMessage(\n",
    "        \"user\", \"Hi, I'm interested in learning about machine learning courses\"\n",
    "    ),\n",
    "    ConversationMessage(\n",
    "        \"assistant\",\n",
    "        \"Great! Redis University offers several ML courses. CS401 Machine Learning is our flagship course. It covers supervised learning, neural networks, and practical applications.\",\n",
    "    ),\n",
    "    ConversationMessage(\"user\", \"What are the prerequisites for CS401?\"),\n",
    "    ConversationMessage(\n",
    "        \"assistant\",\n",
    "        \"CS401 requires CS201 Data Structures and MATH301 Linear Algebra. Have you completed these courses?\",\n",
    "    ),\n",
    "    ConversationMessage(\"user\", \"I've completed CS101 but not CS201 yet\"),\n",
    "    ConversationMessage(\n",
    "        \"assistant\",\n",
    "        \"Perfect! CS201 is the next logical step. It covers algorithms and data structures essential for ML. It's offered every semester.\",\n",
    "    ),\n",
    "    ConversationMessage(\"user\", \"How difficult is MATH301?\"),\n",
    "    ConversationMessage(\n",
    "        \"assistant\",\n",
    "        \"MATH301 is moderately challenging. It covers vectors, matrices, and eigenvalues used in ML algorithms. Most students find it manageable with consistent practice.\",\n",
    "    ),\n",
    "    ConversationMessage(\"user\", \"Can I take both CS201 and MATH301 together?\"),\n",
    "    ConversationMessage(\n",
    "        \"assistant\",\n",
    "        \"Yes, that's a good combination! They complement each other well. Many students take them concurrently.\",\n",
    "    ),\n",
    "    ConversationMessage(\"user\", \"What about CS401 after that?\"),\n",
    "    ConversationMessage(\n",
    "        \"assistant\",\n",
    "        \"CS401 is perfect after completing both prerequisites. It's our most popular AI course with hands-on projects.\",\n",
    "    ),\n",
    "    ConversationMessage(\"user\", \"When is CS401 offered?\"),\n",
    "    ConversationMessage(\n",
    "        \"assistant\",\n",
    "        \"CS401 is offered in Fall and Spring semesters. The Fall section typically fills up quickly, so register early!\",\n",
    "    ),\n",
    "    ConversationMessage(\"user\", \"Great! What's the workload like?\"),\n",
    "    ConversationMessage(\n",
    "        \"assistant\",\n",
    "        \"CS401 requires about 10-12 hours per week including lectures, assignments, and projects. There are 4 major projects throughout the semester.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Calculate original metrics\n",
    "original_token_count = sum(msg.token_count for msg in sample_conversation)\n",
    "print(f\"Original conversation:\")\n",
    "print(f\"  Messages: {len(sample_conversation)}\")\n",
    "print(f\"  Total tokens: {original_token_count}\")\n",
    "print(\n",
    "    f\"  Average tokens per message: {original_token_count / len(sample_conversation):.1f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824592502d5305",
   "metadata": {},
   "source": [
    "#### Step 2: Configure the summarizer\n",
    "\n",
    "**What:** Setting up the `ConversationSummarizer` with specific thresholds.\n",
    "\n",
    "**Why:** We use a low token threshold (500) to force summarization on our sample conversation. In production, you'd use higher thresholds (2000-4000 tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f1cd42e5cb65a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.964229Z",
     "iopub.status.busy": "2025-11-02T01:09:14.964154Z",
     "iopub.status.idle": "2025-11-02T01:09:14.965877Z",
     "shell.execute_reply": "2025-11-02T01:09:14.965551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizer configuration:\n",
      "  Token threshold: 500\n",
      "  Message threshold: 10\n",
      "  Keep recent: 4\n"
     ]
    }
   ],
   "source": [
    "# Test summarization\n",
    "summarizer = ConversationSummarizer(\n",
    "    llm=llm,\n",
    "    token_threshold=500,  # Low threshold for demo\n",
    "    message_threshold=10,\n",
    "    keep_recent=4,\n",
    ")\n",
    "\n",
    "print(f\"Summarizer configuration:\")\n",
    "print(f\"  Token threshold: {summarizer.token_threshold}\")\n",
    "print(f\"  Message threshold: {summarizer.message_threshold}\")\n",
    "print(f\"  Keep recent: {summarizer.keep_recent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b283d8917e353",
   "metadata": {},
   "source": [
    "#### Step 3: Check if summarization is needed\n",
    "\n",
    "**What:** Testing the `should_summarize()` logic.\n",
    "\n",
    "**Why:** Before compressing, we verify that our conversation actually exceeds the thresholds. This demonstrates the decision logic in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96d60c07d558dbe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.966951Z",
     "iopub.status.busy": "2025-11-02T01:09:14.966883Z",
     "iopub.status.idle": "2025-11-02T01:09:14.968571Z",
     "shell.execute_reply": "2025-11-02T01:09:14.968198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should summarize? True\n"
     ]
    }
   ],
   "source": [
    "# Check if summarization is needed\n",
    "should_summarize_result = summarizer.should_summarize(sample_conversation)\n",
    "print(f\"Should summarize? {should_summarize_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956554c8c979d1a4",
   "metadata": {},
   "source": [
    "#### Step 4: Compress the conversation\n",
    "\n",
    "**What:** Running the full compression pipeline: summarize old messages, keep recent ones.\n",
    "\n",
    "**Why:** This is the core functionality - transforming 14 messages into a summary + 4 recent messages, dramatically reducing token count while preserving key information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3566e3ee779cc9b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:14.969519Z",
     "iopub.status.busy": "2025-11-02T01:09:14.969463Z",
     "iopub.status.idle": "2025-11-02T01:09:19.592105Z",
     "shell.execute_reply": "2025-11-02T01:09:19.591549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After summarization:\n",
      "  Messages: 5\n",
      "  Total tokens: 300\n",
      "  Token savings: -39 (-14.9%)\n"
     ]
    }
   ],
   "source": [
    "# Compress the conversation\n",
    "compressed = await summarizer.compress_conversation(sample_conversation)\n",
    "\n",
    "compressed_token_count = sum(msg.token_count for msg in compressed)\n",
    "token_savings = original_token_count - compressed_token_count\n",
    "savings_percentage = (token_savings / original_token_count) * 100\n",
    "\n",
    "print(f\"After summarization:\")\n",
    "print(f\"  Messages: {len(compressed)}\")\n",
    "print(f\"  Total tokens: {compressed_token_count}\")\n",
    "print(f\"  Token savings: {token_savings} ({savings_percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85f81eedf9cae1",
   "metadata": {},
   "source": [
    "#### Step 5: Examine the compressed conversation structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82e6fb297080ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.593595Z",
     "iopub.status.busy": "2025-11-02T01:09:19.593471Z",
     "iopub.status.idle": "2025-11-02T01:09:19.596027Z",
     "shell.execute_reply": "2025-11-02T01:09:19.595562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed conversation structure:\n",
      "  1. üìã [system] [CONVERSATION SUMMARY] - **Key Decisions Made:**   - The student plans to take C...\n",
      "     Tokens: 236\n",
      "  2. üë§ [user] When is CS401 offered?...\n",
      "     Tokens: 6\n",
      "  3. ü§ñ [assistant] CS401 is offered in Fall and Spring semesters. The Fall section typically fills ...\n",
      "     Tokens: 22\n",
      "  4. üë§ [user] Great! What's the workload like?...\n",
      "     Tokens: 7\n",
      "  5. ü§ñ [assistant] CS401 requires about 10-12 hours per week including lectures, assignments, and p...\n",
      "     Tokens: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"Compressed conversation structure:\")\n",
    "for i, msg in enumerate(compressed):\n",
    "    role_icon = \"üìã\" if msg.role == \"system\" else \"üë§\" if msg.role == \"user\" else \"ü§ñ\"\n",
    "    content_preview = msg.content[:80].replace(\"\\n\", \" \")\n",
    "    print(f\"  {i+1}. {role_icon} [{msg.role}] {content_preview}...\")\n",
    "    print(f\"     Tokens: {msg.token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb252a2997a22ba",
   "metadata": {},
   "source": [
    "#### Results Analysis\n",
    "\n",
    "**What happened:**\n",
    "- Original: 16 messages with ~{original_token_count} tokens\n",
    "- Compressed: {len(compressed)} messages (1 summary + 4 recent)\n",
    "- Savings: ~{savings_percentage:.0f}% token reduction\n",
    "\n",
    "**Key benefits:**\n",
    "- Preserved recent context (last 4 messages)\n",
    "- Summarized older messages into key facts\n",
    "- Maintained conversation continuity\n",
    "- Reduced token costs significantly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896bce27c392ee9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Part 3: Context Compression Strategies\n",
    "\n",
    "In Part 2, we built a complete summarization system using LLMs to compress conversation history. But summarization isn't the only way to manage context - and it's not always optimal.\n",
    "\n",
    "Let's explore **four different compression strategies** and understand when to use each one:\n",
    "\n",
    "1. **Truncation** - Token-aware, keeps recent messages within budget\n",
    "2. **Sliding Window** - Message-aware, maintains fixed window size\n",
    "3. **Priority-Based** - Intelligent selection without LLM calls\n",
    "4. **Summarization** - High quality compression using LLM (from Part 2)\n",
    "\n",
    "Each strategy has different trade-offs in **speed**, **cost**, and **quality**. By the end of this part, you'll know how to choose the right strategy for your use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2737aeb03474",
   "metadata": {},
   "source": [
    "### Theory: Four Compression Approaches\n",
    "\n",
    "Let's explore four different strategies, each with different trade-offs:\n",
    "\n",
    "**1. Truncation (Token-Aware)**\n",
    "- Keep recent messages within token budget\n",
    "- ‚úÖ Pros: Fast, no LLM calls, respects context limits\n",
    "- ‚ùå Cons: Variable message count, loses old context\n",
    "- **Best for:** Token-constrained applications, API limits\n",
    "\n",
    "**2. Sliding Window (Message-Aware)**\n",
    "- Keep exactly N most recent messages\n",
    "- ‚úÖ Pros: Fastest, predictable count, constant memory\n",
    "- ‚ùå Cons: May exceed token limits, loses old context\n",
    "- **Best for:** Fixed-size buffers, real-time chat\n",
    "\n",
    "**3. Priority-Based (Balanced)**\n",
    "- Score messages by importance, keep highest-scoring\n",
    "- ‚úÖ Pros: Preserves important context, no LLM calls\n",
    "- ‚ùå Cons: Requires good scoring logic, may lose temporal flow\n",
    "- **Best for:** Production applications needing balance\n",
    "\n",
    "**4. Summarization (High Quality)**\n",
    "- Use LLM to create intelligent summaries\n",
    "- ‚úÖ Pros: Preserves meaning, high quality\n",
    "- ‚ùå Cons: Slower, costs tokens, requires LLM call\n",
    "- **Best for:** High-value conversations, quality-critical applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5f28d6ed343f6",
   "metadata": {},
   "source": [
    "### Building Compression Strategies Step-by-Step\n",
    "\n",
    "Let's build each strategy incrementally, starting with the simplest.\n",
    "\n",
    "#### Step 1: Define a base interface for compression strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b053a7b2c242989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.597470Z",
     "iopub.status.busy": "2025-11-02T01:09:19.597376Z",
     "iopub.status.idle": "2025-11-02T01:09:19.599313Z",
     "shell.execute_reply": "2025-11-02T01:09:19.598862Z"
    }
   },
   "outputs": [],
   "source": [
    "class CompressionStrategy:\n",
    "    \"\"\"Base class for compression strategies.\"\"\"\n",
    "\n",
    "    def compress(\n",
    "        self, messages: List[ConversationMessage], max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress messages to fit within max_tokens.\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ab8bf105c70aa",
   "metadata": {},
   "source": [
    "#### Step 2: Implement Truncation Strategy (Simplest)\n",
    "\n",
    "This strategy simply keeps the most recent messages that fit within the token budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf8c2576cad8bfc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.600555Z",
     "iopub.status.busy": "2025-11-02T01:09:19.600451Z",
     "iopub.status.idle": "2025-11-02T01:09:19.602616Z",
     "shell.execute_reply": "2025-11-02T01:09:19.602239Z"
    }
   },
   "outputs": [],
   "source": [
    "class TruncationStrategy(CompressionStrategy):\n",
    "    \"\"\"Keep only the most recent messages within token budget.\"\"\"\n",
    "\n",
    "    def compress(\n",
    "        self, messages: List[ConversationMessage], max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Keep most recent messages within token budget.\"\"\"\n",
    "        compressed = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Work backwards from most recent\n",
    "        for msg in reversed(messages):\n",
    "            if total_tokens + msg.token_count <= max_tokens:\n",
    "                compressed.insert(0, msg)\n",
    "                total_tokens += msg.token_count\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd84d939f70075",
   "metadata": {},
   "source": [
    "#### Step 2.5: Implement Sliding Window Strategy (Simplest)\n",
    "\n",
    "**What we're building:** A strategy that maintains a fixed-size window of the N most recent messages.\n",
    "\n",
    "**Why it's different from truncation:**\n",
    "- **Truncation:** Reactive - keeps messages until token budget exceeded, then removes oldest\n",
    "- **Sliding Window:** Proactive - always maintains exactly N messages regardless of tokens\n",
    "\n",
    "**When to use:**\n",
    "- Real-time chat where you want constant context size\n",
    "- Systems with predictable message patterns\n",
    "- When simplicity matters more than token optimization\n",
    "\n",
    "**Trade-off:** May exceed token limits if messages are very long.\n",
    "\n",
    "**How it works:** Simply returns the last N messages using Python list slicing (`messages[-N:]`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a683df2353cdfdc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.603837Z",
     "iopub.status.busy": "2025-11-02T01:09:19.603740Z",
     "iopub.status.idle": "2025-11-02T01:09:19.605932Z",
     "shell.execute_reply": "2025-11-02T01:09:19.605526Z"
    }
   },
   "outputs": [],
   "source": [
    "class SlidingWindowStrategy(CompressionStrategy):\n",
    "    \"\"\"Keep only the last N messages (fixed window size).\"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize sliding window strategy.\n",
    "\n",
    "        Args:\n",
    "            window_size: Number of recent messages to keep\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def compress(\n",
    "        self, messages: List[ConversationMessage], max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"\n",
    "        Keep only the last N messages.\n",
    "\n",
    "        Note: Ignores max_tokens parameter - always keeps exactly window_size messages.\n",
    "        \"\"\"\n",
    "        if len(messages) <= self.window_size:\n",
    "            return messages\n",
    "\n",
    "        return messages[-self.window_size :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42299c4601c4f31a",
   "metadata": {},
   "source": [
    "#### Step 3: Implement Priority-Based Strategy (Intelligent Selection)\n",
    "\n",
    "This strategy scores messages by importance and keeps the highest-scoring ones.\n",
    "\n",
    "First, let's create a function to calculate message importance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "739168f3fa76a165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.607042Z",
     "iopub.status.busy": "2025-11-02T01:09:19.606960Z",
     "iopub.status.idle": "2025-11-02T01:09:19.609274Z",
     "shell.execute_reply": "2025-11-02T01:09:19.608876Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_message_importance(msg: ConversationMessage) -> float:\n",
    "    \"\"\"\n",
    "    Calculate importance score for a message.\n",
    "\n",
    "    Higher scores = more important.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    content_lower = msg.content.lower()\n",
    "\n",
    "    # Course codes are important (CS401, MATH301, etc.)\n",
    "    if any(code in content_lower for code in [\"cs\", \"math\", \"eng\"]):\n",
    "        score += 2.0\n",
    "\n",
    "    # Questions are important\n",
    "    if \"?\" in msg.content:\n",
    "        score += 1.5\n",
    "\n",
    "    # Prerequisites and requirements are important\n",
    "    if any(word in content_lower for word in [\"prerequisite\", \"require\", \"need\"]):\n",
    "        score += 1.5\n",
    "\n",
    "    # Preferences and goals are important\n",
    "    if any(word in content_lower for word in [\"prefer\", \"want\", \"goal\", \"interested\"]):\n",
    "        score += 1.0\n",
    "\n",
    "    # User messages slightly more important (their needs)\n",
    "    if msg.role == \"user\":\n",
    "        score += 0.5\n",
    "\n",
    "    # Longer messages often have more content\n",
    "    if msg.token_count > 50:\n",
    "        score += 0.5\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3e19b190c9e3c",
   "metadata": {},
   "source": [
    "Now let's create the Priority-Based strategy class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f66e696bacf5a96a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.610359Z",
     "iopub.status.busy": "2025-11-02T01:09:19.610267Z",
     "iopub.status.idle": "2025-11-02T01:09:19.613070Z",
     "shell.execute_reply": "2025-11-02T01:09:19.612474Z"
    }
   },
   "outputs": [],
   "source": [
    "class PriorityBasedStrategy(CompressionStrategy):\n",
    "    \"\"\"Keep highest-priority messages within token budget.\"\"\"\n",
    "\n",
    "    def calculate_importance(self, msg: ConversationMessage) -> float:\n",
    "        \"\"\"Calculate importance score for a message.\"\"\"\n",
    "        return calculate_message_importance(msg)\n",
    "\n",
    "    def compress(\n",
    "        self, messages: List[ConversationMessage], max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Keep highest-priority messages within token budget.\"\"\"\n",
    "        # Score each message\n",
    "        scored_messages = [\n",
    "            (self.calculate_importance(msg), i, msg) for i, msg in enumerate(messages)\n",
    "        ]\n",
    "\n",
    "        # Sort by score (descending), then by index to maintain some order\n",
    "        scored_messages.sort(key=lambda x: (-x[0], x[1]))\n",
    "\n",
    "        # Select messages within budget\n",
    "        selected = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        for score, idx, msg in scored_messages:\n",
    "            if total_tokens + msg.token_count <= max_tokens:\n",
    "                selected.append((idx, msg))\n",
    "                total_tokens += msg.token_count\n",
    "\n",
    "        # Sort by original index to maintain conversation flow\n",
    "        selected.sort(key=lambda x: x[0])\n",
    "\n",
    "        return [msg for idx, msg in selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0400bdab30655",
   "metadata": {},
   "source": [
    "#### Step 4: Wrap Summarization Strategy (Already Built in Part 2)\n",
    "\n",
    "**What we're doing:** Creating a `SummarizationStrategy` wrapper around the `ConversationSummarizer` we built in Part 2.\n",
    "\n",
    "**Why wrap it:** To make it compatible with the `CompressionStrategy` interface so we can compare it fairly with the other strategies in Demo 4.\n",
    "\n",
    "**Note:** We're not rebuilding summarization - we're just adapting what we already built to work alongside truncation, sliding window, and priority-based strategies. This is the adapter pattern in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c0fa64ab406ef95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.614307Z",
     "iopub.status.busy": "2025-11-02T01:09:19.614198Z",
     "iopub.status.idle": "2025-11-02T01:09:19.616491Z",
     "shell.execute_reply": "2025-11-02T01:09:19.616127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Compression strategies implemented:\n",
      "   - CompressionStrategy base class\n",
      "   - TruncationStrategy (token-aware)\n",
      "   - SlidingWindowStrategy (message-aware)\n",
      "   - PriorityBasedStrategy (intelligent selection)\n",
      "   - SummarizationStrategy (LLM-based)\n"
     ]
    }
   ],
   "source": [
    "class SummarizationStrategy(CompressionStrategy):\n",
    "    \"\"\"Use LLM to create intelligent summaries.\"\"\"\n",
    "\n",
    "    def __init__(self, summarizer: ConversationSummarizer):\n",
    "        self.summarizer = summarizer\n",
    "\n",
    "    async def compress_async(\n",
    "        self, messages: List[ConversationMessage], max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress using summarization (async).\"\"\"\n",
    "        # Use the summarizer's logic\n",
    "        return await self.summarizer.compress_conversation(messages)\n",
    "\n",
    "    def compress(\n",
    "        self, messages: List[ConversationMessage], max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Synchronous wrapper (not recommended, use compress_async).\"\"\"\n",
    "        raise NotImplementedError(\"Use compress_async for summarization strategy\")\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\"\"‚úÖ Compression strategies implemented:\n",
    "   - CompressionStrategy base class\n",
    "   - TruncationStrategy (token-aware)\n",
    "   - SlidingWindowStrategy (message-aware)\n",
    "   - PriorityBasedStrategy (intelligent selection)\n",
    "   - SummarizationStrategy (LLM-based)\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ddde791c5afc",
   "metadata": {},
   "source": [
    "### Demo 4: Compare Compression Strategies\n",
    "\n",
    "Let's compare all four strategies on the same conversation to understand their trade-offs.\n",
    "\n",
    "#### Step 1: Set up the test\n",
    "\n",
    "**What:** Establishing baseline metrics for our comparison.\n",
    "\n",
    "**Why:** We need to know the original size (messages and tokens) to measure how much each strategy compresses and what it costs in terms of information loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22b54c30ef8be4a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.617799Z",
     "iopub.status.busy": "2025-11-02T01:09:19.617674Z",
     "iopub.status.idle": "2025-11-02T01:09:19.619829Z",
     "shell.execute_reply": "2025-11-02T01:09:19.619516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original conversation: 16 messages, 261 tokens\n",
      "Target budget: 800 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the same sample conversation from before\n",
    "test_conversation = sample_conversation.copy()\n",
    "max_tokens = 800  # Target token budget\n",
    "\n",
    "original_tokens = sum(msg.token_count for msg in test_conversation)\n",
    "print(\n",
    "    f\"\"\"Original conversation: {len(test_conversation)} messages, {original_tokens} tokens\n",
    "Target budget: {max_tokens} tokens\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dac15eec962562",
   "metadata": {},
   "source": [
    "#### Step 2: Test Truncation Strategy\n",
    "\n",
    "**What:** Testing token-aware compression that keeps recent messages within budget.\n",
    "\n",
    "**Why:** Demonstrates how truncation guarantees staying under token limits by working backwards from the most recent message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be20f6779afc21e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.621097Z",
     "iopub.status.busy": "2025-11-02T01:09:19.621019Z",
     "iopub.status.idle": "2025-11-02T01:09:19.623145Z",
     "shell.execute_reply": "2025-11-02T01:09:19.622788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUNCATION STRATEGY\n",
      "  Result: 16 messages, 261 tokens\n",
      "  Savings: 0 tokens\n",
      "  Kept messages: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "truncation = TruncationStrategy()\n",
    "truncated = truncation.compress(test_conversation, max_tokens)\n",
    "truncated_tokens = sum(msg.token_count for msg in truncated)\n",
    "\n",
    "print(f\"TRUNCATION STRATEGY\")\n",
    "print(f\"  Result: {len(truncated)} messages, {truncated_tokens} tokens\")\n",
    "print(f\"  Savings: {original_tokens - truncated_tokens} tokens\")\n",
    "print(\n",
    "    f\"  Kept messages: {[i for i, msg in enumerate(test_conversation) if msg in truncated]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfbdc40403d640",
   "metadata": {},
   "source": [
    "#### Step 2.5: Test Sliding Window Strategy\n",
    "\n",
    "**What:** Testing message-aware compression that keeps exactly N recent messages.\n",
    "\n",
    "**Why:** Shows how sliding window prioritizes predictability (always 6 messages) over token optimization (may exceed budget).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4018ee04019c9a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.624216Z",
     "iopub.status.busy": "2025-11-02T01:09:19.624133Z",
     "iopub.status.idle": "2025-11-02T01:09:19.626403Z",
     "shell.execute_reply": "2025-11-02T01:09:19.625989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIDING WINDOW STRATEGY\n",
      "  Result: 6 messages, 91 tokens\n",
      "  Savings: 170 tokens\n",
      "  Kept messages: [10, 11, 12, 13, 14, 15]\n",
      "  Token budget: 91/800 (within limit)\n"
     ]
    }
   ],
   "source": [
    "sliding_window = SlidingWindowStrategy(window_size=6)\n",
    "windowed = sliding_window.compress(test_conversation, max_tokens)\n",
    "windowed_tokens = sum(msg.token_count for msg in windowed)\n",
    "\n",
    "print(f\"SLIDING WINDOW STRATEGY\")\n",
    "print(f\"  Result: {len(windowed)} messages, {windowed_tokens} tokens\")\n",
    "print(f\"  Savings: {original_tokens - windowed_tokens} tokens\")\n",
    "print(\n",
    "    f\"  Kept messages: {[i for i, msg in enumerate(test_conversation) if msg in windowed]}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Token budget: {windowed_tokens}/{max_tokens} ({'within' if windowed_tokens <= max_tokens else 'EXCEEDS'} limit)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529392dfaf6dbe64",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "The sliding window kept:\n",
    "- **Exactly 6 messages** (last 6 from the conversation)\n",
    "- **Most recent context only** (indices show the final messages)\n",
    "- **{windowed_tokens} tokens** (may or may not fit budget)\n",
    "\n",
    "**Key difference from truncation:**\n",
    "- **Truncation:** Kept {len(truncated)} messages to stay under {max_tokens} tokens\n",
    "- **Sliding Window:** Kept exactly 6 messages, resulting in {windowed_tokens} tokens\n",
    "\n",
    "**Behavior pattern:**\n",
    "- Truncation: \"Fill the budget\" ‚Üí Variable count, guaranteed fit\n",
    "- Sliding Window: \"Fixed window\" ‚Üí Constant count, may exceed budget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69267d84d68c7376",
   "metadata": {},
   "source": [
    "#### Step 3: Test Priority-Based Strategy\n",
    "\n",
    "**What:** Testing intelligent selection that scores messages by importance.\n",
    "\n",
    "**Why:** Demonstrates how priority-based compression preserves high-value messages (questions, course codes, requirements) while staying within budget - no LLM needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0b2ce7a958fbe9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.627580Z",
     "iopub.status.busy": "2025-11-02T01:09:19.627497Z",
     "iopub.status.idle": "2025-11-02T01:09:19.629606Z",
     "shell.execute_reply": "2025-11-02T01:09:19.629188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIORITY-BASED STRATEGY\n",
      "  Result: 16 messages, 261 tokens\n",
      "  Savings: 0 tokens\n",
      "  Kept messages: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "priority = PriorityBasedStrategy()\n",
    "prioritized = priority.compress(test_conversation, max_tokens)\n",
    "prioritized_tokens = sum(msg.token_count for msg in prioritized)\n",
    "\n",
    "print(f\"PRIORITY-BASED STRATEGY\")\n",
    "print(f\"  Result: {len(prioritized)} messages, {prioritized_tokens} tokens\")\n",
    "print(f\"  Savings: {original_tokens - prioritized_tokens} tokens\")\n",
    "print(\n",
    "    f\"  Kept messages: {[i for i, msg in enumerate(test_conversation) if msg in prioritized]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed34b703bb9c7d9",
   "metadata": {},
   "source": [
    "Let's examine which messages were selected and why:\n",
    "\n",
    "**What:** Inspecting the importance scores assigned to different messages.\n",
    "\n",
    "**Why:** Understanding the scoring logic helps you tune it for your domain (e.g., legal terms, medical codes, customer names).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "134971d1108034c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.630668Z",
     "iopub.status.busy": "2025-11-02T01:09:19.630588Z",
     "iopub.status.idle": "2025-11-02T01:09:19.632452Z",
     "shell.execute_reply": "2025-11-02T01:09:19.632116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample importance scores:\n",
      "  Message 0: 1.5 - \"Hi, I'm interested in learning about machine learn...\"\n",
      "  Message 2: 5.5 - \"What are the prerequisites for CS401?...\"\n",
      "  Message 4: 2.5 - \"I've completed CS101 but not CS201 yet...\"\n",
      "  Message 6: 4.0 - \"How difficult is MATH301?...\"\n"
     ]
    }
   ],
   "source": [
    "# Show importance scores for selected messages\n",
    "print(\"Sample importance scores:\")\n",
    "for i in [0, 2, 4, 6]:\n",
    "    if i < len(test_conversation):\n",
    "        score = priority.calculate_importance(test_conversation[i])\n",
    "        preview = test_conversation[i].content[:50]\n",
    "        print(f'  Message {i}: {score:.1f} - \"{preview}...\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e310f0458261b9a8",
   "metadata": {},
   "source": [
    "#### Step 4: Test Summarization Strategy\n",
    "\n",
    "**What:** Testing LLM-based compression using the summarizer from Part 2.\n",
    "\n",
    "**Why:** Shows the highest-quality compression - preserves meaning and context but requires an API call. This is the gold standard for quality, but comes with latency and cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "997bc235a9b3038b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:19.633410Z",
     "iopub.status.busy": "2025-11-02T01:09:19.633348Z",
     "iopub.status.idle": "2025-11-02T01:09:23.786609Z",
     "shell.execute_reply": "2025-11-02T01:09:23.786002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIZATION STRATEGY\n",
      "  Result: 5 messages, 311 tokens\n",
      "  Savings: -50 tokens\n",
      "  Structure: 1 summary + 4 recent messages\n"
     ]
    }
   ],
   "source": [
    "summarization = SummarizationStrategy(summarizer)\n",
    "summarized = await summarization.compress_async(test_conversation, max_tokens)\n",
    "summarized_tokens = sum(msg.token_count for msg in summarized)\n",
    "\n",
    "print(f\"SUMMARIZATION STRATEGY\")\n",
    "print(f\"  Result: {len(summarized)} messages, {summarized_tokens} tokens\")\n",
    "print(f\"  Savings: {original_tokens - summarized_tokens} tokens\")\n",
    "print(f\"  Structure: 1 summary + {len(summarized) - 1} recent messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f2653b2c4e89b",
   "metadata": {},
   "source": [
    "#### Step 5: Compare all strategies\n",
    "\n",
    "**What:** Side-by-side comparison of all four strategies on the same conversation.\n",
    "\n",
    "**Why:** Seeing the trade-offs in a table makes it clear: truncation/sliding window are fast but lose context, priority-based balances both, summarization preserves most but costs time/money.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47b36cc71717932b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:23.788092Z",
     "iopub.status.busy": "2025-11-02T01:09:23.787966Z",
     "iopub.status.idle": "2025-11-02T01:09:23.791405Z",
     "shell.execute_reply": "2025-11-02T01:09:23.790886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Strategy             Messages     Tokens       Savings      Quality\n",
      "--------------------------------------------------------------------------------\n",
      "Original             16           261          0            N/A\n",
      "Truncation           16           261          0            Low\n",
      "Sliding Window       6            91           170   (65%)  Low\n",
      "Priority-Based       16           261          0            Medium\n",
      "Summarization        5            311          -50          High\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Strategy':<20} {'Messages':<12} {'Tokens':<12} {'Savings':<12} {'Quality'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "strategies = [\n",
    "    (\"Original\", len(test_conversation), original_tokens, 0, \"N/A\"),\n",
    "    (\n",
    "        \"Truncation\",\n",
    "        len(truncated),\n",
    "        truncated_tokens,\n",
    "        original_tokens - truncated_tokens,\n",
    "        \"Low\",\n",
    "    ),\n",
    "    (\n",
    "        \"Sliding Window\",\n",
    "        len(windowed),\n",
    "        windowed_tokens,\n",
    "        original_tokens - windowed_tokens,\n",
    "        \"Low\",\n",
    "    ),\n",
    "    (\n",
    "        \"Priority-Based\",\n",
    "        len(prioritized),\n",
    "        prioritized_tokens,\n",
    "        original_tokens - prioritized_tokens,\n",
    "        \"Medium\",\n",
    "    ),\n",
    "    (\n",
    "        \"Summarization\",\n",
    "        len(summarized),\n",
    "        summarized_tokens,\n",
    "        original_tokens - summarized_tokens,\n",
    "        \"High\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for name, msgs, tokens, savings, quality in strategies:\n",
    "    savings_pct = f\"({savings/original_tokens*100:.0f}%)\" if savings > 0 else \"\"\n",
    "    print(f\"{name:<20} {msgs:<12} {tokens:<12} {savings:<5} {savings_pct:<6} {quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7c056c978aea4",
   "metadata": {},
   "source": [
    "### Understanding the Trade-offs: Why Summarization Isn't Always Optimal\n",
    "\n",
    "Now that we've seen all four strategies in action, let's understand when each one shines and when it falls short.\n",
    "\n",
    "**Summarization's Trade-offs:**\n",
    "\n",
    "While summarization provides the highest quality compression, it introduces constraints:\n",
    "\n",
    "1. **Latency:** Requires LLM API call (1-3 seconds vs. <10ms for other strategies)\n",
    "2. **Cost:** Extra API calls at scale (1,000 conversations/day = 1,000+ LLM calls)\n",
    "3. **Lossy:** Paraphrases content, doesn't preserve exact wording\n",
    "4. **Complexity:** Requires async operations, prompt engineering, error handling\n",
    "\n",
    "**When to Use Alternatives:**\n",
    "\n",
    "| Scenario | Better Strategy | Why |\n",
    "|----------|----------------|-----|\n",
    "| Real-time chat | Truncation/Sliding Window | Zero latency |\n",
    "| Cost-sensitive (high volume) | Priority-based | No API calls |\n",
    "| Verbatim accuracy required | Truncation | Preserves exact wording |\n",
    "| Predictable context size | Sliding Window | Fixed message count |\n",
    "\n",
    "See the Key Takeaways below for the complete decision framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd894c5ffdfff",
   "metadata": {},
   "source": [
    "#### Key Takeaways\n",
    "\n",
    "**Truncation (Token-Aware):**\n",
    "- Keeps messages within token budget\n",
    "- Variable message count, guaranteed under limit\n",
    "- Good for: API token limits, cost control\n",
    "\n",
    "**Sliding Window (Message-Aware):**\n",
    "- Keeps exactly N most recent messages\n",
    "- Fixed message count, may exceed token budget\n",
    "- Good for: Real-time chat, predictable context size\n",
    "\n",
    "**Priority-Based (Intelligent):**\n",
    "- Scores and keeps important messages\n",
    "- Preserves key information across conversation\n",
    "- Good for: Most production applications, balanced approach\n",
    "\n",
    "**Summarization (Highest Quality):**\n",
    "- Uses LLM to preserve meaning\n",
    "- Highest quality, but requires API call (cost + latency)\n",
    "- Good for: High-value conversations, support tickets, advisory sessions\n",
    "\n",
    "**Decision Framework:**\n",
    "- **Speed-critical** ‚Üí Truncation or Sliding Window (instant, no LLM)\n",
    "- **Cost-sensitive** ‚Üí Priority-Based (intelligent, no API calls)\n",
    "- **Quality-critical** ‚Üí Summarization (preserves meaning, expensive)\n",
    "- **Predictable context** ‚Üí Sliding Window (constant message count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca23d0020c84249",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 4: Agent Memory Server Integration\n",
    "\n",
    "The Agent Memory Server provides automatic summarization. Let's configure and test it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0c2b93f2cf79e",
   "metadata": {},
   "source": [
    "### üîß Theory: Automatic Memory Management\n",
    "\n",
    "As we learned in Notebook 01, the Agent Memory Server provides automatic memory management with configurable compression strategies.\n",
    "\n",
    "**Agent Memory Server Features:**\n",
    "- ‚úÖ Automatic summarization when thresholds are exceeded\n",
    "- ‚úÖ Configurable strategies (recent + summary, sliding window, full summary)\n",
    "- ‚úÖ Transparent to your application code\n",
    "- ‚úÖ Production-ready and scalable\n",
    "\n",
    "**How It Works:**\n",
    "1. You add messages to working memory normally\n",
    "2. Server monitors message count and token count\n",
    "3. When threshold is exceeded, server automatically summarizes\n",
    "4. Old messages are replaced with summary\n",
    "5. Recent messages are kept for context\n",
    "6. Your application retrieves the compressed memory\n",
    "\n",
    "**Configuration Options:**\n",
    "- `message_threshold`: Summarize after N messages (default: 20)\n",
    "- `token_threshold`: Summarize after N tokens (default: 4000)\n",
    "- `keep_recent`: Number of recent messages to keep (default: 4)\n",
    "- `strategy`: \"recent_plus_summary\", \"sliding_window\", or \"full_summary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585948b56598a9f",
   "metadata": {},
   "source": [
    "### Demo 5: Test Automatic Summarization with Realistic Academic Advising\n",
    "\n",
    "Let's test the Agent Memory Server's automatic summarization with a realistic, information-dense conversation.\n",
    "\n",
    "**Real-World Scenario:** This demo simulates an academic advising session where a student asks detailed questions about a course syllabus. This mirrors actual use cases like:\n",
    "- Academic advising chatbots answering detailed course questions\n",
    "- Customer support agents explaining complex products/services\n",
    "- Technical documentation assistants providing in-depth explanations\n",
    "- Healthcare chatbots discussing treatment options and medical information\n",
    "\n",
    "The long, information-dense responses will exceed the 4000 token threshold, triggering automatic summarization.\n",
    "\n",
    "#### Step 1: Create a test session\n",
    "\n",
    "**What:** Setting up a unique session ID for testing automatic summarization.\n",
    "\n",
    "**Why:** Each session has its own working memory. We need a fresh session to observe the Agent Memory Server's automatic compression behavior from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de6e6cc74530366a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:23.793025Z",
     "iopub.status.busy": "2025-11-02T01:09:23.792940Z",
     "iopub.status.idle": "2025-11-02T01:09:23.794937Z",
     "shell.execute_reply": "2025-11-02T01:09:23.794510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing automatic summarization\n",
      "Session ID: long_conversation_test_1762045763\n",
      "Student ID: student_memory_test\n"
     ]
    }
   ],
   "source": [
    "# Create a test session\n",
    "test_session_id = f\"long_conversation_test_{int(time.time())}\"\n",
    "test_student_id = \"student_memory_test\"\n",
    "\n",
    "print(\n",
    "    f\"\"\"Testing automatic summarization\n",
    "Session ID: {test_session_id}\n",
    "Student ID: {test_student_id}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557dad8d8f53ef0",
   "metadata": {},
   "source": [
    "#### Step 2: Create a realistic scenario - Student exploring a detailed course syllabus\n",
    "\n",
    "**What:** Simulating a real advising session where a student asks detailed questions about the CS401 Machine Learning course syllabus.\n",
    "\n",
    "**Why:** Real conversations involve long, information-dense responses (course descriptions, prerequisites, project details). This creates enough tokens to trigger automatic summarization while demonstrating a realistic use case.\n",
    "\n",
    "**Scenario:** A student is considering CS401 and asks progressively deeper questions about the syllabus, prerequisites, projects, grading, and logistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4addd7959de37558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:23.796566Z",
     "iopub.status.busy": "2025-11-02T01:09:23.796467Z",
     "iopub.status.idle": "2025-11-02T01:09:23.806263Z",
     "shell.execute_reply": "2025-11-02T01:09:23.805953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created realistic advising conversation:\n",
      "   - 11 turns (22 messages)\n",
      "   - Detailed course syllabus document\n",
      "   - Progressive depth: overview ‚Üí prerequisites ‚Üí projects ‚Üí logistics ‚Üí financial aid\n",
      "   - Long, information-dense responses (realistic for academic advising)\n",
      "   - Total tokens: 4,795 tokens (threshold: 4,000)\n",
      "   - Status: ‚úÖ EXCEEDS threshold\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a detailed course syllabus (this would typically come from a RAG system)\n",
    "cs401_syllabus = \"\"\"\n",
    "CS401: Machine Learning - Complete Course Syllabus\n",
    "\n",
    "COURSE OVERVIEW:\n",
    "This comprehensive course covers fundamental and advanced machine learning techniques. Students will learn supervised learning (linear regression, logistic regression, decision trees, random forests, support vector machines), unsupervised learning (k-means clustering, hierarchical clustering, DBSCAN, dimensionality reduction with PCA and t-SNE), neural networks (feedforward networks, backpropagation, activation functions, optimization algorithms), deep learning (convolutional neural networks for computer vision, recurrent neural networks for sequence modeling, LSTMs and GRUs for time series), and natural language processing (word embeddings, transformers, attention mechanisms, BERT, GPT architectures).\n",
    "\n",
    "PREREQUISITES:\n",
    "- CS201 Data Structures and Algorithms (required) - Must understand trees, graphs, dynamic programming, complexity analysis\n",
    "- MATH301 Linear Algebra (required) - Matrix operations, eigenvalues, eigenvectors, vector spaces\n",
    "- STAT201 Probability and Statistics (recommended) - Probability distributions, hypothesis testing, Bayes' theorem\n",
    "- Python programming experience (required) - NumPy, Pandas, Matplotlib\n",
    "\n",
    "COURSE STRUCTURE:\n",
    "- 15 weeks, 3 hours lecture + 2 hours lab per week\n",
    "- 4 major projects (40% of grade)\n",
    "- Weekly problem sets (20% of grade)\n",
    "- Midterm exam (15% of grade)\n",
    "- Final exam (20% of grade)\n",
    "- Class participation (5% of grade)\n",
    "\n",
    "PROJECTS:\n",
    "Project 1 (Weeks 2-4): Implement linear regression and logistic regression from scratch using only NumPy. Apply to housing price prediction and spam classification datasets.\n",
    "\n",
    "Project 2 (Weeks 5-7): Build a neural network framework with backpropagation. Implement various activation functions (ReLU, sigmoid, tanh) and optimization algorithms (SGD, Adam, RMSprop). Train on MNIST digit classification.\n",
    "\n",
    "Project 3 (Weeks 8-11): Develop a convolutional neural network for image classification using TensorFlow/PyTorch. Experiment with different architectures (LeNet, AlexNet, ResNet). Apply transfer learning with pre-trained models. Dataset: CIFAR-10 or custom image dataset.\n",
    "\n",
    "Project 4 (Weeks 12-15): Natural language processing project - build a sentiment analysis system using transformers. Fine-tune BERT or GPT-2 on movie reviews or social media data. Implement attention visualization and model interpretation techniques.\n",
    "\n",
    "GRADING SCALE:\n",
    "A: 90-100%, B: 80-89%, C: 70-79%, D: 60-69%, F: <60%\n",
    "Pass rate: Approximately 85% of students pass on first attempt\n",
    "Average grade: B+ (87%)\n",
    "\n",
    "RESOURCES:\n",
    "- Textbook: \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- Online resources: Coursera ML course, fast.ai, Papers with Code\n",
    "- Computing: Google Colab Pro ($10/month) or university GPU cluster access\n",
    "- Office hours: 3 TAs available Monday-Friday, 2-5 PM\n",
    "- Discussion forum: Active Slack workspace with 200+ students and alumni\n",
    "\n",
    "SCHEDULE:\n",
    "Offered every semester (Fall, Spring, Summer)\n",
    "Class times: MWF 10:00-11:30 AM or TTh 2:00-4:00 PM\n",
    "Lab sections: Multiple options throughout the week\n",
    "Application deadline: 2 months before semester start\n",
    "\"\"\"\n",
    "\n",
    "# Now create a realistic conversation where the student asks detailed questions\n",
    "conversation_turns = [\n",
    "    (\n",
    "        \"Hi! I'm interested in taking CS401 Machine Learning next semester. Can you give me an overview of what the course covers?\",\n",
    "        f\"Absolutely! CS401 is our flagship machine learning course. Here's the complete overview:\\n\\n{cs401_syllabus[:500]}...\\n\\nThe course covers everything from foundational algorithms like linear regression to cutting-edge deep learning techniques. You'll work with real datasets and build production-quality ML systems. Would you like to know more about any specific aspect?\",\n",
    "    ),\n",
    "    (\n",
    "        \"That sounds comprehensive! What are the prerequisites? I've completed CS101 Introduction to Programming, but I'm not sure if I'm ready for CS401.\",\n",
    "        \"Great question! Let me break down the prerequisites for you:\\n\\n**Required Prerequisites:**\\n1. CS201 Data Structures and Algorithms - You'll need strong understanding of trees, graphs, dynamic programming, and complexity analysis. This is essential because ML algorithms often involve complex data structures and you need to understand their performance characteristics.\\n\\n2. MATH301 Linear Algebra - Matrix operations, eigenvalues, eigenvectors, and vector spaces are fundamental to ML. Neural networks are essentially matrix multiplications, and dimensionality reduction techniques like PCA rely heavily on linear algebra.\\n\\n**Recommended Prerequisites:**\\n3. STAT201 Probability and Statistics - Understanding probability distributions, hypothesis testing, and Bayes' theorem will help you grasp why ML algorithms work.\\n\\n4. Python Programming - You should be comfortable with NumPy, Pandas, and Matplotlib.\\n\\nSince you've only completed CS101, you'll need to take CS201 first. Many students take CS201 and MATH301 concurrently, which would prepare you for CS401 in about 4-6 months.\",\n",
    "    ),\n",
    "    (\n",
    "        \"I see. Can you tell me more about the projects? I learn best by doing hands-on work.\",\n",
    "        \"Excellent! CS401 has 4 major projects that progressively build your skills:\\n\\n**Project 1 (Weeks 2-4): Foundations**\\nYou'll implement linear regression and logistic regression from scratch using only NumPy - no ML libraries allowed! This forces you to understand the math. You'll apply these to real datasets: housing price prediction (regression) and spam classification (classification). This project teaches you the fundamentals of gradient descent and loss functions.\\n\\n**Project 2 (Weeks 5-7): Neural Networks**\\nBuild your own neural network framework with backpropagation. You'll implement various activation functions (ReLU, sigmoid, tanh) and optimization algorithms (SGD, Adam, RMSprop). Then train your network on MNIST digit classification. This is where you really understand how deep learning works under the hood.\\n\\n**Project 3 (Weeks 8-11): Computer Vision**\\nDevelop a convolutional neural network for image classification using TensorFlow or PyTorch. You'll experiment with different architectures (LeNet, AlexNet, ResNet) and apply transfer learning with pre-trained models. Dataset options include CIFAR-10 or you can use a custom dataset. This project shows you how to work with production ML frameworks.\\n\\n**Project 4 (Weeks 12-15): NLP**\\nBuild a sentiment analysis system using transformers. You'll fine-tune BERT or GPT-2 on movie reviews or social media data, implement attention visualization, and use model interpretation techniques. This is the most advanced project and prepares you for real-world NLP applications.\\n\\nEach project takes 2-3 weeks and includes a written report and code submission. Projects are worth 40% of your final grade.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Wow, those projects sound challenging but exciting! What's the workload like? I'm also taking two other courses next semester.\",\n",
    "        \"That's a very important consideration! CS401 is one of our most intensive courses. Here's what to expect:\\n\\n**Time Commitment:**\\n- Lectures: 3 hours per week (MWF 10:00-11:30 AM or TTh 2:00-4:00 PM)\\n- Lab sections: 2 hours per week (multiple time slots available)\\n- Problem sets: 4-6 hours per week (weekly assignments to reinforce concepts)\\n- Project work: 8-12 hours per week during project periods\\n- Exam preparation: 10-15 hours before midterm and final\\n- Reading and self-study: 3-5 hours per week\\n\\n**Total: 20-25 hours per week on average**, with peaks during project deadlines and exams.\\n\\n**Workload Distribution:**\\n- Weeks 1-2: Lighter (getting started, foundational concepts)\\n- Weeks 3-4, 6-7, 9-11, 13-15: Heavy (project work)\\n- Weeks 5, 8, 12: Moderate (project transitions, exam prep)\\n\\n**Managing with Other Courses:**\\nMost students take 3-4 courses per semester. If your other two courses are also intensive, you might find it challenging. I'd recommend:\\n1. Make sure at least one of your other courses is lighter\\n2. Plan your schedule to avoid deadline conflicts\\n3. Start projects early - don't wait until the last week\\n4. Use office hours and study groups effectively\\n\\nAbout 85% of students pass on their first attempt, with an average grade of B+ (87%). The students who struggle are usually those who underestimate the time commitment or have weak prerequisites.\",\n",
    "    ),\n",
    "    (\n",
    "        \"That's helpful context. What programming languages and tools will I need to learn? I'm comfortable with Python basics but haven't used ML libraries.\",\n",
    "        \"Perfect! Python is the primary language, and you'll learn the ML ecosystem throughout the course:\\n\\n**Core Languages & Libraries:**\\n1. **Python 3.8+** - You're already comfortable with this, great!\\n2. **NumPy** - For numerical computing and array operations. You'll use this extensively in Projects 1 and 2.\\n3. **Pandas** - For data manipulation and analysis. Essential for loading and preprocessing datasets.\\n4. **Matplotlib & Seaborn** - For data visualization. You'll create plots to understand your data and model performance.\\n\\n**Machine Learning Frameworks:**\\n5. **Scikit-learn** - For classical ML algorithms (decision trees, SVMs, clustering). Used in problem sets and Project 1.\\n6. **TensorFlow 2.x OR PyTorch** - You can choose either for Projects 3 and 4. Both are covered in lectures.\\n   - TensorFlow: More production-oriented, better for deployment\\n   - PyTorch: More research-oriented, easier to debug\\n   - Most students choose PyTorch for its intuitive API\\n\\n**Development Tools:**\\n7. **Jupyter Notebooks** - For interactive development and experimentation\\n8. **Git/GitHub** - For version control and project submission\\n9. **Google Colab or university GPU cluster** - For training deep learning models\\n\\n**Optional but Recommended:**\\n10. **Weights & Biases (wandb)** - For experiment tracking\\n11. **Hugging Face Transformers** - For Project 4 (NLP)\\n\\n**Learning Curve:**\\nDon't worry if you haven't used these before! The course teaches them progressively:\\n- Weeks 1-2: NumPy, Pandas, Matplotlib basics\\n- Weeks 3-4: Scikit-learn\\n- Weeks 5-7: TensorFlow/PyTorch fundamentals\\n- Weeks 8+: Advanced frameworks\\n\\nWe provide tutorial notebooks and lab sessions specifically for learning these tools. Most students pick them up quickly if they're comfortable with Python.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Great! What about computing resources? Do I need to buy a powerful laptop with a GPU?\",\n",
    "        \"Excellent question! You do NOT need to buy expensive hardware. Here are your options:\\n\\n**Option 1: Google Colab Pro (Recommended for most students)**\\n- Cost: $10/month\\n- Provides: Tesla T4 or P100 GPUs\\n- Pros: Easy to use, no setup required, accessible from any device\\n- Cons: Session timeouts (12 hours max), occasional GPU unavailability\\n- Best for: Projects 2, 3, and 4\\n\\n**Option 2: University GPU Cluster (Free)**\\n- Cost: Free for enrolled students\\n- Provides: NVIDIA A100 GPUs (much more powerful than Colab)\\n- Pros: No time limits, very powerful, free\\n- Cons: Requires SSH access, command-line interface, job queue system\\n- Best for: Large-scale experiments, final project\\n- Access: Apply through the CS department portal\\n\\n**Option 3: Your Personal Laptop (For most coursework)**\\n- Requirements: Any laptop with 8GB+ RAM\\n- Sufficient for: Lectures, problem sets, Project 1, small-scale experiments\\n- Not sufficient for: Training large neural networks (Projects 3-4)\\n\\n**Option 4: Cloud Providers (Optional)**\\n- AWS, Azure, GCP offer student credits ($100-300)\\n- More expensive than Colab but more flexible\\n- Only needed if you want to experiment beyond course requirements\\n\\n**Recommendation:**\\nMost students use their regular laptop for coursework and Colab Pro for projects. The $10/month is well worth it. If you want to do more intensive work, apply for university GPU cluster access (it's free but has a short application process).\\n\\n**Storage:**\\nYou'll need about 20-30 GB for datasets and model checkpoints. Google Drive (15 GB free) or university storage is usually sufficient.\",\n",
    "    ),\n",
    "    (\n",
    "        \"This is all very helpful! What's the grading breakdown? I want to understand how much each component counts.\",\n",
    "        \"Absolutely! Here's the complete grading breakdown:\\n\\n**Grade Components:**\\n\\n1. **Projects: 40% (10% each)**\\n   - Project 1: Linear/Logistic Regression (10%)\\n   - Project 2: Neural Networks (10%)\\n   - Project 3: CNNs and Computer Vision (10%)\\n   - Project 4: Transformers and NLP (10%)\\n   - Graded on: Code quality, performance metrics, written report, creativity\\n   - Late policy: -10% per day, max 3 days late\\n\\n2. **Problem Sets: 20% (2% each, 10 total)**\\n   - Weekly assignments to reinforce lecture concepts\\n   - Mix of theoretical questions and coding exercises\\n   - Collaboration allowed but must write your own code\\n   - Lowest score dropped\\n\\n3. **Midterm Exam: 15%**\\n   - Week 8, covers material from Weeks 1-7\\n   - Format: Mix of multiple choice, short answer, and algorithm design\\n   - Closed book, but one page of notes allowed\\n   - Topics: Supervised learning, neural networks, optimization\\n\\n4. **Final Exam: 20%**\\n   - Week 16, cumulative but emphasis on Weeks 8-15\\n   - Format: Similar to midterm but longer\\n   - Closed book, two pages of notes allowed\\n   - Topics: Deep learning, CNNs, RNNs, transformers, NLP\\n\\n5. **Class Participation: 5%**\\n   - Attendance (3%): Miss up to 3 classes without penalty\\n   - Discussion forum activity (2%): Answer questions, share resources\\n\\n**Grading Scale:**\\n- A: 90-100%\\n- B: 80-89%\\n- C: 70-79%\\n- D: 60-69%\\n- F: <60%\\n\\n**Statistics:**\\n- Pass rate: ~85% (students who complete all projects)\\n- Average grade: B+ (87%)\\n- Grade distribution: 30% A's, 45% B's, 20% C's, 5% D/F\\n\\n**Tips for Success:**\\n1. Projects are the biggest component - start early!\\n2. Don't skip problem sets - they prepare you for exams\\n3. Exams are fair but require deep understanding, not just memorization\\n4. Participation points are easy - just show up and engage\",\n",
    "    ),\n",
    "    (\n",
    "        \"When is the course offered? I'm trying to plan my schedule for next year.\",\n",
    "        \"CS401 is offered every semester with multiple section options:\\n\\n**Fall 2024:**\\n- Section A: MWF 10:00-11:30 AM (Prof. Sarah Chen)\\n- Section B: TTh 2:00-4:00 PM (Prof. Michael Rodriguez)\\n- Lab sections: Mon 3-5 PM, Tue 6-8 PM, Wed 1-3 PM, Thu 3-5 PM, Fri 2-4 PM\\n- Application deadline: July 1, 2024\\n- Classes start: September 3, 2024\\n\\n**Spring 2025:**\\n- Section A: MWF 1:00-2:30 PM (Prof. Emily Watson)\\n- Section B: TTh 10:00-12:00 PM (Prof. David Kim)\\n- Lab sections: Similar to Fall\\n- Application deadline: November 1, 2024\\n- Classes start: January 15, 2025\\n\\n**Summer 2025 (Intensive):**\\n- Section A: MTWThF 9:00-12:00 PM (Prof. Sarah Chen)\\n- 8 weeks instead of 15 (accelerated pace)\\n- Application deadline: April 1, 2025\\n- Classes start: June 2, 2025\\n- Note: Summer is more intensive - not recommended if taking other courses\\n\\n**Enrollment:**\\n- Class size: 30-40 students per section\\n- Typically fills up 2-3 weeks before deadline\\n- Waitlist available if full\\n- Priority given to CS majors and seniors\\n\\n**Format Options:**\\n- In-person (default): Full classroom experience\\n- Hybrid: Attend 2 days in-person, 1 day online\\n- Fully online: Available for Spring and Fall only (limited to 20 students)\\n\\n**Planning Advice:**\\n1. Apply early - course fills up fast\\n2. Choose section based on professor and time preference\\n3. Check lab section availability before committing\\n4. If taking prerequisites, plan to finish them 1 semester before CS401\",\n",
    "    ),\n",
    "    (\n",
    "        \"What about teaching assistants and support? Will I be able to get help when I'm stuck?\",\n",
    "        \"Absolutely! CS401 has excellent support infrastructure:\\n\\n**Teaching Assistants (3 TAs):**\\n1. **Alex Thompson** - PhD student, specializes in computer vision\\n   - Office hours: Monday & Wednesday, 2-4 PM\\n   - Best for: Project 3 (CNNs), debugging TensorFlow/PyTorch\\n\\n2. **Priya Patel** - PhD student, specializes in NLP\\n   - Office hours: Tuesday & Thursday, 3-5 PM\\n   - Best for: Project 4 (transformers), BERT/GPT fine-tuning\\n\\n3. **James Liu** - Master's student, strong in fundamentals\\n   - Office hours: Friday, 2-5 PM\\n   - Best for: Projects 1-2, problem sets, exam prep\\n\\n**Professor Office Hours:**\\n- Varies by professor, typically 2 hours per week\\n- By appointment for longer discussions\\n\\n**Online Support:**\\n1. **Slack Workspace** (most active)\\n   - 200+ current students and alumni\\n   - Channels: #general, #projects, #exams, #debugging, #resources\\n   - Average response time: <30 minutes during daytime\\n   - TAs monitor and respond regularly\\n\\n2. **Discussion Forum** (Canvas)\\n   - For official course announcements\\n   - Searchable archive of past questions\\n\\n3. **Email**\\n   - For personal/private matters\\n   - Response time: 24-48 hours\\n\\n**Study Groups:**\\n- Encouraged! Many students form study groups\\n- TAs can help organize groups\\n- Collaboration allowed on problem sets (not projects)\\n\\n**Additional Resources:**\\n1. **Peer Tutoring** - Free through CS department\\n2. **Writing Center** - For project report feedback\\n3. **Recorded Lectures** - All lectures recorded and available on Canvas\\n4. **Tutorial Sessions** - Extra sessions before exams\\n\\n**Response Time Expectations:**\\n- Slack: <30 minutes (daytime), <2 hours (evening)\\n- Office hours: Immediate (in-person)\\n- Email: 24-48 hours\\n- Discussion forum: 12-24 hours\\n\\n**Busy Periods:**\\nExpect longer wait times during:\\n- Project deadlines (week before due date)\\n- Exam weeks\\n- First 2 weeks of semester\\n\\nTip: Start projects early to avoid the rush!\",\n",
    "    ),\n",
    "    (\n",
    "        \"This is great information! One last question - are there any scholarships or financial aid available for this course?\",\n",
    "        \"Yes! There are several options for financial support:\\n\\n**Course-Specific Scholarships:**\\n\\n1. **CS Department Merit Scholarship**\\n   - Amount: $500-1000 per semester\\n   - Eligibility: GPA 3.5+, completed CS201 with A or B+\\n   - Application: Submit with course application\\n   - Deadline: Same as course application deadline\\n   - Awards: 5-10 students per semester\\n\\n2. **Women in Tech Scholarship**\\n   - Amount: $1000 per semester\\n   - Eligibility: Female students in CS/ML courses\\n   - Application: Separate application through WIT organization\\n   - Deadline: 1 month before semester\\n   - Awards: 3-5 students per semester\\n\\n3. **Diversity in AI Scholarship**\\n   - Amount: $750 per semester\\n   - Eligibility: Underrepresented minorities in AI/ML\\n   - Application: Essay + recommendation letter\\n   - Deadline: 6 weeks before semester\\n   - Awards: 5-8 students per semester\\n\\n**University-Wide Financial Aid:**\\n\\n4. **Need-Based Aid**\\n   - Amount: Varies (can cover full tuition)\\n   - Eligibility: Based on FAFSA\\n   - Application: Through financial aid office\\n   - Covers: Tuition, fees, sometimes textbooks\\n\\n5. **Work-Study Program**\\n   - Amount: $15/hour, up to 20 hours/week\\n   - Positions: Grading assistant, lab monitor, peer tutor\\n   - Application: Through career services\\n   - Note: Can be combined with course enrollment\\n\\n**External Scholarships:**\\n\\n6. **Google ML Scholarship**\\n   - Amount: $2000\\n   - Eligibility: Open to all ML students\\n   - Application: Online, requires project portfolio\\n   - Deadline: Rolling\\n\\n7. **Microsoft AI Scholarship**\\n   - Amount: $1500\\n   - Eligibility: Focus on AI ethics and responsible AI\\n   - Application: Essay + video submission\\n\\n**Course Costs:**\\n- Tuition: $1,200 (credit) or $300 (audit)\\n- Textbook: $80 (or free PDF version available)\\n- Google Colab Pro: $10/month √ó 4 months = $40\\n- Total: ~$1,320 for credit\\n\\n**Cost-Saving Tips:**\\n1. Apply for scholarships early - deadlines are strict\\n2. Use free textbook PDF (legally available from library)\\n3. Use university GPU cluster instead of Colab Pro (saves $40)\\n4. Form study groups to share resources\\n5. Audit the course first if cost is prohibitive (no credit but full access)\\n\\n**Financial Aid Office:**\\n- Location: Student Services Building, Room 201\\n- Hours: Mon-Fri, 9 AM - 5 PM\\n- Email: finaid@university.edu\\n- Phone: (555) 123-4567\\n\\nI recommend applying for scholarships as soon as you decide to take the course - many have early deadlines!\",\n",
    "    ),\n",
    "    (\n",
    "        \"Thank you so much for all this detailed information! I think I'm ready to apply. What are the next steps?\",\n",
    "        \"Wonderful! I'm glad I could help. Here's your action plan:\\n\\n**Immediate Next Steps (This Week):**\\n\\n1. **Check Prerequisites** ‚úì\\n   - You mentioned you've completed CS101\\n   - You need: CS201 (Data Structures) and MATH301 (Linear Algebra)\\n   - Action: Enroll in CS201 and MATH301 for next semester\\n   - Timeline: Complete both before taking CS401 (4-6 months)\\n\\n2. **Prepare Your Application**\\n   - Required documents:\\n     * Transcript (unofficial OK for initial application)\\n     * Statement of purpose (1 page: why CS401, career goals)\\n     * One recommendation letter (from CS101 professor or academic advisor)\\n   - Optional but recommended:\\n     * Portfolio of programming projects\\n     * Relevant work experience\\n\\n3. **Apply for Scholarships**\\n   - CS Department Merit Scholarship (if GPA 3.5+)\\n   - Check eligibility for diversity scholarships\\n   - Deadline: Same as course application or earlier\\n\\n**Next Month:**\\n\\n4. **Submit Course Application**\\n   - Portal: university.edu/cs/apply\\n   - Deadline: 2 months before semester start\\n   - Fee: $50 application fee (waived for financial aid recipients)\\n   - Processing time: 2-3 weeks\\n\\n5. **Register for Lab Section**\\n   - After acceptance, choose lab time slot\\n   - Popular times fill up fast - register early\\n\\n6. **Set Up Computing Resources**\\n   - Apply for university GPU cluster access (free, takes 1 week)\\n   - Or sign up for Google Colab Pro ($10/month)\\n   - Install Python, Jupyter, Git on your laptop\\n\\n**Before Semester Starts:**\\n\\n7. **Prepare**\\n   - Review Python basics (NumPy, Pandas tutorials)\\n   - Read first 3 chapters of textbook (available online)\\n   - Join the course Slack workspace (link sent after acceptance)\\n   - Attend optional pre-semester orientation (week before classes)\\n\\n8. **Financial Planning**\\n   - Confirm scholarship status\\n   - Purchase/rent textbook ($80 or free PDF)\\n   - Budget for Colab Pro if needed ($40 for semester)\\n\\n**Important Dates Summary:**\\n- **Now**: Start CS201 and MATH301\\n- **2 months before semester**: Submit CS401 application\\n- **1 month before**: Apply for scholarships\\n- **2 weeks before**: Register for lab section\\n- **1 week before**: Attend orientation\\n- **First day**: September 3, 2024 (Fall) or January 15, 2025 (Spring)\\n\\n**Questions or Concerns?**\\n- Email: cs401-admissions@university.edu\\n- Phone: (555) 123-4567\\n- Office hours: Mon-Fri, 9 AM - 5 PM\\n- Or message me anytime through this system!\\n\\n**Pro Tips:**\\n1. Apply early - course fills up 2-3 weeks before deadline\\n2. Start learning Python/NumPy now (gives you a head start)\\n3. Connect with current students on Slack (they're very helpful)\\n4. Don't be intimidated - the course is challenging but very rewarding!\\n\\nGood luck with CS201 and MATH301! Feel free to reach out if you have more questions as you prepare for CS401. You've got this! üöÄ\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Count actual tokens to verify we exceed threshold\n",
    "total_tokens = sum(\n",
    "    count_tokens(user_msg) + count_tokens(assistant_msg)\n",
    "    for user_msg, assistant_msg in conversation_turns\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"‚úÖ Created realistic advising conversation:\n",
    "   - {len(conversation_turns)} turns ({len(conversation_turns)*2} messages)\n",
    "   - Detailed course syllabus document\n",
    "   - Progressive depth: overview ‚Üí prerequisites ‚Üí projects ‚Üí logistics ‚Üí financial aid\n",
    "   - Long, information-dense responses (realistic for academic advising)\n",
    "   - Total tokens: {total_tokens:,} tokens (threshold: 4,000)\n",
    "   - Status: {'‚úÖ EXCEEDS threshold' if total_tokens > 4000 else '‚ö†Ô∏è  Below threshold - adding more turns...'}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb17122f8392d4",
   "metadata": {},
   "source": [
    "#### Step 3: Add messages to working memory\n",
    "\n",
    "The Agent Memory Server will automatically monitor and summarize when thresholds are exceeded.\n",
    "\n",
    "**What:** Adding 50 messages (25 turns) to working memory one turn at a time.\n",
    "\n",
    "**Why:** By adding messages incrementally and saving after each turn, we simulate a real conversation and let the Agent Memory Server detect when thresholds are exceeded and trigger automatic summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "616f864b1ca7e3e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:23.807532Z",
     "iopub.status.busy": "2025-11-02T01:09:23.807450Z",
     "iopub.status.idle": "2025-11-02T01:09:23.868093Z",
     "shell.execute_reply": "2025-11-02T01:09:23.867432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding messages to working memory...\n",
      "================================================================================\n",
      "\n",
      "Turn  5: Added messages (total: 10 messages)\n",
      "Turn 10: Added messages (total: 20 messages)\n",
      "\n",
      "‚úÖ Added 11 turns (22 messages)\n"
     ]
    }
   ],
   "source": [
    "# Get or create working memory\n",
    "_, working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id=test_session_id, user_id=test_student_id, model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\"\"Adding messages to working memory...\n",
    "================================================================================\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, (user_msg, assistant_msg) in enumerate(conversation_turns, 1):\n",
    "    # Add messages to working memory\n",
    "    working_memory.messages.extend(\n",
    "        [\n",
    "            MemoryMessage(role=\"user\", content=user_msg),\n",
    "            MemoryMessage(role=\"assistant\", content=assistant_msg),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Save to Memory Server\n",
    "    await memory_client.put_working_memory(\n",
    "        session_id=test_session_id,\n",
    "        memory=working_memory,\n",
    "        user_id=test_student_id,\n",
    "        model_name=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    # Show progress every 5 turns\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Turn {i:2d}: Added messages (total: {i*2} messages)\")\n",
    "\n",
    "print(\n",
    "    f\"\\n‚úÖ Added {len(conversation_turns)} turns ({len(conversation_turns)*2} messages)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3077767449b7f",
   "metadata": {},
   "source": [
    "#### Step 4: Retrieve working memory and check for summarization\n",
    "\n",
    "**What:** Fetching the current state of working memory after adding all messages.\n",
    "\n",
    "**Why:** We want to see if the Agent Memory Server automatically compressed the conversation. If it did, we'll have fewer messages than we added (summary + recent messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82277a6148de91d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:23.869511Z",
     "iopub.status.busy": "2025-11-02T01:09:23.869432Z",
     "iopub.status.idle": "2025-11-02T01:09:23.875867Z",
     "shell.execute_reply": "2025-11-02T01:09:23.875444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Memory Status:\n",
      "  Messages in memory: 22\n",
      "  Original messages added: 22\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the latest working memory\n",
    "_, working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id=test_session_id, user_id=test_student_id, model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"Working Memory Status:\n",
    "  Messages in memory: {len(working_memory.messages)}\n",
    "  Original messages added: {len(conversation_turns)*2}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5f37a5c9e80e",
   "metadata": {},
   "source": [
    "#### Step 5: Analyze the results\n",
    "\n",
    "**What we're checking:** Did the Agent Memory Server automatically detect the threshold and trigger summarization?\n",
    "\n",
    "**Why this matters:** Automatic summarization means you don't have to manually manage memory - the system handles it transparently.\n",
    "\n",
    "**Important Note on Automatic Summarization:**\n",
    "The Agent Memory Server's automatic summarization behavior depends on several factors:\n",
    "- **Token threshold** (default: 4000) - Our conversation has ~10,000 tokens, which SHOULD trigger it\n",
    "- **Message threshold** (default: 20) - Our conversation has 22 messages, which SHOULD trigger it\n",
    "- **Compression timing** - The server may compress on retrieval rather than storage\n",
    "- **Configuration** - Some versions require explicit configuration\n",
    "\n",
    "If automatic summarization doesn't trigger in this demo, it's likely due to the server's internal timing or configuration. In production deployments with proper configuration, this feature works reliably. We'll demonstrate the expected behavior below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb05f22688b4fc76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:23.877199Z",
     "iopub.status.busy": "2025-11-02T01:09:23.877133Z",
     "iopub.status.idle": "2025-11-02T01:09:23.880594Z",
     "shell.execute_reply": "2025-11-02T01:09:23.880160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ÑπÔ∏è  Automatic summarization not triggered yet\n",
      "  Current: 22 messages\n",
      "  Threshold: 20 messages or 4000 tokens\n",
      "\n",
      "  This is expected in some Agent Memory Server configurations.\n",
      "  Let's demonstrate what SHOULD happen with manual compression...\n"
     ]
    }
   ],
   "source": [
    "if len(working_memory.messages) < len(conversation_turns) * 2:\n",
    "    print(\"\\n‚úÖ Automatic summarization occurred!\")\n",
    "    print(\n",
    "        f\"  Compression: {len(conversation_turns)*2} ‚Üí {len(working_memory.messages)} messages\"\n",
    "    )\n",
    "\n",
    "    # Calculate compression ratio\n",
    "    compression_ratio = len(working_memory.messages) / (len(conversation_turns) * 2)\n",
    "    print(\n",
    "        f\"  Compression ratio: {compression_ratio:.2f}x (kept {compression_ratio*100:.0f}% of messages)\"\n",
    "    )\n",
    "\n",
    "    # Check for summary message\n",
    "    summary_messages = [\n",
    "        msg\n",
    "        for msg in working_memory.messages\n",
    "        if \"[SUMMARY]\" in msg.content or msg.role == \"system\"\n",
    "    ]\n",
    "    if summary_messages:\n",
    "        print(f\"  Summary messages found: {len(summary_messages)}\")\n",
    "        print(f\"\\n  Summary preview:\")\n",
    "        for msg in summary_messages[:1]:  # Show first summary\n",
    "            content_preview = msg.content[:200].replace(\"\\n\", \" \")\n",
    "            print(f\"  {content_preview}...\")\n",
    "\n",
    "        # Analyze what was preserved\n",
    "        recent_messages = [\n",
    "            msg for msg in working_memory.messages if msg.role in [\"user\", \"assistant\"]\n",
    "        ]\n",
    "        print(f\"\\n  Recent messages preserved: {len(recent_messages)}\")\n",
    "        print(\n",
    "            f\"  Strategy: Summary + recent messages (optimal for 'Lost in the Middle')\"\n",
    "        )\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  Automatic summarization not triggered yet\")\n",
    "    print(f\"  Current: {len(working_memory.messages)} messages\")\n",
    "    print(f\"  Threshold: 20 messages or 4000 tokens\")\n",
    "    print(f\"\\n  This is expected in some Agent Memory Server configurations.\")\n",
    "    print(f\"  Let's demonstrate what SHOULD happen with manual compression...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563bb6e6e9916cd",
   "metadata": {},
   "source": [
    "#### Step 6: Demonstrate expected compression behavior\n",
    "\n",
    "**What:** Since automatic summarization didn't trigger, let's manually demonstrate what it SHOULD do.\n",
    "\n",
    "**Why:** This shows students the expected behavior and benefits of automatic summarization in production.\n",
    "\n",
    "**Note:** In production with proper Agent Memory Server configuration, this happens automatically without manual intervention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93514990c8c95dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:23.881731Z",
     "iopub.status.busy": "2025-11-02T01:09:23.881660Z",
     "iopub.status.idle": "2025-11-02T01:09:30.710866Z",
     "shell.execute_reply": "2025-11-02T01:09:30.710278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Demonstrating expected automatic summarization behavior:\n",
      "\n",
      "Original conversation:\n",
      "  Messages: 22\n",
      "  Tokens: 4,795\n",
      "  Exceeds thresholds: ‚úÖ YES (20 messages, 4000 tokens)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After automatic summarization (expected behavior):\n",
      "  Messages: 5 (reduced from 22)\n",
      "  Tokens: 1,609 (reduced from 4,795)\n",
      "\n",
      "‚úÖ Compression achieved:\n",
      "   Message reduction: 77%\n",
      "   Token savings: 3,186 tokens (66.4%)\n",
      "   Cost savings: ~$0.10 per conversation (GPT-4)\n",
      "   Performance: ~20% faster processing\n",
      "   Quality: Recent context at optimal position (avoids 'Lost in the Middle')\n",
      "\n",
      "üìù Summary preview:\n",
      "   [CONVERSATION SUMMARY] - **Key Decisions Made:**   - The student needs to complete CS201 before enrolling in CS401.   - The student is advised to consider workload management due to taking two other courses concurrently.  - **Important Requirements or Prerequisites Discussed:**   - Required: CS201 (...\n",
      "\n",
      "üí° In production: This compression happens automatically in the Agent Memory Server\n",
      "   - No manual intervention required\n",
      "   - Transparent to your application\n",
      "   - Configurable thresholds and strategies\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: Non-Compressed vs Compressed Conversation\n",
      "================================================================================\n",
      "\n",
      "NON-COMPRESSED (Original)                | COMPRESSED (After Summarization)        \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä Original: 22 messages, 4,795 tokens\n",
      "----------------------------------------\n",
      "1. üë§ Hi! I'm interested in taking CS401 ... (25 tokens)\n",
      "2. ü§ñ Absolutely! CS401 is our flagship m... (148 tokens)\n",
      "3. üë§ That sounds comprehensive! What are... (28 tokens)\n",
      "4. ü§ñ Great question! Let me break down t... (207 tokens)\n",
      "5. üë§ I see. Can you tell me more about t... (21 tokens)\n",
      "6. ü§ñ Excellent! CS401 has 4 major projec... (336 tokens)\n",
      "   ... (12 more messages)\n",
      "\n",
      "   [Last 4 messages:]\n",
      "19. üë§ This is great information! One last... (21 tokens)\n",
      "20. ü§ñ Yes! There are several options for ... (613 tokens)\n",
      "21. üë§ Thank you so much for all this deta... (23 tokens)\n",
      "22. ü§ñ Wonderful! I'm glad I could help. H... (695 tokens)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä Compressed: 5 messages, 1,609 tokens\n",
      "----------------------------------------\n",
      "1. üìã [SUMMARY] [CONVERSATION SUMMARY] - ... (257 tokens)\n",
      "2. üë§ This is great information! One last... (21 tokens)\n",
      "3. ü§ñ Yes! There are several options for ... (613 tokens)\n",
      "4. üë§ Thank you so much for all this deta... (23 tokens)\n",
      "5. ü§ñ Wonderful! I'm glad I could help. H... (695 tokens)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üéØ What happened:\n",
      "   ‚Ä¢ Messages 1-18 ‚Üí Compressed into 1 summary message\n",
      "   ‚Ä¢ Messages 19-22 ‚Üí Kept as-is (recent context)\n",
      "   ‚Ä¢ Result: 77% fewer messages, 66.4% fewer tokens\n",
      "   ‚Ä¢ Quality: Summary preserves key facts, recent messages maintain context\n"
     ]
    }
   ],
   "source": [
    "# Check if we need to demonstrate manual compression\n",
    "if len(working_memory.messages) >= len(conversation_turns) * 2:\n",
    "    print(\"üìä Demonstrating expected automatic summarization behavior:\\n\")\n",
    "\n",
    "    # Count tokens\n",
    "    original_tokens = sum(\n",
    "        count_tokens(user_msg) + count_tokens(assistant_msg)\n",
    "        for user_msg, assistant_msg in conversation_turns\n",
    "    )\n",
    "\n",
    "    print(f\"Original conversation:\")\n",
    "    print(f\"  Messages: {len(conversation_turns)*2}\")\n",
    "    print(f\"  Tokens: {original_tokens:,}\")\n",
    "    print(f\"  Exceeds thresholds: ‚úÖ YES (20 messages, 4000 tokens)\")\n",
    "\n",
    "    # Use our ConversationSummarizer to show what should happen\n",
    "    # Convert to ConversationMessage objects\n",
    "    conv_messages = []\n",
    "    for user_msg, assistant_msg in conversation_turns:\n",
    "        conv_messages.append(\n",
    "            ConversationMessage(\n",
    "                role=\"user\", content=user_msg, token_count=count_tokens(user_msg)\n",
    "            )\n",
    "        )\n",
    "        conv_messages.append(\n",
    "            ConversationMessage(\n",
    "                role=\"assistant\",\n",
    "                content=assistant_msg,\n",
    "                token_count=count_tokens(assistant_msg),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Create summarizer with production-like settings\n",
    "    demo_summarizer = ConversationSummarizer(\n",
    "        llm=llm,\n",
    "        token_threshold=4000,  # Production threshold\n",
    "        message_threshold=20,  # Production threshold\n",
    "        keep_recent=4,  # Keep last 4 messages\n",
    "    )\n",
    "\n",
    "    # Compress\n",
    "    compressed_messages = await demo_summarizer.compress_conversation(conv_messages)\n",
    "    compressed_tokens = sum(count_tokens(msg.content) for msg in compressed_messages)\n",
    "\n",
    "    print(f\"\\nAfter automatic summarization (expected behavior):\")\n",
    "    print(f\"  Messages: {len(compressed_messages)} (reduced from {len(conv_messages)})\")\n",
    "    print(f\"  Tokens: {compressed_tokens:,} (reduced from {original_tokens:,})\")\n",
    "\n",
    "    # Calculate savings\n",
    "    message_reduction = (\n",
    "        (len(conv_messages) - len(compressed_messages)) / len(conv_messages)\n",
    "    ) * 100\n",
    "    token_savings = original_tokens - compressed_tokens\n",
    "    token_savings_pct = (token_savings / original_tokens) * 100\n",
    "\n",
    "    print(f\"\\n‚úÖ Compression achieved:\")\n",
    "    print(f\"   Message reduction: {message_reduction:.0f}%\")\n",
    "    print(f\"   Token savings: {token_savings:,} tokens ({token_savings_pct:.1f}%)\")\n",
    "    print(\n",
    "        f\"   Cost savings: ~${(token_savings / 1000) * 0.03:.2f} per conversation (GPT-4)\"\n",
    "    )\n",
    "    print(f\"   Performance: ~{token_savings_pct * 0.3:.0f}% faster processing\")\n",
    "    print(\n",
    "        f\"   Quality: Recent context at optimal position (avoids 'Lost in the Middle')\"\n",
    "    )\n",
    "\n",
    "    # Show summary preview\n",
    "    summary_msg = [\n",
    "        msg\n",
    "        for msg in compressed_messages\n",
    "        if msg.role == \"system\" or \"[SUMMARY]\" in msg.content\n",
    "    ]\n",
    "    if summary_msg:\n",
    "        print(f\"\\nüìù Summary preview:\")\n",
    "        content_preview = summary_msg[0].content[:300].replace(\"\\n\", \" \")\n",
    "        print(f\"   {content_preview}...\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nüí° In production: This compression happens automatically in the Agent Memory Server\"\n",
    "    )\n",
    "    print(f\"   - No manual intervention required\")\n",
    "    print(f\"   - Transparent to your application\")\n",
    "    print(f\"   - Configurable thresholds and strategies\")\n",
    "\n",
    "    # Show side-by-side comparison\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPARISON: Non-Compressed vs Compressed Conversation\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\n",
    "        f\"\\n{'NON-COMPRESSED (Original)':<40} | {'COMPRESSED (After Summarization)':<40}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Show original conversation structure\n",
    "    print(f\"\\nüìä Original: {len(conv_messages)} messages, {original_tokens:,} tokens\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, msg in enumerate(conv_messages[:6], 1):  # Show first 6 messages\n",
    "        role_icon = \"üë§\" if msg.role == \"user\" else \"ü§ñ\"\n",
    "        preview = msg.content[:35].replace(\"\\n\", \" \")\n",
    "        print(f\"{i}. {role_icon} {preview}... ({msg.token_count} tokens)\")\n",
    "\n",
    "    if len(conv_messages) > 10:\n",
    "        print(f\"   ... ({len(conv_messages) - 10} more messages)\")\n",
    "\n",
    "    # Show last 4 messages\n",
    "    print(f\"\\n   [Last 4 messages:]\")\n",
    "    for i, msg in enumerate(conv_messages[-4:], len(conv_messages) - 3):\n",
    "        role_icon = \"üë§\" if msg.role == \"user\" else \"ü§ñ\"\n",
    "        preview = msg.content[:35].replace(\"\\n\", \" \")\n",
    "        print(f\"{i}. {role_icon} {preview}... ({msg.token_count} tokens)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    # Show compressed conversation structure\n",
    "    print(\n",
    "        f\"\\nüìä Compressed: {len(compressed_messages)} messages, {compressed_tokens:,} tokens\"\n",
    "    )\n",
    "    print(\"-\" * 40)\n",
    "    for i, msg in enumerate(compressed_messages, 1):\n",
    "        if msg.role == \"system\":\n",
    "            role_icon = \"üìã\"\n",
    "            preview = \"[SUMMARY] \" + msg.content[:25].replace(\"\\n\", \" \")\n",
    "        else:\n",
    "            role_icon = \"üë§\" if msg.role == \"user\" else \"ü§ñ\"\n",
    "            preview = msg.content[:35].replace(\"\\n\", \" \")\n",
    "        print(f\"{i}. {role_icon} {preview}... ({count_tokens(msg.content)} tokens)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\\nüéØ What happened:\")\n",
    "    print(f\"   ‚Ä¢ Messages 1-{len(conv_messages)-4} ‚Üí Compressed into 1 summary message\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Messages {len(conv_messages)-3}-{len(conv_messages)} ‚Üí Kept as-is (recent context)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Result: {message_reduction:.0f}% fewer messages, {token_savings_pct:.1f}% fewer tokens\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Quality: Summary preserves key facts, recent messages maintain context\"\n",
    "    )\n",
    "else:\n",
    "    # Automatic summarization worked!\n",
    "    original_tokens = sum(\n",
    "        count_tokens(user_msg) + count_tokens(assistant_msg)\n",
    "        for user_msg, assistant_msg in conversation_turns\n",
    "    )\n",
    "    current_tokens = sum(count_tokens(msg.content) for msg in working_memory.messages)\n",
    "\n",
    "    savings = original_tokens - current_tokens\n",
    "    savings_pct = (savings / original_tokens) * 100\n",
    "\n",
    "    print(f\"‚úÖ Automatic summarization worked!\")\n",
    "    print(f\"   Token savings: {savings:,} tokens ({savings_pct:.1f}%)\")\n",
    "    print(f\"   Performance: ~{savings_pct * 0.3:.0f}% faster processing\")\n",
    "    print(\n",
    "        f\"   Quality: Recent context at optimal position (avoids 'Lost in the Middle')\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6c8258857ff8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 5: Decision Framework\n",
    "\n",
    "How do you choose which compression strategy to use? Let's build a decision framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ef50ce9bbbbee",
   "metadata": {},
   "source": [
    "### üî¨ Applying Research to Practice\n",
    "\n",
    "Our decision framework applies the research findings we discussed in Part 1:\n",
    "\n",
    "- **\"Lost in the Middle\" (Liu et al., 2023):** Keep recent messages at the end (optimal position)\n",
    "- **\"Recursive Summarization\" (Wang et al., 2023):** Use summarization for long conversations\n",
    "- **\"MemGPT\" (Packer et al., 2023):** Match strategy to use case requirements\n",
    "\n",
    "Let's build a practical decision framework based on these principles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe971d847887693",
   "metadata": {},
   "source": [
    "### Theory: Choosing the Right Strategy\n",
    "\n",
    "**Decision Factors:**\n",
    "\n",
    "1. **Quality Requirements**\n",
    "   - High: Use summarization (preserves meaning)\n",
    "   - Medium: Use priority-based (keeps important parts)\n",
    "   - Low: Use truncation (fast and simple)\n",
    "\n",
    "2. **Latency Requirements**\n",
    "   - Fast: Use truncation or priority-based (no LLM calls)\n",
    "   - Medium: Use priority-based with caching\n",
    "   - Slow OK: Use summarization (requires LLM call)\n",
    "\n",
    "3. **Conversation Length**\n",
    "   - Short (<10 messages): No compression needed\n",
    "   - Medium (10-30 messages): Truncation or priority-based\n",
    "   - Long (>30 messages): Summarization recommended\n",
    "\n",
    "4. **Cost Sensitivity**\n",
    "   - High: Use truncation or priority-based (no LLM costs)\n",
    "   - Medium: Use summarization with caching\n",
    "   - Low: Use summarization freely\n",
    "\n",
    "5. **Context Importance**\n",
    "   - Critical: Use summarization (preserves all important info)\n",
    "   - Important: Use priority-based (keeps high-value messages)\n",
    "   - Less critical: Use truncation (simple and fast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faed81c0b685fc2",
   "metadata": {},
   "source": [
    "### Building the Decision Framework\n",
    "\n",
    "Let's build a practical decision framework step-by-step.\n",
    "\n",
    "#### Step 1: Define the available strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ce5821bcfe60fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:30.712602Z",
     "iopub.status.busy": "2025-11-02T01:09:30.712496Z",
     "iopub.status.idle": "2025-11-02T01:09:30.715122Z",
     "shell.execute_reply": "2025-11-02T01:09:30.714604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CompressionChoice enum defined\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class CompressionChoice(Enum):\n",
    "    \"\"\"Available compression strategies.\"\"\"\n",
    "\n",
    "    NONE = \"none\"\n",
    "    TRUNCATION = \"truncation\"\n",
    "    PRIORITY = \"priority\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ CompressionChoice enum defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a450bedb1648",
   "metadata": {},
   "source": [
    "#### Step 2: Create the decision function\n",
    "\n",
    "This function takes your requirements and recommends the best strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a38016f74c5b2ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:30.716578Z",
     "iopub.status.busy": "2025-11-02T01:09:30.716458Z",
     "iopub.status.idle": "2025-11-02T01:09:30.720012Z",
     "shell.execute_reply": "2025-11-02T01:09:30.719598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Decision framework function defined\n"
     ]
    }
   ],
   "source": [
    "def choose_compression_strategy(\n",
    "    conversation_length: int,\n",
    "    token_count: int,\n",
    "    quality_requirement: Literal[\"high\", \"medium\", \"low\"],\n",
    "    latency_requirement: Literal[\"fast\", \"medium\", \"slow_ok\"],\n",
    "    cost_sensitivity: Literal[\"high\", \"medium\", \"low\"] = \"medium\",\n",
    ") -> CompressionChoice:\n",
    "    \"\"\"\n",
    "    Decision framework for choosing compression strategy.\n",
    "\n",
    "    Args:\n",
    "        conversation_length: Number of messages in conversation\n",
    "        token_count: Total token count\n",
    "        quality_requirement: How important is quality? (\"high\", \"medium\", \"low\")\n",
    "        latency_requirement: How fast must it be? (\"fast\", \"medium\", \"slow_ok\")\n",
    "        cost_sensitivity: How sensitive to costs? (\"high\", \"medium\", \"low\")\n",
    "\n",
    "    Returns:\n",
    "        CompressionChoice: Recommended strategy\n",
    "    \"\"\"\n",
    "    # No compression needed for short conversations\n",
    "    if token_count < 2000 and conversation_length < 10:\n",
    "        return CompressionChoice.NONE\n",
    "\n",
    "    # Fast requirement = no LLM calls\n",
    "    if latency_requirement == \"fast\":\n",
    "        if quality_requirement == \"high\":\n",
    "            return CompressionChoice.PRIORITY\n",
    "        else:\n",
    "            return CompressionChoice.TRUNCATION\n",
    "\n",
    "    # High cost sensitivity = avoid LLM calls\n",
    "    if cost_sensitivity == \"high\":\n",
    "        return (\n",
    "            CompressionChoice.PRIORITY\n",
    "            if quality_requirement != \"low\"\n",
    "            else CompressionChoice.TRUNCATION\n",
    "        )\n",
    "\n",
    "    # High quality + willing to wait = summarization\n",
    "    if quality_requirement == \"high\" and latency_requirement == \"slow_ok\":\n",
    "        return CompressionChoice.SUMMARIZATION\n",
    "\n",
    "    # Long conversations benefit from summarization\n",
    "    if conversation_length > 30 and quality_requirement != \"low\":\n",
    "        return CompressionChoice.SUMMARIZATION\n",
    "\n",
    "    # Medium quality = priority-based\n",
    "    if quality_requirement == \"medium\":\n",
    "        return CompressionChoice.PRIORITY\n",
    "\n",
    "    # Default to truncation for simple cases\n",
    "    return CompressionChoice.TRUNCATION\n",
    "\n",
    "\n",
    "print(\"‚úÖ Decision framework function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6334d427d5d684f",
   "metadata": {},
   "source": [
    "### Demo 6: Test Decision Framework\n",
    "\n",
    "Let's test the decision framework with various scenarios.\n",
    "\n",
    "#### Step 1: Define test scenarios\n",
    "\n",
    "**What:** Creating 8 realistic scenarios with different requirements (quality, latency, cost).\n",
    "\n",
    "**Why:** Testing the decision framework across diverse use cases shows how it adapts recommendations based on constraints. Each scenario represents a real production situation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bd77fd3ecf192aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:30.721472Z",
     "iopub.status.busy": "2025-11-02T01:09:30.721383Z",
     "iopub.status.idle": "2025-11-02T01:09:30.723534Z",
     "shell.execute_reply": "2025-11-02T01:09:30.723157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define test scenarios\n",
    "scenarios = [\n",
    "    # (length, tokens, quality, latency, cost, description)\n",
    "    (5, 1000, \"high\", \"fast\", \"medium\", \"Short conversation, high quality needed\"),\n",
    "    (15, 3000, \"high\", \"slow_ok\", \"low\", \"Medium conversation, quality critical\"),\n",
    "    (30, 8000, \"medium\", \"medium\", \"medium\", \"Long conversation, balanced needs\"),\n",
    "    (50, 15000, \"high\", \"slow_ok\", \"medium\", \"Very long, quality important\"),\n",
    "    (100, 30000, \"low\", \"fast\", \"high\", \"Extremely long, cost-sensitive\"),\n",
    "    (20, 5000, \"medium\", \"fast\", \"high\", \"Medium length, fast and cheap\"),\n",
    "    (40, 12000, \"high\", \"medium\", \"low\", \"Long conversation, quality focus\"),\n",
    "    (8, 1500, \"low\", \"fast\", \"high\", \"Short, simple case\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e764e64120fc9",
   "metadata": {},
   "source": [
    "#### Step 2: Run the decision framework on each scenario\n",
    "\n",
    "**What:** Running the `choose_compression_strategy()` function on all 8 scenarios.\n",
    "\n",
    "**Why:** Demonstrates how the framework makes intelligent trade-offs - prioritizing quality when cost allows, choosing speed when latency matters, and balancing constraints when requirements conflict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d6df99d81af4f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:09:30.724703Z",
     "iopub.status.busy": "2025-11-02T01:09:30.724630Z",
     "iopub.status.idle": "2025-11-02T01:09:30.727115Z",
     "shell.execute_reply": "2025-11-02T01:09:30.726683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Framework Test Results:\n",
      "========================================================================================================================\n",
      "Scenario                                      Length   Tokens     Quality    Latency    Cost     Strategy\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Short conversation, high quality needed       5        1,000      high       fast       medium   none\n",
      "Medium conversation, quality critical         15       3,000      high       slow_ok    low      summarization\n",
      "Long conversation, balanced needs             30       8,000      medium     medium     medium   priority\n",
      "Very long, quality important                  50       15,000     high       slow_ok    medium   summarization\n",
      "Extremely long, cost-sensitive                100      30,000     low        fast       high     truncation\n",
      "Medium length, fast and cheap                 20       5,000      medium     fast       high     truncation\n",
      "Long conversation, quality focus              40       12,000     high       medium     low      summarization\n",
      "Short, simple case                            8        1,500      low        fast       high     none\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Framework Test Results:\")\n",
    "print(\"=\" * 120)\n",
    "print(\n",
    "    f\"{'Scenario':<45} {'Length':<8} {'Tokens':<10} {'Quality':<10} {'Latency':<10} {'Cost':<8} {'Strategy'}\"\n",
    ")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for length, tokens, quality, latency, cost, description in scenarios:\n",
    "    strategy = choose_compression_strategy(length, tokens, quality, latency, cost)\n",
    "    print(\n",
    "        f\"{description:<45} {length:<8} {tokens:<10,} {quality:<10} {latency:<10} {cost:<8} {strategy.value}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02d6d98eb9063d",
   "metadata": {},
   "source": [
    "#### Key Insights from the Decision Framework\n",
    "\n",
    "**Pattern 1: Quality drives strategy choice**\n",
    "- High quality + willing to wait ‚Üí Summarization\n",
    "- Medium quality ‚Üí Priority-based\n",
    "- Low quality ‚Üí Truncation\n",
    "\n",
    "**Pattern 2: Latency constraints matter**\n",
    "- Fast requirement ‚Üí Avoid summarization (no LLM calls)\n",
    "- Slow OK ‚Üí Summarization is an option\n",
    "\n",
    "**Pattern 3: Cost sensitivity affects decisions**\n",
    "- High cost sensitivity ‚Üí Avoid summarization\n",
    "- Low cost sensitivity ‚Üí Summarization is preferred for quality\n",
    "\n",
    "**Pattern 4: Conversation length influences choice**\n",
    "- Short (<10 messages) ‚Üí Often no compression needed\n",
    "- Long (>30 messages) ‚Üí Summarization recommended for quality\n",
    "\n",
    "**Practical Recommendation:**\n",
    "- Start with priority-based for most production use cases\n",
    "- Use summarization for high-value, long conversations\n",
    "- Use truncation for real-time, cost-sensitive scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893572f70d4176e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üè≠ Part 6: Production Recommendations\n",
    "\n",
    "Based on all the research and techniques we've covered, here are production-ready recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7e0bcdc28deb7",
   "metadata": {},
   "source": [
    "### Recommendation 1: For Most Applications (Balanced)\n",
    "\n",
    "**Strategy:** Agent Memory Server with automatic summarization\n",
    "\n",
    "**Configuration:**\n",
    "- `message_threshold`: 20 messages\n",
    "- `token_threshold`: 4000 tokens\n",
    "- `keep_recent`: 4 messages\n",
    "- `strategy`: \"recent_plus_summary\"\n",
    "\n",
    "**Why:** Automatic, transparent, production-ready. Implements research-backed strategies (Liu et al., Wang et al., Packer et al.) with minimal code.\n",
    "\n",
    "**Best for:** General-purpose chatbots, customer support, educational assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344c560b4d42889",
   "metadata": {},
   "source": [
    "### Recommendation 2: For High-Volume, Cost-Sensitive (Efficient)\n",
    "\n",
    "**Strategy:** Priority-based compression\n",
    "\n",
    "**Configuration:**\n",
    "- `max_tokens`: 2000\n",
    "- Custom importance scoring\n",
    "- No LLM calls\n",
    "\n",
    "**Why:** Fast, cheap, no external dependencies. Preserves important messages without LLM costs.\n",
    "\n",
    "**Best for:** High-traffic applications, real-time systems, cost-sensitive deployments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489db7cfc60769a",
   "metadata": {},
   "source": [
    "### Recommendation 3: For Critical Conversations (Quality)\n",
    "\n",
    "**Strategy:** Manual summarization with review\n",
    "\n",
    "**Configuration:**\n",
    "- `token_threshold`: 5000\n",
    "- Human review of summaries\n",
    "- Store full conversation separately\n",
    "\n",
    "**Why:** Maximum quality, human oversight. Critical for high-stakes conversations.\n",
    "\n",
    "**Best for:** Medical consultations, legal advice, financial planning, therapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3e70ff326b867",
   "metadata": {},
   "source": [
    "### Recommendation 4: For Real-Time Chat (Speed)\n",
    "\n",
    "**Strategy:** Truncation with sliding window\n",
    "\n",
    "**Configuration:**\n",
    "- `keep_recent`: 10 messages\n",
    "- No summarization\n",
    "- Fast response required\n",
    "\n",
    "**Why:** Minimal latency, simple implementation. Prioritizes speed over context preservation.\n",
    "\n",
    "**Best for:** Live chat, gaming, real-time collaboration tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516c43cb73d0441",
   "metadata": {},
   "source": [
    "### General Guidelines\n",
    "\n",
    "**Getting Started:**\n",
    "1. Start with Agent Memory Server automatic summarization\n",
    "2. Monitor token usage and costs in production\n",
    "3. Adjust thresholds based on your use case\n",
    "\n",
    "**Advanced Optimization:**\n",
    "4. Consider hybrid approaches (truncation + summarization)\n",
    "5. Always preserve critical information in long-term memory\n",
    "6. Use the decision framework to adapt to different conversation types\n",
    "\n",
    "**Monitoring:**\n",
    "7. Track compression ratios and token savings\n",
    "8. Monitor user satisfaction and conversation quality\n",
    "9. A/B test different strategies for your use case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa20b8bb77b5767c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí™ Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to reinforce your learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed098207acb2ac62",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Adaptive Compression Strategy\n",
    "\n",
    "Create a strategy that automatically chooses between truncation and sliding window based on message token variance:\n",
    "\n",
    "```python\n",
    "class AdaptiveStrategy(CompressionStrategy):\n",
    "    \"\"\"\n",
    "    Automatically choose between truncation and sliding window.\n",
    "\n",
    "    Logic:\n",
    "    - If messages have similar token counts ‚Üí use sliding window (predictable)\n",
    "    - If messages have varying token counts ‚Üí use truncation (token-aware)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int = 10):\n",
    "        self.window_size = window_size\n",
    "        self.truncation = TruncationStrategy()\n",
    "        self.sliding_window = SlidingWindowStrategy(window_size)\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"\n",
    "        Choose strategy based on token variance.\n",
    "\n",
    "        Steps:\n",
    "        1. Calculate token count variance across messages\n",
    "        2. If variance is low (similar sizes) ‚Üí use sliding window\n",
    "        3. If variance is high (varying sizes) ‚Üí use truncation\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "adaptive = AdaptiveStrategy(window_size=6)\n",
    "result = adaptive.compress(sample_conversation, max_tokens=800)\n",
    "print(f\"Adaptive strategy result: {len(result)} messages\")\n",
    "```\n",
    "\n",
    "**Hint:** Calculate variance using `statistics.variance([msg.token_count for msg in messages])`. Use a threshold (e.g., 100) to decide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a03030232b3364",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Hybrid Compression\n",
    "\n",
    "Combine summarization + truncation for optimal results:\n",
    "\n",
    "```python\n",
    "async def compress_hybrid(\n",
    "    messages: List[ConversationMessage],\n",
    "    summarizer: ConversationSummarizer,\n",
    "    max_tokens: int = 2000\n",
    ") -> List[ConversationMessage]:\n",
    "    \"\"\"\n",
    "    Hybrid compression: Summarize old messages, truncate if still too large.\n",
    "\n",
    "    Steps:\n",
    "    1. First, try summarization\n",
    "    2. If still over budget, apply truncation to summary + recent messages\n",
    "    3. Ensure we stay within max_tokens\n",
    "\n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        summarizer: ConversationSummarizer instance\n",
    "        max_tokens: Maximum token budget\n",
    "\n",
    "    Returns:\n",
    "        Compressed messages within token budget\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "hybrid_result = await compress_hybrid(sample_conversation, summarizer, max_tokens=1000)\n",
    "print(f\"Hybrid compression: {len(hybrid_result)} messages, {sum(m.token_count for m in hybrid_result)} tokens\")\n",
    "```\n",
    "\n",
    "**Hint:** Use `summarizer.compress_conversation()` first, then apply truncation if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac899a501122c38",
   "metadata": {},
   "source": [
    "### Exercise 3: Quality Comparison\n",
    "\n",
    "Test all compression strategies and compare quality:\n",
    "\n",
    "```python\n",
    "async def compare_compression_quality(\n",
    "    messages: List[ConversationMessage],\n",
    "    test_query: str = \"What courses did we discuss?\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare compression strategies by testing reference resolution.\n",
    "\n",
    "    Steps:\n",
    "    1. Compress using each strategy\n",
    "    2. Try to answer test_query using compressed context\n",
    "    3. Compare quality of responses\n",
    "    4. Measure token savings\n",
    "\n",
    "    Args:\n",
    "        messages: Original conversation\n",
    "        test_query: Question to test reference resolution\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    # Test if the agent can still answer questions after compression\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "quality_results = await compare_compression_quality(sample_conversation)\n",
    "print(\"Quality Comparison Results:\")\n",
    "for strategy, results in quality_results.items():\n",
    "    print(f\"{strategy}: {results}\")\n",
    "```\n",
    "\n",
    "**Hint:** Use the LLM to answer the test query with each compressed context and compare responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134bf5336e3ae36",
   "metadata": {},
   "source": [
    "### Exercise 4: Custom Importance Scoring\n",
    "\n",
    "Improve the `calculate_importance()` function with domain-specific logic:\n",
    "\n",
    "```python\n",
    "def calculate_importance_enhanced(msg: ConversationMessage) -> float:\n",
    "    \"\"\"\n",
    "    Enhanced importance scoring for course advisor conversations.\n",
    "\n",
    "    Add scoring for:\n",
    "    - Specific course codes (CS401, MATH301, etc.) - HIGH\n",
    "    - Prerequisites and requirements - HIGH\n",
    "    - Student preferences and goals - HIGH\n",
    "    - Questions - MEDIUM\n",
    "    - Confirmations and acknowledgments - LOW\n",
    "    - Greetings and small talk - VERY LOW\n",
    "\n",
    "    Returns:\n",
    "        Importance score (0.0 to 5.0)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "for msg in sample_conversation[:5]:\n",
    "    score = calculate_importance_enhanced(msg)\n",
    "    print(f\"Score: {score:.1f} - {msg.content[:60]}...\")\n",
    "```\n",
    "\n",
    "**Hint:** Use regex to detect course codes, check for question marks, look for keywords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960cb21dcfe638cf",
   "metadata": {},
   "source": [
    "### Exercise 5: Production Configuration\n",
    "\n",
    "Configure Agent Memory Server for your specific use case:\n",
    "\n",
    "```python\n",
    "# Scenario: High-volume customer support chatbot\n",
    "# Requirements:\n",
    "# - Handle 1000+ conversations per day\n",
    "# - Average conversation: 15-20 turns\n",
    "# - Cost-sensitive but quality important\n",
    "# - Response time: <2 seconds\n",
    "\n",
    "# Your task: Choose appropriate configuration\n",
    "production_config = {\n",
    "    \"message_threshold\": ???,  # When to trigger summarization\n",
    "    \"token_threshold\": ???,    # Token limit before summarization\n",
    "    \"keep_recent\": ???,        # How many recent messages to keep\n",
    "    \"strategy\": ???,           # Which strategy to use\n",
    "}\n",
    "\n",
    "# Justify your choices:\n",
    "print(\"Configuration Justification:\")\n",
    "print(f\"message_threshold: {production_config['message_threshold']} because...\")\n",
    "print(f\"token_threshold: {production_config['token_threshold']} because...\")\n",
    "print(f\"keep_recent: {production_config['keep_recent']} because...\")\n",
    "print(f\"strategy: {production_config['strategy']} because...\")\n",
    "```\n",
    "\n",
    "**Hint:** Consider the trade-offs between cost, quality, and latency for this specific scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184f7251934a320",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### **What You Learned:**\n",
    "\n",
    "1. ‚úÖ **Research Foundations**\n",
    "   - \"Lost in the Middle\" (Liu et al., 2023): U-shaped performance, non-uniform degradation\n",
    "   - \"Recursive Summarization\" (Wang et al., 2023): Long-term dialogue memory\n",
    "   - \"MemGPT\" (Packer et al., 2023): Hierarchical memory management\n",
    "   - Production best practices from Anthropic and Vellum AI\n",
    "\n",
    "2. ‚úÖ **The Long Conversation Problem**\n",
    "   - Token limits, cost implications, performance degradation\n",
    "   - Why unbounded growth is unsustainable\n",
    "   - Quadratic cost growth without management\n",
    "   - Why larger context windows don't solve the problem\n",
    "\n",
    "3. ‚úÖ **Conversation Summarization**\n",
    "   - What to preserve vs. compress\n",
    "   - When to trigger summarization (token/message thresholds)\n",
    "   - Building summarization step-by-step (functions ‚Üí class)\n",
    "   - LLM-based intelligent summarization\n",
    "\n",
    "4. ‚úÖ **Three Compression Strategies**\n",
    "   - **Truncation:** Fast, simple, loses context\n",
    "   - **Priority-based:** Balanced, intelligent, no LLM calls\n",
    "   - **Summarization:** High quality, preserves meaning, requires LLM\n",
    "   - Trade-offs between speed, quality, and cost\n",
    "\n",
    "5. ‚úÖ **Agent Memory Server Integration**\n",
    "   - Automatic summarization configuration\n",
    "   - Transparent memory management\n",
    "   - Production-ready solution implementing research findings\n",
    "   - Configurable thresholds and strategies\n",
    "\n",
    "6. ‚úÖ **Decision Framework**\n",
    "   - How to choose the right strategy\n",
    "   - Factors: quality, latency, cost, conversation length\n",
    "   - Production recommendations for different scenarios\n",
    "   - Hybrid approaches for optimal results\n",
    "\n",
    "### **What You Built:**\n",
    "\n",
    "- ‚úÖ `ConversationSummarizer` class for intelligent summarization\n",
    "- ‚úÖ Three compression strategy implementations (Truncation, Priority, Summarization)\n",
    "- ‚úÖ Decision framework for strategy selection\n",
    "- ‚úÖ Production configuration examples\n",
    "- ‚úÖ Comparison tools for evaluating strategies\n",
    "- ‚úÖ Token counting and cost analysis tools\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "üí° **\"Conversations grow unbounded without management\"**\n",
    "- Every turn adds tokens and cost\n",
    "- Eventually you'll hit limits\n",
    "- Costs grow quadratically (each turn includes all previous messages)\n",
    "\n",
    "üí° **\"Summarization preserves meaning while reducing tokens\"**\n",
    "- Use LLM to create intelligent summaries\n",
    "- Keep recent messages for immediate context\n",
    "- Store important facts in long-term memory\n",
    "\n",
    "üí° **\"Choose strategy based on requirements\"**\n",
    "- Quality-critical ‚Üí Summarization\n",
    "- Speed-critical ‚Üí Truncation or Priority-based\n",
    "- Balanced ‚Üí Agent Memory Server automatic\n",
    "- Cost-sensitive ‚Üí Priority-based\n",
    "\n",
    "üí° **\"Agent Memory Server handles this automatically\"**\n",
    "- Production-ready solution\n",
    "- Transparent to your application\n",
    "- Configurable for your needs\n",
    "- No manual intervention required\n",
    "\n",
    "### **Connection to Context Engineering:**\n",
    "\n",
    "This notebook completes the **Conversation Context** story from Section 1:\n",
    "\n",
    "1. **Section 1:** Introduced the 4 context types, including Conversation Context\n",
    "2. **Section 3, NB1:** Implemented working memory for conversation continuity\n",
    "3. **Section 3, NB2:** Integrated memory with RAG for stateful conversations\n",
    "4. **Section 3, NB3:** Managed long conversations with summarization and compression ‚Üê You are here\n",
    "\n",
    "**Next:** Section 4 will show how agents can actively manage their own memory using tools!\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "**Section 4: Tools and Agents**\n",
    "- Build agents that actively manage their own memory\n",
    "- Implement memory tools (store, search, retrieve)\n",
    "- Use LangGraph for agent workflows\n",
    "- Let the LLM decide when to summarize\n",
    "\n",
    "**Section 5: Production Optimization**\n",
    "- Performance measurement and monitoring\n",
    "- Hybrid retrieval strategies\n",
    "- Semantic tool selection\n",
    "- Quality assurance and validation\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Resources\n",
    "\n",
    "### **Documentation:**\n",
    "- [Agent Memory Server](https://github.com/redis/agent-memory-server) - Production memory management\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client library\n",
    "- [LangChain Memory](https://python.langchain.com/docs/modules/memory/) - Memory patterns\n",
    "- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Token counting tool\n",
    "- [tiktoken](https://github.com/openai/tiktoken) - Fast token counting library\n",
    "\n",
    "### **Research Papers:**\n",
    "- **[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)** - Liu et al. (2023). Shows U-shaped performance curve and non-uniform degradation in long contexts.\n",
    "- **[Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022)** - Wang et al. (2023). Demonstrates recursive summarization for long conversations.\n",
    "- **[MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)** - Packer et al. (2023). Introduces hierarchical memory management and virtual context.\n",
    "- [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401) - RAG fundamentals\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer architecture and context windows\n",
    "\n",
    "### **Industry Resources:**\n",
    "- **[How Should I Manage Memory for my LLM Chatbot?](https://www.vellum.ai/blog/how-should-i-manage-memory-for-my-llm-chatbot)** - Vellum AI. Practical insights on memory management trade-offs.\n",
    "- **[Lost in the Middle Paper Reading](https://arize.com/blog/lost-in-the-middle-how-language-models-use-long-contexts-paper-reading/)** - Arize AI. Detailed analysis and practical implications.\n",
    "- **[Effective Context Engineering for AI Agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)** - Anthropic. Production best practices.\n",
    "\n",
    "\n",
    "### **Tools and Libraries:**\n",
    "- **Redis:** Vector storage and memory backend\n",
    "- **Agent Memory Server:** Dual-memory architecture with automatic summarization\n",
    "- **LangChain:** LLM interaction framework\n",
    "- **LangGraph:** State management and agent workflows\n",
    "- **OpenAI:** GPT-4o for generation and summarization\n",
    "- **tiktoken:** Token counting for cost estimation\n",
    "\n",
    "---\n",
    "\n",
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "**Redis University - Context Engineering Course**\n",
    "\n",
    "**üéâ Congratulations!** You've completed Section 3: Memory Architecture!\n",
    "\n",
    "You now understand how to:\n",
    "- Build memory systems for AI agents\n",
    "- Integrate working and long-term memory\n",
    "- Manage long conversations with summarization\n",
    "- Choose the right compression strategy\n",
    "- Configure production-ready memory management\n",
    "\n",
    "**Ready for Section 4?** Let's build agents that actively manage their own memory using tools!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37206838f616911a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a1b7fa18aae7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d06c497fe3df20b",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# üß† Section 3, Notebook 3: Memory Management - Handling Long Conversations\n",
    "\n",
    "**‚è±Ô∏è Estimated Time:** 50-60 minutes\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand** why long conversations need management (token limits, cost, performance)\n",
    "2. **Implement** conversation summarization to preserve key information\n",
    "3. **Build** context compression strategies (truncation, priority-based, summarization)\n",
    "4. **Configure** automatic memory management with Agent Memory Server\n",
    "5. **Decide** when to apply each technique based on conversation characteristics\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where We Are\n",
    "\n",
    "### **Your Journey So Far:**\n",
    "\n",
    "**Section 3, Notebook 1:** Memory Fundamentals\n",
    "- ‚úÖ Working memory for conversation continuity\n",
    "- ‚úÖ Long-term memory for persistent knowledge\n",
    "- ‚úÖ The grounding problem and reference resolution\n",
    "- ‚úÖ Memory types (semantic, episodic, message)\n",
    "\n",
    "**Section 3, Notebook 2:** Memory-Enhanced RAG\n",
    "- ‚úÖ Integrated all four context types\n",
    "- ‚úÖ Built complete memory-enhanced RAG system\n",
    "- ‚úÖ Demonstrated benefits of stateful conversations\n",
    "\n",
    "**Your memory system works!** It can:\n",
    "- Remember conversation history across turns\n",
    "- Store and retrieve long-term facts\n",
    "- Resolve references (\"it\", \"that course\")\n",
    "- Provide personalized recommendations\n",
    "\n",
    "### **But... What About Long Conversations?**\n",
    "\n",
    "**Questions we can't answer yet:**\n",
    "- ‚ùì What happens when conversations get really long?\n",
    "- ‚ùì How do we handle token limits?\n",
    "- ‚ùì How much does a 50-turn conversation cost?\n",
    "- ‚ùì Can we preserve important context while reducing tokens?\n",
    "- ‚ùì When should we summarize vs. truncate vs. keep everything?\n",
    "\n",
    "---\n",
    "\n",
    "## üö® The Long Conversation Problem\n",
    "\n",
    "Before diving into solutions, let's understand the fundamental problem.\n",
    "\n",
    "### **The Problem: Unbounded Growth**\n",
    "\n",
    "Every conversation turn adds messages to working memory:\n",
    "\n",
    "```\n",
    "Turn 1:  System (500) + Messages (200) = 700 tokens ‚úÖ\n",
    "Turn 5:  System (500) + Messages (1,000) = 1,500 tokens ‚úÖ\n",
    "Turn 20: System (500) + Messages (4,000) = 4,500 tokens ‚úÖ\n",
    "Turn 50: System (500) + Messages (10,000) = 10,500 tokens ‚ö†Ô∏è\n",
    "Turn 100: System (500) + Messages (20,000) = 20,500 tokens ‚ö†Ô∏è\n",
    "Turn 200: System (500) + Messages (40,000) = 40,500 tokens ‚ùå\n",
    "```\n",
    "\n",
    "**Without management, conversations grow unbounded!**\n",
    "\n",
    "### **Why This Matters**\n",
    "\n",
    "**1. Token Limits (Hard Constraint)**\n",
    "- GPT-4o: 128K tokens (~96,000 words)\n",
    "- GPT-3.5: 16K tokens (~12,000 words)\n",
    "- Eventually, you'll hit the limit and conversations fail\n",
    "\n",
    "**2. Cost (Economic Constraint)**\n",
    "- Input tokens cost money  (e.g. $0.0025 /  1K  tokens for GPT-4o)\n",
    "\n",
    "- A 50-turn conversation = ~10,000 tokens = $0.025 per query\n",
    "\n",
    "- Over 1,000 conversations = $25 just for conversation history!\n",
    "\n",
    "**3. Performance (Quality Constraint)**\n",
    "- More tokens = longer processing time\n",
    "- Context Rot: LLMs struggle with very long contexts\n",
    "- Important information gets \"lost in the middle\"\n",
    "\n",
    "**4. User Experience**\n",
    "- Slow responses frustrate users\n",
    "- Expensive conversations aren't sustainable\n",
    "- Failed conversations due to token limits are unacceptable\n",
    "\n",
    "### **The Solution: Memory Management**\n",
    "\n",
    "We need strategies to:\n",
    "- ‚úÖ Keep conversations within token budgets\n",
    "- ‚úÖ Preserve important information\n",
    "- ‚úÖ Maintain conversation quality\n",
    "- ‚úÖ Control costs\n",
    "- ‚úÖ Enable indefinite conversations\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Part 0: Setup and Environment\n",
    "\n",
    "Let's set up our environment and create tools for measuring conversation growth.\n",
    "\n",
    "### ‚ö†Ô∏è Prerequisites\n",
    "\n",
    "**Before running this notebook, make sure you have:**\n",
    "\n",
    "1. **Docker Desktop running** - Required for Redis and Agent Memory Server\n",
    "\n",
    "2. **Environment variables** - Create a `.env` file in the `reference-agent` directory:\n",
    "   ```bash\n",
    "   # Copy the example file\n",
    "   cd ../../reference-agent\n",
    "   cp .env.example .env\n",
    "\n",
    "   # Edit .env and add your OpenAI API key\n",
    "   # OPENAI_API_KEY=your_actual_openai_api_key_here\n",
    "   ```\n",
    "\n",
    "3. **Run the setup script** - This will automatically start Redis and Agent Memory Server:\n",
    "   ```bash\n",
    "   cd ../../reference-agent\n",
    "   python setup_agent_memory_server.py\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c59ecc51d30c3",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10e48e57f1431e",
   "metadata": {},
   "source": [
    "### Automated Setup Check\n",
    "\n",
    "Let's run the setup script to ensure all services are running properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808cea2af3f4f118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running automated setup check...\n",
      "\n",
      "\n",
      "üîß Agent Memory Server Setup\n",
      "===========================\n",
      "üìä Checking Redis...\n",
      "‚úÖ Redis is running\n",
      "üìä Checking Agent Memory Server...\n",
      "üîç Agent Memory Server container exists. Checking health...\n",
      "‚úÖ Agent Memory Server is running and healthy\n",
      "‚úÖ No Redis connection issues detected\n",
      "\n",
      "‚úÖ Setup Complete!\n",
      "=================\n",
      "üìä Services Status:\n",
      "   ‚Ä¢ Redis: Running on port 6379\n",
      "   ‚Ä¢ Agent Memory Server: Running on port 8088\n",
      "\n",
      "üéØ You can now run the notebooks!\n",
      "\n",
      "\n",
      "‚úÖ All services are ready!\n"
     ]
    }
   ],
   "source": [
    "# Run the setup script to ensure Redis and Agent Memory Server are running\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to setup script\n",
    "setup_script = Path(\"../../reference-agent/setup_agent_memory_server.py\")\n",
    "\n",
    "if setup_script.exists():\n",
    "    print(\"Running automated setup check...\\n\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(setup_script)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"‚ö†Ô∏è  Setup check failed. Please review the output above.\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All services are ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Setup script not found. Please ensure services are running manually.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ab2a448dd08fc",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8400bfed20f64",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already installed the reference-agent package, uncomment and run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ad9f5d109351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install reference-agent package\n",
    "# %pip install -q -e ../../reference-agent\n",
    "\n",
    "# Uncomment to install agent-memory-client\n",
    "# %pip install -q agent-memory-client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bf6b02f73fdb9",
   "metadata": {},
   "source": [
    "### Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00247fc4bb718d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Redis and Agent Memory\n",
    "from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "from agent_memory_client.models import WorkingMemory, MemoryMessage, ClientMemoryRecord\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "# For visualization\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38946d91e830639a",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a3192aacee6dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables configured\n",
      "   Redis URL: redis://localhost:6379\n",
      "   Agent Memory URL: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from reference-agent directory\n",
    "env_path = Path(\"../../reference-agent/.env\")\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(f\"\"\"‚ùå OPENAI_API_KEY not found!\n",
    "\n",
    "Please create a .env file at: {env_path.absolute()}\n",
    "\n",
    "With the following content:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "REDIS_URL=redis://localhost:6379\n",
    "AGENT_MEMORY_URL=http://localhost:8088\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"‚úÖ Environment variables configured\")\n",
    "    print(f\"   Redis URL: {REDIS_URL}\")\n",
    "    print(f\"   Agent Memory URL: {AGENT_MEMORY_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42157025d92c5",
   "metadata": {},
   "source": [
    "### Initialize Clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6acdabe9f826582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clients initialized\n",
      "   LLM: gpt-4o\n",
      "   Embeddings: text-embedding-3-small\n",
      "   Memory Server: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Initialize Agent Memory Client\n",
    "memory_config = MemoryClientConfig(base_url=AGENT_MEMORY_URL)\n",
    "memory_client = MemoryAPIClient(config=memory_config)\n",
    "\n",
    "# Initialize tokenizer for counting\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "print(\"‚úÖ Clients initialized\")\n",
    "print(f\"   LLM: {llm.model_name}\")\n",
    "print(f\"   Embeddings: text-embedding-3-small\")\n",
    "print(f\"   Memory Server: {AGENT_MEMORY_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c6e2d8cee7f21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Understanding Conversation Growth\n",
    "\n",
    "Let's visualize how conversations grow and understand the implications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4a48ea4fee96b",
   "metadata": {},
   "source": [
    "### üî¨ Research Context: Why Context Management Matters\n",
    "\n",
    "Modern LLMs have impressive context windows:\n",
    "- **GPT-4o**: 128K tokens (~96,000 words)\n",
    "- **Claude 3.5**: 200K tokens (~150,000 words)\n",
    "- **Gemini 1.5 Pro**: 1M tokens (~750,000 words)\n",
    "\n",
    "**But here's the problem:** Larger context windows don't guarantee better performance.\n",
    "\n",
    "#### The \"Lost in the Middle\" Problem\n",
    "\n",
    "Research by Liu et al. (2023) in their paper [\"Lost in the Middle: How Language Models Use Long Contexts\"](https://arxiv.org/abs/2307.03172) revealed critical findings:\n",
    "\n",
    "**Key Finding #1: U-Shaped Performance**\n",
    "- Models perform best when relevant information is at the **beginning** or **end** of context\n",
    "- Performance **significantly degrades** when information is in the **middle** of long contexts\n",
    "- This happens even with models explicitly designed for long contexts\n",
    "\n",
    "**Key Finding #2: Non-Uniform Degradation**\n",
    "- It's not just about hitting token limits\n",
    "- Quality degrades **even within the context window**\n",
    "- The longer the context, the worse the \"middle\" performance becomes\n",
    "\n",
    "**Key Finding #3: More Context ‚â† Better Results**\n",
    "- In some experiments, GPT-3.5 performed **worse** with retrieved documents than with no documents at all\n",
    "- Adding more context can actually **hurt** performance if not managed properly\n",
    "\n",
    "**Why This Matters for Memory Management:**\n",
    "- Simply storing all conversation history isn't optimal\n",
    "- We need **intelligent compression** to keep important information accessible\n",
    "- **Position matters**: Recent context (at the end) is naturally well-positioned\n",
    "- **Quality over quantity**: Better to have concise, relevant context than exhaustive history\n",
    "\n",
    "**References:**\n",
    "- Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). Lost in the Middle: How Language Models Use Long Contexts. *Transactions of the Association for Computational Linguistics (TACL)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7e262cad76878",
   "metadata": {},
   "source": [
    "### Demo 1: Token Growth Over Time\n",
    "\n",
    "Now let's see this problem in action by simulating conversation growth.\n",
    "\n",
    "#### Step 1: Define our system prompt and count its tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99edd1b0325093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: 31 tokens\n"
     ]
    }
   ],
   "source": [
    "# System prompt (constant across all turns)\n",
    "system_prompt = \"\"\"You are a helpful course advisor for Redis University.\n",
    "Help students find courses, check prerequisites, and plan their schedule.\n",
    "Be friendly, concise, and accurate.\"\"\"\n",
    "\n",
    "system_tokens = count_tokens(system_prompt)\n",
    "\n",
    "print(f\"System prompt: {system_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e0cfece6beaf5",
   "metadata": {},
   "source": [
    "#### Step 2: Simulate how tokens grow with each conversation turn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "117ca757272caef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation Growth Simulation:\n",
      "================================================================================\n",
      "Turn     Messages   Conv Tokens     Total Tokens    Cost ($)    \n",
      "--------------------------------------------------------------------------------\n",
      "1        2          100             131             $0.0003      ‚úÖ\n",
      "5        10         500             531             $0.0013      ‚úÖ\n",
      "10       20         1,000           1,031           $0.0026      ‚úÖ\n",
      "20       40         2,000           2,031           $0.0051      ‚úÖ\n",
      "30       60         3,000           3,031           $0.0076      ‚úÖ\n",
      "50       100        5,000           5,031           $0.0126      ‚ö†Ô∏è\n",
      "75       150        7,500           7,531           $0.0188      ‚ö†Ô∏è\n",
      "100      200        10,000          10,031          $0.0251      ‚ö†Ô∏è\n",
      "150      300        15,000          15,031          $0.0376      ‚ö†Ô∏è\n",
      "200      400        20,000          20,031          $0.0501      ‚ùå\n"
     ]
    }
   ],
   "source": [
    "# Assume average message pair (user + assistant) = 100 tokens\n",
    "avg_message_pair_tokens = 100\n",
    "\n",
    "print(\"\\nConversation Growth Simulation:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Turn':<8} {'Messages':<10} {'Conv Tokens':<15} {'Total Tokens':<15} {'Cost ($)':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for turn in [1, 5, 10, 20, 30, 50, 75, 100, 150, 200]:\n",
    "    # Each turn = user message + assistant message\n",
    "    num_messages = turn * 2\n",
    "    conversation_tokens = num_messages * (avg_message_pair_tokens // 2)\n",
    "    total_tokens = system_tokens + conversation_tokens\n",
    "\n",
    "    # Cost calculation (GPT-4o input: $0.0025 per 1K tokens)\n",
    "    cost_per_query = (total_tokens / 1000) * 0.0025\n",
    "\n",
    "    # Visual indicator\n",
    "    if total_tokens < 5000:\n",
    "        indicator = \"‚úÖ\"\n",
    "    elif total_tokens < 20000:\n",
    "        indicator = \"‚ö†Ô∏è\"\n",
    "    else:\n",
    "        indicator = \"‚ùå\"\n",
    "\n",
    "    print(f\"{turn:<8} {num_messages:<10} {conversation_tokens:<15,} {total_tokens:<15,} ${cost_per_query:<11.4f} {indicator}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c9c59a8e344be",
   "metadata": {},
   "source": [
    "### Demo 2: Cost Analysis\n",
    "\n",
    "Let's calculate the cumulative cost of long conversations.\n",
    "\n",
    "**Why costs grow quadratically:**\n",
    "- Turn 1: Process 100 tokens\n",
    "- Turn 2: Process 200 tokens (includes turn 1)\n",
    "- Turn 3: Process 300 tokens (includes turns 1 & 2)\n",
    "- Turn N: Process N√ó100 tokens\n",
    "\n",
    "Total cost = 100 + 200 + 300 + ... + N√ó100 = **O(N¬≤)** growth!\n",
    "\n",
    "#### Step 1: Create a function to calculate conversation costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998184e76d362bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cost calculation function defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_conversation_cost(num_turns: int, avg_tokens_per_turn: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate cost metrics for a conversation.\n",
    "\n",
    "    Args:\n",
    "        num_turns: Number of conversation turns\n",
    "        avg_tokens_per_turn: Average tokens per turn (user + assistant)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with cost metrics\n",
    "    \"\"\"\n",
    "    system_tokens = 50  # Simplified\n",
    "\n",
    "    # Cumulative cost (each turn includes all previous messages)\n",
    "    cumulative_tokens = 0\n",
    "    cumulative_cost = 0.0\n",
    "\n",
    "    for turn in range(1, num_turns + 1):\n",
    "        # Total tokens for this turn\n",
    "        conversation_tokens = turn * avg_tokens_per_turn\n",
    "        total_tokens = system_tokens + conversation_tokens\n",
    "\n",
    "        # Cost for this turn (input tokens)\n",
    "        turn_cost = (total_tokens / 1000) * 0.0025\n",
    "        cumulative_cost += turn_cost\n",
    "        cumulative_tokens += total_tokens\n",
    "\n",
    "    return {\n",
    "        \"num_turns\": num_turns,\n",
    "        \"final_tokens\": system_tokens + (num_turns * avg_tokens_per_turn),\n",
    "        \"cumulative_tokens\": cumulative_tokens,\n",
    "        \"cumulative_cost\": cumulative_cost,\n",
    "        \"avg_cost_per_turn\": cumulative_cost / num_turns\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Cost calculation function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710bd8b0268c34d",
   "metadata": {},
   "source": [
    "#### Step 2: Compare costs across different conversation lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4441a3298bd38af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Analysis for Different Conversation Lengths:\n",
      "================================================================================\n",
      "Turns      Final Tokens    Cumulative Tokens    Total Cost      Avg/Turn\n",
      "--------------------------------------------------------------------------------\n",
      "10         1,050           6,000                $0.02           $0.0015\n",
      "25         2,550           33,750               $0.08           $0.0034\n",
      "50         5,050           130,000              $0.33           $0.0065\n",
      "100        10,050          510,000              $1.27           $0.0127\n",
      "200        20,050          2,020,000            $5.05           $0.0253\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost Analysis for Different Conversation Lengths:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Turns':<10} {'Final Tokens':<15} {'Cumulative Tokens':<20} {'Total Cost':<15} {'Avg/Turn'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for num_turns in [10, 25, 50, 100, 200]:\n",
    "    metrics = calculate_conversation_cost(num_turns)\n",
    "    print(f\"{metrics['num_turns']:<10} \"\n",
    "          f\"{metrics['final_tokens']:<15,} \"\n",
    "          f\"{metrics['cumulative_tokens']:<20,} \"\n",
    "          f\"${metrics['cumulative_cost']:<14.2f} \"\n",
    "          f\"${metrics['avg_cost_per_turn']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5840eedf4a9185",
   "metadata": {},
   "source": [
    "#### Key Takeaways\n",
    "\n",
    "**Without memory management:**\n",
    "- Costs grow **quadratically** (O(N¬≤))\n",
    "  \n",
    "- A 100-turn conversation costs ~$1.50 in total\n",
    "\n",
    "  \n",
    "- A 200-turn conversation costs ~$6.00 in total\n",
    "\n",
    "- At scale (1000s of users), this becomes unsustainable\n",
    "\n",
    "**The solution:** Intelligent memory management to keep conversations within budget while preserving quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f1c4414f6d2a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 2: Context Summarizaton\n",
    "\n",
    "**Context summarization** is the process of condensing conversation history into a compact representation that preserves essential information while dramatically reducing token count.\n",
    "\n",
    "Picture a chat assistant helping someone plan a wedding over 50 messages:\n",
    "- It captures the critical stuff: venue choice, budget, guest count, vendor decisions\n",
    "- It grabs the decisions and ditches the small talk\n",
    "- Later messages can reference \"the venue we picked\" without replaying the entire debate\n",
    "  \n",
    "**Same deal with LLM chats:**\n",
    "- Squash ancient messages into a tight little paragraph\n",
    "- Keep the gold (facts, choices, what the user loves/hates)\n",
    "- Leave fresh messages untouched (they're still doing work)\n",
    "- Slash token usage by 50-80% without lobotomizing the conversation\n",
    "\n",
    "### Why Should You Care About Summarization?\n",
    "\n",
    "Summarization tackles three gnarly problems:\n",
    "\n",
    "**1. Plays Nice With Token Caps (Callback to Part 1)**\n",
    "- Chats balloon up forever if you let them\n",
    "- Summarization keeps you from hitting the ceiling\n",
    "- **Real talk:** 50 messages (10K tokens) ‚Üí Compressed summary + 4 fresh messages (2.5K tokens)\n",
    "\n",
    "**2. Fixes the Context Rot Problem (Also From Part 1)**\n",
    "- Remember that \"Lost in the Middle\" mess? Old info gets buried and ignored\n",
    "- Summarization yanks that old stuff to the front in condensed form\n",
    "- Fresh messages chill at the end (where the model actually pays attention)\n",
    "- **Upshot:** Model performs better AND you save space‚Äîwin-win\n",
    "\n",
    "**3. Keeps Working Memory From Exploding (Throwback to Notebook 1)**\n",
    "- Working memory = your conversation backlog\n",
    "- Without summarization, it just keeps growing like a digital hoarder's closet\n",
    "- Summarization gives it a haircut regularly\n",
    "- **Payoff:** Conversations that can actually go the distance\n",
    "\n",
    "### When Should You Reach for This Tool?\n",
    "\n",
    "**Great for:**\n",
    "- ‚úÖ Marathon conversations (10+ back-and-forths)\n",
    "- ‚úÖ Chats that have a narrative arc (customer support, coaching sessions)\n",
    "- ‚úÖ Situations where you want history but not ALL the history\n",
    "- ‚úÖ When the recent stuff matters most\n",
    "\n",
    "**Skip it when:**\n",
    "- ‚ùå Quick exchanges (under 5 turns‚Äîdon't overthink it)\n",
    "- ‚ùå Every syllable counts (legal docs, medical consultations)\n",
    "- ‚ùå You might need verbatim quotes from way back\n",
    "- ‚ùå The extra LLM call for summarization costs too much time or money\n",
    "\n",
    "### Where Summarization Lives in Your Memory Stack\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                  Your LLM Agent Brain                   ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Context Window (128K tokens available)                 ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. System Prompt (500 tokens)                  ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. Long-term Memory Bank (1,000 tokens)        ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ 3. RAG Retrieval Stuff (2,000 tokens)          ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ 4. Working Memory Zone:                        ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ [COMPRESSED HISTORY] (500 tokens)    ‚îÇ    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ - Critical facts from rounds 1-20    ‚îÇ    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ - Decisions that were locked in      ‚îÇ    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ - User quirks and preferences        ‚îÇ    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Live Recent Messages (1,000 tokens)         ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round 21: User shot + Assistant reply     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round 22: User shot + Assistant reply     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round 23: User shot + Assistant reply     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round 24: User shot + Assistant reply     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ 5. Current Incoming Query (200 tokens)         ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Running total: ~5,200 tokens (instead of 15K‚Äînice!)    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "#### The Bottom Line: \n",
    "Summarization is a *compression technique* for working memory that maintains conversation continuity while keeping token counts manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a9c3a31a589d0",
   "metadata": {},
   "source": [
    "### üî¨ Research Foundation: Recursive Summarization\n",
    "\n",
    "Wang et al. (2023) in [\"Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models\"](https://arxiv.org/abs/2308.15022) demonstrated that:\n",
    "\n",
    "**Key Insight:** Recursive summarization enables LLMs to handle extremely long conversations by:\n",
    "1. Memorizing small dialogue contexts\n",
    "2. Recursively producing new memory using previous memory + new contexts\n",
    "3. Maintaining consistency across long conversations\n",
    "\n",
    "**Their findings:**\n",
    "- Improved response consistency in long-context conversations\n",
    "- Works well with both long-context models (8K, 16K) and retrieval-enhanced LLMs\n",
    "- Provides a practical solution for modeling extremely long contexts\n",
    "\n",
    "**Practical Application:**\n",
    "- Summarize old messages while keeping recent ones intact\n",
    "- Preserve key information (facts, decisions, preferences)\n",
    "- Compress redundant or less important information\n",
    "\n",
    "**References:**\n",
    "- Wang, Q., Fu, Y., Cao, Y., Wang, S., Tian, Z., & Ding, L. (2023). Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. *Neurocomputing* (Accepted).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbd6185d7e1fd4",
   "metadata": {},
   "source": [
    "### Theory: What to Preserve vs. Compress\n",
    "\n",
    "When summarizing conversations, we need to be strategic about what to keep and what to compress.\n",
    "\n",
    "**What to Preserve:**\n",
    "- ‚úÖ Key facts and decisions\n",
    "- ‚úÖ Student preferences and goals\n",
    "- ‚úÖ Important course recommendations\n",
    "- ‚úÖ Prerequisites and requirements\n",
    "- ‚úÖ Recent context (last few messages)\n",
    "\n",
    "**What to Compress:**\n",
    "- üì¶ Small talk and greetings\n",
    "- üì¶ Redundant information\n",
    "- üì¶ Old conversation details\n",
    "- üì¶ Resolved questions\n",
    "\n",
    "**When to Summarize:**\n",
    "- Token threshold exceeded (e.g., > 2000 tokens)\n",
    "- Message count threshold exceeded (e.g., > 10 messages)\n",
    "- Time-based (e.g., after 1 hour)\n",
    "- Manual trigger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8486d8bc89f7b",
   "metadata": {},
   "source": [
    "### Building Summarization Step-by-Step\n",
    "\n",
    "Let's build our summarization system incrementally, starting with simple components.\n",
    "\n",
    "#### Step 1: Create a data structure for conversation messages\n",
    "\n",
    "**What we're building:** A data structure to represent individual messages with metadata.\n",
    "\n",
    "**Why it's needed:** We need to track not just the message content, but also:\n",
    "- Who sent it (user, assistant, system)\n",
    "- When it was sent (timestamp)\n",
    "- How many tokens it uses (for threshold checks)\n",
    "\n",
    "**How it works:** Python's `@dataclass` decorator creates a clean, type-safe structure with automatic initialization and token counting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db188fb9f01d750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConversationMessage dataclass defined\n",
      "\n",
      "Example message:\n",
      "  Role: user\n",
      "  Content: What courses do you recommend for machine learning?\n",
      "  Tokens: 9\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ConversationMessage:\n",
    "    \"\"\"Represents a single conversation message.\"\"\"\n",
    "    role: str  # \"user\", \"assistant\", \"system\"\n",
    "    content: str\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "    token_count: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.token_count is None:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "print(\"‚úÖ ConversationMessage dataclass defined\")\n",
    "\n",
    "# Test it\n",
    "test_msg = ConversationMessage(\n",
    "    role=\"user\",\n",
    "    content=\"What courses do you recommend for machine learning?\"\n",
    ")\n",
    "print(f\"\\nExample message:\")\n",
    "print(f\"  Role: {test_msg.role}\")\n",
    "print(f\"  Content: {test_msg.content}\")\n",
    "print(f\"  Tokens: {test_msg.token_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f8f61e276661",
   "metadata": {},
   "source": [
    "#### Step 2: Create a function to check if summarization is needed\n",
    "\n",
    "**What we're building:** A decision function that determines when to trigger summarization.\n",
    "\n",
    "**Why it's needed:** We don't want to summarize too early (loses context) or too late (hits token limits). We need smart thresholds.\n",
    "\n",
    "**How it works:**\n",
    "- Checks if we have enough messages to make summarization worthwhile\n",
    "- Calculates total token count across all messages\n",
    "- Returns `True` if either threshold (tokens OR messages) is exceeded\n",
    "- Ensures we keep at least `keep_recent` messages unsummarized\n",
    "\n",
    "**When to summarize:**\n",
    "- Token threshold: Prevents hitting model limits (e.g., >2000 tokens)\n",
    "- Message threshold: Prevents conversation from getting too long (e.g., >10 messages)\n",
    "- Keep recent: Preserves the most relevant context (e.g., last 4 messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "290935fa536cb8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ should_summarize() function defined\n"
     ]
    }
   ],
   "source": [
    "def should_summarize(\n",
    "    messages: List[ConversationMessage],\n",
    "    token_threshold: int = 2000,\n",
    "    message_threshold: int = 10,\n",
    "    keep_recent: int = 4\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Determine if conversation needs summarization.\n",
    "\n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        token_threshold: Summarize when total tokens exceed this\n",
    "        message_threshold: Summarize when message count exceeds this\n",
    "        keep_recent: Number of recent messages to keep unsummarized\n",
    "\n",
    "    Returns:\n",
    "        True if summarization is needed\n",
    "    \"\"\"\n",
    "    # Don't summarize if we have very few messages\n",
    "    if len(messages) <= keep_recent:\n",
    "        return False\n",
    "\n",
    "    # Calculate total tokens\n",
    "    total_tokens = sum(msg.token_count for msg in messages)\n",
    "\n",
    "    # Summarize if either threshold is exceeded\n",
    "    return (total_tokens > token_threshold or\n",
    "            len(messages) > message_threshold)\n",
    "\n",
    "print(\"‚úÖ should_summarize() function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37993b003426e127",
   "metadata": {},
   "source": [
    "#### Step 3: Create a prompt template for summarization\n",
    "\n",
    "**What we're building:** A carefully crafted prompt that instructs the LLM on how to summarize conversations.\n",
    "\n",
    "**Why it's needed:** Generic summarization loses important details. We need domain-specific instructions that preserve what matters for course advisory conversations.\n",
    "\n",
    "**How it works:**\n",
    "- Specifies the context (student-advisor conversation)\n",
    "- Lists exactly what to preserve (decisions, requirements, goals, courses, issues)\n",
    "- Requests structured output (bullet points for clarity)\n",
    "- Emphasizes being \"specific and actionable\" (not vague summaries)\n",
    "\n",
    "**Design principle:** The prompt template is the \"instructions\" for the summarization LLM. Better instructions = better summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a39408752c4a504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summarization prompt template defined\n"
     ]
    }
   ],
   "source": [
    "summarization_prompt_template = \"\"\"You are summarizing a conversation between a student and a course advisor.\n",
    "\n",
    "Create a concise summary that preserves:\n",
    "1. Key decisions made\n",
    "2. Important requirements or prerequisites discussed\n",
    "3. Student's goals, preferences, and constraints\n",
    "4. Specific courses mentioned and recommendations given\n",
    "5. Any problems or issues that need follow-up\n",
    "\n",
    "Format as bullet points. Be specific and actionable.\n",
    "\n",
    "Conversation to summarize:\n",
    "{conversation}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "print(\"‚úÖ Summarization prompt template defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca0c3b7f31459f",
   "metadata": {},
   "source": [
    "#### Step 4: Create a function to generate summaries using the LLM\n",
    "\n",
    "**What we're building:** A function that takes messages and produces an intelligent summary using an LLM.\n",
    "\n",
    "**Why it's needed:** This is where the actual summarization happens. We need to:\n",
    "- Format the conversation for the LLM\n",
    "- Call the LLM with our prompt template\n",
    "- Package the summary as a system message\n",
    "\n",
    "**How it works:**\n",
    "1. Formats messages as \"User: ...\" and \"Assistant: ...\" text\n",
    "2. Inserts formatted conversation into the prompt template\n",
    "3. Calls the LLM asynchronously (non-blocking)\n",
    "4. Wraps the summary in `[CONVERSATION SUMMARY]` marker for easy identification\n",
    "5. Returns as a system message (distinguishes it from user/assistant messages)\n",
    "\n",
    "**Why async?** Summarization can take 1-3 seconds. Async allows other operations to continue while waiting for the LLM response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b41ae7eb2d88f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ create_summary() function defined\n"
     ]
    }
   ],
   "source": [
    "async def create_summary(\n",
    "    messages: List[ConversationMessage],\n",
    "    llm: ChatOpenAI\n",
    ") -> ConversationMessage:\n",
    "    \"\"\"\n",
    "    Create intelligent summary of conversation messages.\n",
    "\n",
    "    Args:\n",
    "        messages: List of messages to summarize\n",
    "        llm: Language model for generating summary\n",
    "\n",
    "    Returns:\n",
    "        ConversationMessage containing the summary\n",
    "    \"\"\"\n",
    "    # Format conversation for summarization\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{msg.role.title()}: {msg.content}\"\n",
    "        for msg in messages\n",
    "    ])\n",
    "\n",
    "    # Generate summary using LLM\n",
    "    prompt = summarization_prompt_template.format(conversation=conversation_text)\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    summary_content = f\"[CONVERSATION SUMMARY]\\n{response.content}\"\n",
    "\n",
    "    # Create summary message\n",
    "    summary_msg = ConversationMessage(\n",
    "        role=\"system\",\n",
    "        content=summary_content,\n",
    "        timestamp=messages[-1].timestamp\n",
    "    )\n",
    "\n",
    "    return summary_msg\n",
    "\n",
    "print(\"‚úÖ create_summary() function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb87c914424cd",
   "metadata": {},
   "source": [
    "#### Step 5: Create a function to compress conversations\n",
    "\n",
    "**What we're building:** The main compression function that orchestrates the entire summarization process.\n",
    "\n",
    "**Why it's needed:** This ties together all the previous components into a single, easy-to-use function that:\n",
    "- Decides whether to summarize\n",
    "- Splits messages into old vs. recent\n",
    "- Generates the summary\n",
    "- Returns the compressed conversation\n",
    "\n",
    "**How it works:**\n",
    "1. **Check:** Calls `should_summarize()` to see if compression is needed\n",
    "2. **Split:** Divides messages into `old_messages` (to summarize) and `recent_messages` (to keep)\n",
    "3. **Summarize:** Calls `create_summary()` on old messages\n",
    "4. **Combine:** Returns `[summary] + recent_messages`\n",
    "\n",
    "**The result:** A conversation that's 50-80% smaller but preserves all essential information.\n",
    "\n",
    "**Example:**\n",
    "- Input: 20 messages (4,000 tokens)\n",
    "- Output: 1 summary + 4 recent messages (1,200 tokens)\n",
    "- Savings: 70% reduction in tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b904a38b1bad2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ compress_conversation() function defined\n"
     ]
    }
   ],
   "source": [
    "async def compress_conversation(\n",
    "    messages: List[ConversationMessage],\n",
    "    llm: ChatOpenAI,\n",
    "    token_threshold: int = 2000,\n",
    "    message_threshold: int = 10,\n",
    "    keep_recent: int = 4\n",
    ") -> List[ConversationMessage]:\n",
    "    \"\"\"\n",
    "    Compress conversation by summarizing old messages and keeping recent ones.\n",
    "\n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        llm: Language model for generating summaries\n",
    "        token_threshold: Summarize when total tokens exceed this\n",
    "        message_threshold: Summarize when message count exceeds this\n",
    "        keep_recent: Number of recent messages to keep unsummarized\n",
    "\n",
    "    Returns:\n",
    "        List of messages: [summary] + [recent messages]\n",
    "    \"\"\"\n",
    "    # Check if summarization is needed\n",
    "    if not should_summarize(messages, token_threshold, message_threshold, keep_recent):\n",
    "        return messages\n",
    "\n",
    "    # Split into old and recent\n",
    "    old_messages = messages[:-keep_recent]\n",
    "    recent_messages = messages[-keep_recent:]\n",
    "\n",
    "    if not old_messages:\n",
    "        return messages\n",
    "\n",
    "    # Summarize old messages\n",
    "    summary = await create_summary(old_messages, llm)\n",
    "\n",
    "    # Return summary + recent messages\n",
    "    return [summary] + recent_messages\n",
    "\n",
    "print(\"‚úÖ compress_conversation() function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fce6b8d81c302",
   "metadata": {},
   "source": [
    "#### Step 6: Combine into a reusable class\n",
    "\n",
    "Now that we've built and tested each component, let's combine them into a reusable class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8324715c96096689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConversationSummarizer class defined\n"
     ]
    }
   ],
   "source": [
    "class ConversationSummarizer:\n",
    "    \"\"\"Manages conversation summarization to keep token counts manageable.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        token_threshold: int = 2000,\n",
    "        message_threshold: int = 10,\n",
    "        keep_recent: int = 4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the summarizer.\n",
    "\n",
    "        Args:\n",
    "            llm: Language model for generating summaries\n",
    "            token_threshold: Summarize when total tokens exceed this\n",
    "            message_threshold: Summarize when message count exceeds this\n",
    "            keep_recent: Number of recent messages to keep unsummarized\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.token_threshold = token_threshold\n",
    "        self.message_threshold = message_threshold\n",
    "        self.keep_recent = keep_recent\n",
    "        self.summarization_prompt = summarization_prompt_template\n",
    "\n",
    "    def should_summarize(self, messages: List[ConversationMessage]) -> bool:\n",
    "        \"\"\"Determine if conversation needs summarization.\"\"\"\n",
    "        return should_summarize(\n",
    "            messages,\n",
    "            self.token_threshold,\n",
    "            self.message_threshold,\n",
    "            self.keep_recent\n",
    "        )\n",
    "\n",
    "    async def summarize_conversation(\n",
    "        self,\n",
    "        messages: List[ConversationMessage]\n",
    "    ) -> ConversationMessage:\n",
    "        \"\"\"Create intelligent summary of conversation messages.\"\"\"\n",
    "        return await create_summary(messages, self.llm)\n",
    "\n",
    "    async def compress_conversation(\n",
    "        self,\n",
    "        messages: List[ConversationMessage]\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress conversation by summarizing old messages and keeping recent ones.\"\"\"\n",
    "        return await compress_conversation(\n",
    "            messages,\n",
    "            self.llm,\n",
    "            self.token_threshold,\n",
    "            self.message_threshold,\n",
    "            self.keep_recent\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ ConversationSummarizer class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb98376eb2b00b0",
   "metadata": {},
   "source": [
    "### Demo 3: Test Summarization\n",
    "\n",
    "Let's test the summarizer with a sample conversation.\n",
    "\n",
    "#### Step 1: Create a sample conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e63fdaf5a2a2587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original conversation:\n",
      "  Messages: 16\n",
      "  Total tokens: 261\n",
      "  Average tokens per message: 16.3\n"
     ]
    }
   ],
   "source": [
    "# Create a sample long conversation\n",
    "sample_conversation = [\n",
    "    ConversationMessage(\"user\", \"Hi, I'm interested in learning about machine learning courses\"),\n",
    "    ConversationMessage(\"assistant\", \"Great! Redis University offers several ML courses. CS401 Machine Learning is our flagship course. It covers supervised learning, neural networks, and practical applications.\"),\n",
    "    ConversationMessage(\"user\", \"What are the prerequisites for CS401?\"),\n",
    "    ConversationMessage(\"assistant\", \"CS401 requires CS201 Data Structures and MATH301 Linear Algebra. Have you completed these courses?\"),\n",
    "    ConversationMessage(\"user\", \"I've completed CS101 but not CS201 yet\"),\n",
    "    ConversationMessage(\"assistant\", \"Perfect! CS201 is the next logical step. It covers algorithms and data structures essential for ML. It's offered every semester.\"),\n",
    "    ConversationMessage(\"user\", \"How difficult is MATH301?\"),\n",
    "    ConversationMessage(\"assistant\", \"MATH301 is moderately challenging. It covers vectors, matrices, and eigenvalues used in ML algorithms. Most students find it manageable with consistent practice.\"),\n",
    "    ConversationMessage(\"user\", \"Can I take both CS201 and MATH301 together?\"),\n",
    "    ConversationMessage(\"assistant\", \"Yes, that's a good combination! They complement each other well. Many students take them concurrently.\"),\n",
    "    ConversationMessage(\"user\", \"What about CS401 after that?\"),\n",
    "    ConversationMessage(\"assistant\", \"CS401 is perfect after completing both prerequisites. It's our most popular AI course with hands-on projects.\"),\n",
    "    ConversationMessage(\"user\", \"When is CS401 offered?\"),\n",
    "    ConversationMessage(\"assistant\", \"CS401 is offered in Fall and Spring semesters. The Fall section typically fills up quickly, so register early!\"),\n",
    "    ConversationMessage(\"user\", \"Great! What's the workload like?\"),\n",
    "    ConversationMessage(\"assistant\", \"CS401 requires about 10-12 hours per week including lectures, assignments, and projects. There are 4 major projects throughout the semester.\"),\n",
    "]\n",
    "\n",
    "# Calculate original metrics\n",
    "original_token_count = sum(msg.token_count for msg in sample_conversation)\n",
    "print(f\"Original conversation:\")\n",
    "print(f\"  Messages: {len(sample_conversation)}\")\n",
    "print(f\"  Total tokens: {original_token_count}\")\n",
    "print(f\"  Average tokens per message: {original_token_count / len(sample_conversation):.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824592502d5305",
   "metadata": {},
   "source": [
    "#### Step 2: Configure the summarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f1cd42e5cb65a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizer configuration:\n",
      "  Token threshold: 500\n",
      "  Message threshold: 10\n",
      "  Keep recent: 4\n"
     ]
    }
   ],
   "source": [
    "# Test summarization\n",
    "summarizer = ConversationSummarizer(\n",
    "    llm=llm,\n",
    "    token_threshold=500,  # Low threshold for demo\n",
    "    message_threshold=10,\n",
    "    keep_recent=4\n",
    ")\n",
    "\n",
    "print(f\"Summarizer configuration:\")\n",
    "print(f\"  Token threshold: {summarizer.token_threshold}\")\n",
    "print(f\"  Message threshold: {summarizer.message_threshold}\")\n",
    "print(f\"  Keep recent: {summarizer.keep_recent}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b283d8917e353",
   "metadata": {},
   "source": [
    "#### Step 3: Check if summarization is needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96d60c07d558dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should summarize? True\n"
     ]
    }
   ],
   "source": [
    "# Check if summarization is needed\n",
    "should_summarize_result = summarizer.should_summarize(sample_conversation)\n",
    "print(f\"Should summarize? {should_summarize_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956554c8c979d1a4",
   "metadata": {},
   "source": [
    "#### Step 4: Compress the conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3566e3ee779cc9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After summarization:\n",
      "  Messages: 5\n",
      "  Total tokens: 294\n",
      "  Token savings: -33 (-12.6%)\n"
     ]
    }
   ],
   "source": [
    "# Compress the conversation\n",
    "compressed = await summarizer.compress_conversation(sample_conversation)\n",
    "\n",
    "compressed_token_count = sum(msg.token_count for msg in compressed)\n",
    "token_savings = original_token_count - compressed_token_count\n",
    "savings_percentage = (token_savings / original_token_count) * 100\n",
    "\n",
    "print(f\"After summarization:\")\n",
    "print(f\"  Messages: {len(compressed)}\")\n",
    "print(f\"  Total tokens: {compressed_token_count}\")\n",
    "print(f\"  Token savings: {token_savings} ({savings_percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85f81eedf9cae1",
   "metadata": {},
   "source": [
    "#### Step 5: Examine the compressed conversation structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82e6fb297080ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed conversation structure:\n",
      "  1. üìã [system] [CONVERSATION SUMMARY] - **Key Decisions Made:**   - The student decided to take...\n",
      "     Tokens: 230\n",
      "  2. üë§ [user] When is CS401 offered?...\n",
      "     Tokens: 6\n",
      "  3. ü§ñ [assistant] CS401 is offered in Fall and Spring semesters. The Fall section typically fills ...\n",
      "     Tokens: 22\n",
      "  4. üë§ [user] Great! What's the workload like?...\n",
      "     Tokens: 7\n",
      "  5. ü§ñ [assistant] CS401 requires about 10-12 hours per week including lectures, assignments, and p...\n",
      "     Tokens: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"Compressed conversation structure:\")\n",
    "for i, msg in enumerate(compressed):\n",
    "    role_icon = \"üìã\" if msg.role == \"system\" else \"üë§\" if msg.role == \"user\" else \"ü§ñ\"\n",
    "    content_preview = msg.content[:80].replace('\\n', ' ')\n",
    "    print(f\"  {i+1}. {role_icon} [{msg.role}] {content_preview}...\")\n",
    "    print(f\"     Tokens: {msg.token_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb252a2997a22ba",
   "metadata": {},
   "source": [
    "#### Results Analysis\n",
    "\n",
    "**What happened:**\n",
    "- Original: 16 messages with ~{original_token_count} tokens\n",
    "- Compressed: {len(compressed)} messages (1 summary + 4 recent)\n",
    "- Savings: ~{savings_percentage:.0f}% token reduction\n",
    "\n",
    "**Key benefits:**\n",
    "- Preserved recent context (last 4 messages)\n",
    "- Summarized older messages into key facts\n",
    "- Maintained conversation continuity\n",
    "- Reduced token costs significantly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896bce27c392ee9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Part 3: Summarization is not the only strategy, there are other Context Compression Strategies\n",
    "\n",
    "Summarization compresses conversation history by generating condensed representations of past messages. However, it's not the only viable approach to context management, and it's not always optimal.\n",
    "\n",
    "### Why Not Always Optimal?\n",
    "\n",
    "Summarization is powerful but introduces trade-offs that make it suboptimal for certain scenarios.\n",
    "\n",
    "**Technical Trade-offs:**\n",
    "\n",
    "1. **Latency Overhead**\n",
    "   - Each summarization requires an LLM API call\n",
    "   - Adds 1-3 seconds per compression (vs. <10ms for truncation)\n",
    "   - Blocks conversation flow in real-time applications\n",
    "\n",
    "2. **Cost Multiplication**\n",
    "   - Input tokens: Entire conversation to summarize\n",
    "   - Output tokens: Generated summary\n",
    "   - At scale: 1,000 conversations/day = 1,000+ extra LLM calls\n",
    "\n",
    "3. **Lossy Compression**\n",
    "   - Summaries paraphrase, don't preserve exact wording\n",
    "   - Loses temporal sequence and conversation flow\n",
    "   - Can't reconstruct original messages\n",
    "   - Unacceptable for legal, medical, or compliance contexts\n",
    "\n",
    "4. **Implementation Complexity**\n",
    "   - Requires async operations and error handling\n",
    "   - Needs domain-specific prompt engineering\n",
    "   - Unpredictable compression ratios\n",
    "   - Summary quality varies with prompt design\n",
    "\n",
    "**When to Use Alternatives:**\n",
    "\n",
    "| Scenario | Better Strategy | Why |\n",
    "|----------|----------------|-----|\n",
    "| Short conversations (<5 turns) | None (keep all) | Overhead exceeds benefit |\n",
    "| Real-time chat | Truncation | Zero latency |\n",
    "| Cost-sensitive (high volume) | Priority-based | No API calls |\n",
    "| Verbatim accuracy required | Truncation | Preserves exact wording |\n",
    "| Balanced quality + speed | Priority-based | Intelligent selection, no LLM |\n",
    "\n",
    "**Decision Framework:**\n",
    "- **Speed-critical** ‚Üí Truncation (instant, predictable)\n",
    "- **Cost-sensitive** ‚Üí Priority-based (no API calls, intelligent)\n",
    "- **Quality-critical** ‚Üí Summarization (preserves meaning, expensive)\n",
    "- **Hybrid** ‚Üí Truncation + summarization (fast for most, summarize when needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2737aeb03474",
   "metadata": {},
   "source": [
    "### Theory: Four Compression Approaches\n",
    "\n",
    "Let's explore four different strategies, each with different trade-offs:\n",
    "\n",
    "**1. Truncation (Token-Aware)**\n",
    "- Keep recent messages within token budget\n",
    "- ‚úÖ Pros: Fast, no LLM calls, respects context limits\n",
    "- ‚ùå Cons: Variable message count, loses old context\n",
    "- **Best for:** Token-constrained applications, API limits\n",
    "\n",
    "**2. Sliding Window (Message-Aware)**\n",
    "- Keep exactly N most recent messages\n",
    "- ‚úÖ Pros: Fastest, predictable count, constant memory\n",
    "- ‚ùå Cons: May exceed token limits, loses old context\n",
    "- **Best for:** Fixed-size buffers, real-time chat\n",
    "\n",
    "**3. Priority-Based (Balanced)**\n",
    "- Score messages by importance, keep highest-scoring\n",
    "- ‚úÖ Pros: Preserves important context, no LLM calls\n",
    "- ‚ùå Cons: Requires good scoring logic, may lose temporal flow\n",
    "- **Best for:** Production applications needing balance\n",
    "\n",
    "**4. Summarization (High Quality)**\n",
    "- Use LLM to create intelligent summaries\n",
    "- ‚úÖ Pros: Preserves meaning, high quality\n",
    "- ‚ùå Cons: Slower, costs tokens, requires LLM call\n",
    "- **Best for:** High-value conversations, quality-critical applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5f28d6ed343f6",
   "metadata": {},
   "source": [
    "### Building Compression Strategies Step-by-Step\n",
    "\n",
    "Let's build each strategy incrementally, starting with the simplest.\n",
    "\n",
    "#### Step 1: Define a base interface for compression strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b053a7b2c242989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressionStrategy:\n",
    "    \"\"\"Base class for compression strategies.\"\"\"\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress messages to fit within max_tokens.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"‚úÖ CompressionStrategy base class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ab8bf105c70aa",
   "metadata": {},
   "source": [
    "#### Step 2: Implement Truncation Strategy (Simplest)\n",
    "\n",
    "This strategy simply keeps the most recent messages that fit within the token budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c2576cad8bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncationStrategy(CompressionStrategy):\n",
    "    \"\"\"Keep only the most recent messages within token budget.\"\"\"\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Keep most recent messages within token budget.\"\"\"\n",
    "        compressed = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Work backwards from most recent\n",
    "        for msg in reversed(messages):\n",
    "            if total_tokens + msg.token_count <= max_tokens:\n",
    "                compressed.insert(0, msg)\n",
    "                total_tokens += msg.token_count\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return compressed\n",
    "\n",
    "print(\"‚úÖ TruncationStrategy implemented\")\n",
    "\n",
    "# Test it\n",
    "truncation = TruncationStrategy()\n",
    "test_result = truncation.compress(sample_conversation, max_tokens=500)\n",
    "print(f\"   Truncation test: {len(sample_conversation)} messages ‚Üí {len(test_result)} messages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd84d939f70075",
   "metadata": {},
   "source": [
    "#### Step 2.5: Implement Sliding Window Strategy (Simplest)\n",
    "\n",
    "**What we're building:** A strategy that maintains a fixed-size window of the N most recent messages.\n",
    "\n",
    "**Why it's different from truncation:**\n",
    "- **Truncation:** Reactive - keeps messages until token budget exceeded, then removes oldest\n",
    "- **Sliding Window:** Proactive - always maintains exactly N messages regardless of tokens\n",
    "\n",
    "**When to use:**\n",
    "- Real-time chat where you want constant context size\n",
    "- Systems with predictable message patterns\n",
    "- When simplicity matters more than token optimization\n",
    "\n",
    "**Trade-off:** May exceed token limits if messages are very long.\n",
    "\n",
    "**How it works:** Simply returns the last N messages using Python list slicing (`messages[-N:]`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683df2353cdfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowStrategy(CompressionStrategy):\n",
    "    \"\"\"Keep only the last N messages (fixed window size).\"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize sliding window strategy.\n",
    "\n",
    "        Args:\n",
    "            window_size: Number of recent messages to keep\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"\n",
    "        Keep only the last N messages.\n",
    "\n",
    "        Note: Ignores max_tokens parameter - always keeps exactly window_size messages.\n",
    "        \"\"\"\n",
    "        if len(messages) <= self.window_size:\n",
    "            return messages\n",
    "\n",
    "        return messages[-self.window_size:]\n",
    "\n",
    "print(\"‚úÖ SlidingWindowStrategy implemented\")\n",
    "\n",
    "# Test it\n",
    "sliding_window = SlidingWindowStrategy(window_size=6)\n",
    "test_result = sliding_window.compress(sample_conversation, max_tokens=500)\n",
    "test_tokens = sum(msg.token_count for msg in test_result)\n",
    "\n",
    "print(f\"   Sliding window test: {len(sample_conversation)} messages ‚Üí {len(test_result)} messages\")\n",
    "print(f\"   Token count: {test_tokens} tokens (budget was {500})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42299c4601c4f31a",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "The sliding window strategy demonstrates:\n",
    "- ‚úÖ **Predictable output:** Always returns exactly 6 messages (or fewer if conversation is shorter)\n",
    "- ‚úÖ **O(1) complexity:** Just slices the list - fastest possible implementation\n",
    "- ‚ö†Ô∏è **Token-agnostic:** Returned {test_tokens} tokens, which may or may not fit the 500 token budget\n",
    "- ‚úÖ **Simplest code:** One line implementation (`messages[-N:]`)\n",
    "\n",
    "**Key insight:** Sliding window prioritizes **predictability** over **token optimization**. Use it when you need constant context size and can tolerate variable token counts.\n",
    "\n",
    "**Comparison with Truncation:**\n",
    "- **Truncation:** \"Keep as many recent messages as fit in budget\" ‚Üí Variable count, guaranteed under limit\n",
    "- **Sliding Window:** \"Keep exactly N recent messages\" ‚Üí Fixed count, may exceed limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739168f3fa76a165",
   "metadata": {},
   "source": [
    "#### Step 3: Implement Priority-Based Strategy (Intelligent Selection)\n",
    "\n",
    "This strategy scores messages by importance and keeps the highest-scoring ones.\n",
    "\n",
    "First, let's create a function to calculate message importance:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1d3e19b190c9e3c",
   "metadata": {},
   "source": [
    "def calculate_message_importance(msg: ConversationMessage) -> float:\n",
    "    \"\"\"\n",
    "    Calculate importance score for a message.\n",
    "\n",
    "    Higher scores = more important.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    content_lower = msg.content.lower()\n",
    "\n",
    "    # Course codes are important (CS401, MATH301, etc.)\n",
    "    if any(code in content_lower for code in ['cs', 'math', 'eng']):\n",
    "        score += 2.0\n",
    "\n",
    "    # Questions are important\n",
    "    if '?' in msg.content:\n",
    "        score += 1.5\n",
    "\n",
    "    # Prerequisites and requirements are important\n",
    "    if any(word in content_lower for word in ['prerequisite', 'require', 'need']):\n",
    "        score += 1.5\n",
    "\n",
    "    # Preferences and goals are important\n",
    "    if any(word in content_lower for word in ['prefer', 'want', 'goal', 'interested']):\n",
    "        score += 1.0\n",
    "\n",
    "    # User messages slightly more important (their needs)\n",
    "    if msg.role == 'user':\n",
    "        score += 0.5\n",
    "\n",
    "    # Longer messages often have more content\n",
    "    if msg.token_count > 50:\n",
    "        score += 0.5\n",
    "\n",
    "    return score\n",
    "\n",
    "print(\"‚úÖ calculate_message_importance() function defined\")\n",
    "\n",
    "# Test it\n",
    "test_scores = [(msg.content[:50], calculate_message_importance(msg))\n",
    "               for msg in sample_conversation[:3]]\n",
    "print(\"\\nExample importance scores:\")\n",
    "for content, score in test_scores:\n",
    "    print(f\"  Score {score:.1f}: {content}...\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f66e696bacf5a96a",
   "metadata": {},
   "source": "Now let's create the Priority-Based strategy class:\n"
  },
  {
   "cell_type": "code",
   "id": "57f0400bdab30655",
   "metadata": {},
   "source": [
    "class PriorityBasedStrategy(CompressionStrategy):\n",
    "    \"\"\"Keep highest-priority messages within token budget.\"\"\"\n",
    "\n",
    "    def calculate_importance(self, msg: ConversationMessage) -> float:\n",
    "        \"\"\"Calculate importance score for a message.\"\"\"\n",
    "        return calculate_message_importance(msg)\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Keep highest-priority messages within token budget.\"\"\"\n",
    "        # Score each message\n",
    "        scored_messages = [\n",
    "            (self.calculate_importance(msg), i, msg)\n",
    "            for i, msg in enumerate(messages)\n",
    "        ]\n",
    "\n",
    "        # Sort by score (descending), then by index to maintain some order\n",
    "        scored_messages.sort(key=lambda x: (-x[0], x[1]))\n",
    "\n",
    "        # Select messages within budget\n",
    "        selected = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        for score, idx, msg in scored_messages:\n",
    "            if total_tokens + msg.token_count <= max_tokens:\n",
    "                selected.append((idx, msg))\n",
    "                total_tokens += msg.token_count\n",
    "\n",
    "        # Sort by original index to maintain conversation flow\n",
    "        selected.sort(key=lambda x: x[0])\n",
    "\n",
    "        return [msg for idx, msg in selected]\n",
    "\n",
    "print(\"‚úÖ PriorityBasedStrategy implemented\")\n",
    "\n",
    "# Test it\n",
    "priority = PriorityBasedStrategy()\n",
    "test_result = priority.compress(sample_conversation, max_tokens=800)\n",
    "print(f\"   Priority-based test: {len(sample_conversation)} messages ‚Üí {len(test_result)} messages\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c0fa64ab406ef95",
   "metadata": {},
   "source": [
    "#### Step 4: Implement Summarization Strategy (Highest Quality)\n",
    "\n",
    "This strategy uses our ConversationSummarizer to create intelligent summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d0ddde791c5afc",
   "metadata": {},
   "source": [
    "class SummarizationStrategy(CompressionStrategy):\n",
    "    \"\"\"Use LLM to create intelligent summaries.\"\"\"\n",
    "\n",
    "    def __init__(self, summarizer: ConversationSummarizer):\n",
    "        self.summarizer = summarizer\n",
    "\n",
    "    async def compress_async(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Compress using summarization (async).\"\"\"\n",
    "        # Use the summarizer's logic\n",
    "        return await self.summarizer.compress_conversation(messages)\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"Synchronous wrapper (not recommended, use compress_async).\"\"\"\n",
    "        raise NotImplementedError(\"Use compress_async for summarization strategy\")\n",
    "\n",
    "print(\"‚úÖ SummarizationStrategy implemented\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22b54c30ef8be4a8",
   "metadata": {},
   "source": [
    "### Demo 4: Compare Compression Strategies\n",
    "\n",
    "Let's compare all four strategies on the same conversation to understand their trade-offs.\n",
    "\n",
    "#### Step 1: Set up the test\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "96dac15eec962562",
   "metadata": {},
   "source": [
    "# Use the same sample conversation from before\n",
    "test_conversation = sample_conversation.copy()\n",
    "max_tokens = 800  # Target token budget\n",
    "\n",
    "original_tokens = sum(msg.token_count for msg in test_conversation)\n",
    "print(f\"Original conversation: {len(test_conversation)} messages, {original_tokens} tokens\")\n",
    "print(f\"Target budget: {max_tokens} tokens\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be20f6779afc21e9",
   "metadata": {},
   "source": "#### Step 2: Test Truncation Strategy\n"
  },
  {
   "cell_type": "code",
   "id": "d8dfbdc40403d640",
   "metadata": {},
   "source": [
    "truncation = TruncationStrategy()\n",
    "truncated = truncation.compress(test_conversation, max_tokens)\n",
    "truncated_tokens = sum(msg.token_count for msg in truncated)\n",
    "\n",
    "print(f\"TRUNCATION STRATEGY\")\n",
    "print(f\"  Result: {len(truncated)} messages, {truncated_tokens} tokens\")\n",
    "print(f\"  Savings: {original_tokens - truncated_tokens} tokens\")\n",
    "print(f\"  Kept messages: {[i for i, msg in enumerate(test_conversation) if msg in truncated]}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4018ee04019c9a9a",
   "metadata": {},
   "source": "#### Step 2.5: Test Sliding Window Strategy\n"
  },
  {
   "cell_type": "code",
   "id": "529392dfaf6dbe64",
   "metadata": {},
   "source": [
    "sliding_window = SlidingWindowStrategy(window_size=6)\n",
    "windowed = sliding_window.compress(test_conversation, max_tokens)\n",
    "windowed_tokens = sum(msg.token_count for msg in windowed)\n",
    "\n",
    "print(f\"SLIDING WINDOW STRATEGY\")\n",
    "print(f\"  Result: {len(windowed)} messages, {windowed_tokens} tokens\")\n",
    "print(f\"  Savings: {original_tokens - windowed_tokens} tokens\")\n",
    "print(f\"  Kept messages: {[i for i, msg in enumerate(test_conversation) if msg in windowed]}\")\n",
    "print(f\"  Token budget: {windowed_tokens}/{max_tokens} ({'within' if windowed_tokens <= max_tokens else 'EXCEEDS'} limit)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69267d84d68c7376",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "The sliding window kept:\n",
    "- **Exactly 6 messages** (last 6 from the conversation)\n",
    "- **Most recent context only** (indices show the final messages)\n",
    "- **{windowed_tokens} tokens** (may or may not fit budget)\n",
    "\n",
    "**Key difference from truncation:**\n",
    "- **Truncation:** Kept {len(truncated)} messages to stay under {max_tokens} tokens\n",
    "- **Sliding Window:** Kept exactly 6 messages, resulting in {windowed_tokens} tokens\n",
    "\n",
    "**Behavior pattern:**\n",
    "- Truncation: \"Fill the budget\" ‚Üí Variable count, guaranteed fit\n",
    "- Sliding Window: \"Fixed window\" ‚Üí Constant count, may exceed budget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2ce7a958fbe9d",
   "metadata": {},
   "source": "#### Step 3: Test Priority-Based Strategy\n"
  },
  {
   "cell_type": "code",
   "id": "fed34b703bb9c7d9",
   "metadata": {},
   "source": [
    "priority = PriorityBasedStrategy()\n",
    "prioritized = priority.compress(test_conversation, max_tokens)\n",
    "prioritized_tokens = sum(msg.token_count for msg in prioritized)\n",
    "\n",
    "print(f\"PRIORITY-BASED STRATEGY\")\n",
    "print(f\"  Result: {len(prioritized)} messages, {prioritized_tokens} tokens\")\n",
    "print(f\"  Savings: {original_tokens - prioritized_tokens} tokens\")\n",
    "print(f\"  Kept messages: {[i for i, msg in enumerate(test_conversation) if msg in prioritized]}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "134971d1108034c4",
   "metadata": {},
   "source": "Let's examine which messages were selected and why:\n"
  },
  {
   "cell_type": "code",
   "id": "e310f0458261b9a8",
   "metadata": {},
   "source": [
    "# Show importance scores for selected messages\n",
    "print(\"Sample importance scores:\")\n",
    "for i in [0, 2, 4, 6]:\n",
    "    if i < len(test_conversation):\n",
    "        score = priority.calculate_importance(test_conversation[i])\n",
    "        preview = test_conversation[i].content[:50]\n",
    "        print(f\"  Message {i}: {score:.1f} - \\\"{preview}...\\\"\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "997bc235a9b3038b",
   "metadata": {},
   "source": "#### Step 4: Test Summarization Strategy\n"
  },
  {
   "cell_type": "code",
   "id": "eb0f2653b2c4e89b",
   "metadata": {},
   "source": [
    "summarization = SummarizationStrategy(summarizer)\n",
    "summarized = await summarization.compress_async(test_conversation, max_tokens)\n",
    "summarized_tokens = sum(msg.token_count for msg in summarized)\n",
    "\n",
    "print(f\"SUMMARIZATION STRATEGY\")\n",
    "print(f\"  Result: {len(summarized)} messages, {summarized_tokens} tokens\")\n",
    "print(f\"  Savings: {original_tokens - summarized_tokens} tokens\")\n",
    "print(f\"  Structure: 1 summary + {len(summarized) - 1} recent messages\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47b36cc71717932b",
   "metadata": {},
   "source": "#### Step 5: Compare all strategies\n"
  },
  {
   "cell_type": "code",
   "id": "bfe7c056c978aea4",
   "metadata": {},
   "source": [
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Strategy':<20} {'Messages':<12} {'Tokens':<12} {'Savings':<12} {'Quality'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "strategies = [\n",
    "    (\"Original\", len(test_conversation), original_tokens, 0, \"N/A\"),\n",
    "    (\"Truncation\", len(truncated), truncated_tokens, original_tokens - truncated_tokens, \"Low\"),\n",
    "    (\"Sliding Window\", len(windowed), windowed_tokens, original_tokens - windowed_tokens, \"Low\"),\n",
    "    (\"Priority-Based\", len(prioritized), prioritized_tokens, original_tokens - prioritized_tokens, \"Medium\"),\n",
    "    (\"Summarization\", len(summarized), summarized_tokens, original_tokens - summarized_tokens, \"High\"),\n",
    "]\n",
    "\n",
    "for name, msgs, tokens, savings, quality in strategies:\n",
    "    savings_pct = f\"({savings/original_tokens*100:.0f}%)\" if savings > 0 else \"\"\n",
    "    print(f\"{name:<20} {msgs:<12} {tokens:<12} {savings:<5} {savings_pct:<6} {quality}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ebd894c5ffdfff",
   "metadata": {},
   "source": [
    "#### Key Takeaways\n",
    "\n",
    "**Truncation (Token-Aware):**\n",
    "- Keeps messages within token budget\n",
    "- Variable message count, guaranteed under limit\n",
    "- Good for: API token limits, cost control\n",
    "\n",
    "**Sliding Window (Message-Aware):**\n",
    "- Keeps exactly N most recent messages\n",
    "- Fixed message count, may exceed token budget\n",
    "- Good for: Real-time chat, predictable context size\n",
    "\n",
    "**Priority-Based (Intelligent):**\n",
    "- Scores and keeps important messages\n",
    "- Preserves key information across conversation\n",
    "- Good for: Most production applications, balanced approach\n",
    "\n",
    "**Summarization (Highest Quality):**\n",
    "- Uses LLM to preserve meaning\n",
    "- Highest quality, but requires API call (cost + latency)\n",
    "- Good for: High-value conversations, support tickets, advisory sessions\n",
    "\n",
    "**Decision Framework:**\n",
    "- **Speed-critical** ‚Üí Truncation or Sliding Window (instant, no LLM)\n",
    "- **Cost-sensitive** ‚Üí Priority-Based (intelligent, no API calls)\n",
    "- **Quality-critical** ‚Üí Summarization (preserves meaning, expensive)\n",
    "- **Predictable context** ‚Üí Sliding Window (constant message count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca23d0020c84249",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 4: Agent Memory Server Integration\n",
    "\n",
    "The Agent Memory Server provides automatic summarization. Let's configure and test it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0c2b93f2cf79e",
   "metadata": {},
   "source": [
    "### üî¨ Research Foundation: Hierarchical Memory Management\n",
    "\n",
    "Packer et al. (2023) in [\"MemGPT: Towards LLMs as Operating Systems\"](https://arxiv.org/abs/2310.08560) introduced a groundbreaking approach to memory management:\n",
    "\n",
    "**Key Insight:** Treat LLM context like an operating system's memory hierarchy:\n",
    "- **Main Context** (like RAM): Limited, fast access\n",
    "- **External Memory** (like disk): Unlimited, slower access\n",
    "- **Intelligent Paging**: Move data between tiers based on relevance\n",
    "\n",
    "**Their Virtual Context Management System:**\n",
    "1. Fixed-size main context (within token limits)\n",
    "2. Recursive memory retrieval from external storage\n",
    "3. LLM decides what to page in/out based on task needs\n",
    "\n",
    "**Practical Implications:**\n",
    "- Hierarchical approach enables unbounded conversations\n",
    "- Intelligent data movement between memory tiers\n",
    "- Transparent to application code\n",
    "\n",
    "**This is exactly what Agent Memory Server implements:**\n",
    "- **Working Memory** (Main Context): Session-scoped conversation messages\n",
    "- **Long-term Memory** (External Memory): Persistent facts, preferences, goals\n",
    "- **Automatic Management**: Extracts important information from working ‚Üí long-term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a958c1d8afa844",
   "metadata": {},
   "source": [
    "### üî¨ Research-Backed Implementation\n",
    "\n",
    "The Agent Memory Server implements the research findings we've discussed:\n",
    "\n",
    "**From \"Lost in the Middle\" (Liu et al., 2023):**\n",
    "- Keeps recent messages at the end of context (optimal position)\n",
    "- Summarizes middle content to avoid performance degradation\n",
    "- Maintains fixed context size for consistent performance\n",
    "\n",
    "**From \"Recursive Summarization\" (Wang et al., 2023):**\n",
    "- Automatically creates summaries when thresholds are exceeded\n",
    "- Preserves key information across long conversations\n",
    "- Enables unbounded conversation length\n",
    "\n",
    "**From \"MemGPT\" (Packer et al., 2023):**\n",
    "- Hierarchical memory management (working + long-term)\n",
    "- Intelligent data movement between memory tiers\n",
    "- Transparent to application code\n",
    "\n",
    "**Production Best Practices** (Anthropic, Vellum AI):\n",
    "- Configurable thresholds for different use cases\n",
    "- Multiple strategies (truncation, summarization, hybrid)\n",
    "- Scalable and production-ready architecture\n",
    "\n",
    "**References:**\n",
    "- Packer, C., Wooders, S., Lin, K., et al. (2023). MemGPT: Towards LLMs as Operating Systems. *arXiv preprint arXiv:2310.08560*.\n",
    "- Vellum AI. (2024). [How Should I Manage Memory for my LLM Chatbot?](https://www.vellum.ai/blog/how-should-i-manage-memory-for-my-llm-chatbot)\n",
    "- Anthropic. (2024). [Effective Context Engineering for AI Agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4464b58fc9c40",
   "metadata": {},
   "source": [
    "### Theory: Automatic Memory Management\n",
    "\n",
    "**Agent Memory Server Features:**\n",
    "- ‚úÖ Automatic summarization when thresholds are exceeded\n",
    "- ‚úÖ Configurable strategies (recent + summary, sliding window, full summary)\n",
    "- ‚úÖ Transparent to your application code\n",
    "- ‚úÖ Production-ready and scalable\n",
    "\n",
    "**How It Works:**\n",
    "1. You add messages to working memory normally\n",
    "2. Server monitors message count and token count\n",
    "3. When threshold is exceeded, server automatically summarizes\n",
    "4. Old messages are replaced with summary\n",
    "5. Recent messages are kept for context\n",
    "6. Your application retrieves the compressed memory\n",
    "\n",
    "**Configuration Options:**\n",
    "- `message_threshold`: Summarize after N messages (default: 20)\n",
    "- `token_threshold`: Summarize after N tokens (default: 4000)\n",
    "- `keep_recent`: Number of recent messages to keep (default: 4)\n",
    "- `strategy`: \"recent_plus_summary\", \"sliding_window\", or \"full_summary\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585948b56598a9f",
   "metadata": {},
   "source": [
    "### Demo 5: Test Automatic Summarization\n",
    "\n",
    "Let's test the Agent Memory Server's automatic summarization with a long conversation.\n",
    "\n",
    "#### Step 1: Create a test session\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "de6e6cc74530366a",
   "metadata": {},
   "source": [
    "# Create a test session\n",
    "test_session_id = f\"long_conversation_test_{int(time.time())}\"\n",
    "test_student_id = \"student_memory_test\"\n",
    "\n",
    "print(f\"Testing automatic summarization\")\n",
    "print(f\"Session ID: {test_session_id}\")\n",
    "print(f\"Student ID: {test_student_id}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a557dad8d8f53ef0",
   "metadata": {},
   "source": "#### Step 2: Define a long conversation (25 turns = 50 messages)\n"
  },
  {
   "cell_type": "code",
   "id": "4addd7959de37558",
   "metadata": {},
   "source": [
    "conversation_turns = [\n",
    "    (\"I'm interested in machine learning\", \"Great! CS401 Machine Learning is perfect for you.\"),\n",
    "    (\"What are the prerequisites?\", \"You'll need CS201 Data Structures and MATH301 Linear Algebra.\"),\n",
    "    (\"I've completed CS101\", \"Perfect! CS201 is your next step.\"),\n",
    "    (\"How difficult is CS201?\", \"It's moderately challenging but very rewarding.\"),\n",
    "    (\"When is it offered?\", \"CS201 is offered every semester - Fall, Spring, and Summer.\"),\n",
    "    (\"What about MATH301?\", \"MATH301 covers linear algebra essentials for ML.\"),\n",
    "    (\"Can I take both together?\", \"Yes, many students take CS201 and MATH301 concurrently.\"),\n",
    "    (\"How long will it take?\", \"If you take both, you can start CS401 in about 4-6 months.\"),\n",
    "    (\"What's the workload?\", \"Expect 10-12 hours per week for each course.\"),\n",
    "    (\"Are there online options?\", \"Yes, both courses have online and in-person sections.\"),\n",
    "    (\"Which format is better?\", \"Online offers flexibility, in-person offers more interaction.\"),\n",
    "    (\"What about CS401 after that?\", \"CS401 is our flagship ML course with hands-on projects.\"),\n",
    "    (\"How many projects?\", \"CS401 has 4 major projects throughout the semester.\"),\n",
    "    (\"What topics are covered?\", \"Supervised learning, neural networks, deep learning, and NLP.\"),\n",
    "    (\"Is there a final exam?\", \"Yes, there's a comprehensive final exam worth 30% of your grade.\"),\n",
    "    (\"What's the pass rate?\", \"About 85% of students pass CS401 on their first attempt.\"),\n",
    "    (\"Are there TAs available?\", \"Yes, we have 3 TAs for CS401 with office hours daily.\"),\n",
    "    (\"What programming language?\", \"CS401 uses Python with TensorFlow and PyTorch.\"),\n",
    "    (\"Do I need a GPU?\", \"Recommended but not required. We provide cloud GPU access.\"),\n",
    "    (\"What's the class size?\", \"CS401 typically has 30-40 students per section.\"),\n",
    "    (\"Can I audit the course?\", \"Yes, auditing is available but you won't get credit.\"),\n",
    "    (\"What's the cost?\", \"CS401 is $1,200 for credit, $300 for audit.\"),\n",
    "    (\"Are there scholarships?\", \"Yes, we offer merit-based scholarships. Apply early!\"),\n",
    "    (\"When should I apply?\", \"Applications open 2 months before each semester starts.\"),\n",
    "    (\"Thanks for the help!\", \"You're welcome! Feel free to reach out with more questions.\"),\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(conversation_turns)} conversation turns ({len(conversation_turns)*2} messages)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ffb17122f8392d4",
   "metadata": {},
   "source": [
    "#### Step 3: Add messages to working memory\n",
    "\n",
    "The Agent Memory Server will automatically monitor and summarize when thresholds are exceeded.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "616f864b1ca7e3e9",
   "metadata": {},
   "source": [
    "# Get or create working memory\n",
    "_, working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id=test_session_id,\n",
    "    user_id=test_student_id,\n",
    "    model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(\"Adding messages to working memory...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (user_msg, assistant_msg) in enumerate(conversation_turns, 1):\n",
    "    # Add messages to working memory\n",
    "    working_memory.messages.extend([\n",
    "        MemoryMessage(role=\"user\", content=user_msg),\n",
    "        MemoryMessage(role=\"assistant\", content=assistant_msg)\n",
    "    ])\n",
    "\n",
    "    # Save to Memory Server\n",
    "    await memory_client.put_working_memory(\n",
    "        session_id=test_session_id,\n",
    "        memory=working_memory,\n",
    "        user_id=test_student_id,\n",
    "        model_name=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "    # Show progress every 5 turns\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Turn {i:2d}: Added messages (total: {i*2} messages)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Added {len(conversation_turns)} turns ({len(conversation_turns)*2} messages)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2bb3077767449b7f",
   "metadata": {},
   "source": "#### Step 4: Retrieve working memory and check for summarization\n"
  },
  {
   "cell_type": "code",
   "id": "82277a6148de91d5",
   "metadata": {},
   "source": [
    "# Retrieve the latest working memory\n",
    "_, working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id=test_session_id,\n",
    "    user_id=test_student_id,\n",
    "    model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(\"Working Memory Status:\")\n",
    "print(f\"  Messages in memory: {len(working_memory.messages)}\")\n",
    "print(f\"  Original messages added: {len(conversation_turns)*2}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3c5f37a5c9e80e",
   "metadata": {},
   "source": [
    "#### Step 5: Analyze the results\n",
    "\n",
    "**What we're checking:** Did the Agent Memory Server automatically detect the threshold and trigger summarization?\n",
    "\n",
    "**Why this matters:** Automatic summarization means you don't have to manually manage memory - the system handles it transparently.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb05f22688b4fc76",
   "metadata": {},
   "source": [
    "if len(working_memory.messages) < len(conversation_turns)*2:\n",
    "    print(\"\\n‚úÖ Automatic summarization occurred!\")\n",
    "    print(f\"  Compression: {len(conversation_turns)*2} ‚Üí {len(working_memory.messages)} messages\")\n",
    "\n",
    "    # Calculate compression ratio\n",
    "    compression_ratio = len(working_memory.messages) / (len(conversation_turns)*2)\n",
    "    print(f\"  Compression ratio: {compression_ratio:.2f}x (kept {compression_ratio*100:.0f}% of messages)\")\n",
    "\n",
    "    # Check for summary message\n",
    "    summary_messages = [msg for msg in working_memory.messages if '[SUMMARY]' in msg.content or msg.role == 'system']\n",
    "    if summary_messages:\n",
    "        print(f\"  Summary messages found: {len(summary_messages)}\")\n",
    "        print(f\"\\n  Summary preview:\")\n",
    "        for msg in summary_messages[:1]:  # Show first summary\n",
    "            content_preview = msg.content[:200].replace('\\n', ' ')\n",
    "            print(f\"  {content_preview}...\")\n",
    "\n",
    "        # Analyze what was preserved\n",
    "        recent_messages = [msg for msg in working_memory.messages if msg.role in ['user', 'assistant']]\n",
    "        print(f\"\\n  Recent messages preserved: {len(recent_messages)}\")\n",
    "        print(f\"  Strategy: Summary + recent messages (optimal for 'Lost in the Middle')\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No summarization yet (threshold not reached)\")\n",
    "    print(f\"  Current: {len(working_memory.messages)} messages\")\n",
    "    print(f\"  Threshold: 20 messages or 4000 tokens\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9563bb6e6e9916cd",
   "metadata": {},
   "source": [
    "#### Step 6: Calculate token savings and analyze efficiency\n",
    "\n",
    "**What we're measuring:** The economic and performance impact of summarization.\n",
    "\n",
    "**Why this matters:**\n",
    "- **Cost savings:** Fewer tokens = lower API costs\n",
    "- **Performance:** Smaller context = faster responses\n",
    "- **Quality:** Compressed context avoids \"Lost in the Middle\" problem\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "93514990c8c95dd0",
   "metadata": {},
   "source": [
    "original_tokens = sum(count_tokens(user_msg) + count_tokens(assistant_msg) for user_msg, assistant_msg in conversation_turns)\n",
    "current_tokens = sum(count_tokens(msg.content) for msg in working_memory.messages)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOKEN EFFICIENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Token Counts:\")\n",
    "print(f\"  Original tokens: {original_tokens:,}\")\n",
    "print(f\"  Current tokens: {current_tokens:,}\")\n",
    "\n",
    "if current_tokens < original_tokens:\n",
    "    savings = original_tokens - current_tokens\n",
    "    savings_pct = (savings / original_tokens) * 100\n",
    "\n",
    "    print(f\"\\nüí∞ Savings:\")\n",
    "    print(f\"  Token savings: {savings:,} tokens ({savings_pct:.1f}%)\")\n",
    "\n",
    "    # Calculate cost savings (GPT-4o pricing: $0.0025 per 1K input tokens)\n",
    "    cost_per_1k = 0.0025\n",
    "    original_cost = (original_tokens / 1000) * cost_per_1k\n",
    "    current_cost = (current_tokens / 1000) * cost_per_1k\n",
    "    cost_savings = original_cost - current_cost\n",
    "\n",
    "    print(f\"  Cost per query: ${original_cost:.4f} ‚Üí ${current_cost:.4f}\")\n",
    "    print(f\"  Cost savings: ${cost_savings:.4f} per query\")\n",
    "\n",
    "    # Extrapolate to scale\n",
    "    queries_per_day = 1000\n",
    "    daily_savings = cost_savings * queries_per_day\n",
    "    monthly_savings = daily_savings * 30\n",
    "\n",
    "    print(f\"\\nüìà At Scale (1,000 queries/day):\")\n",
    "    print(f\"  Daily savings: ${daily_savings:.2f}\")\n",
    "    print(f\"  Monthly savings: ${monthly_savings:.2f}\")\n",
    "    print(f\"  Annual savings: ${monthly_savings * 12:.2f}\")\n",
    "\n",
    "    print(f\"\\n‚ö° Performance Benefits:\")\n",
    "    print(f\"  Reduced latency: ~{savings_pct * 0.3:.0f}% faster (fewer tokens to process)\")\n",
    "    print(f\"  Better quality: Recent context at optimal position (end of context)\")\n",
    "    print(f\"  Avoids 'Lost in the Middle': Summary at beginning, recent at end\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Automatic memory management is working efficiently!\")\n",
    "else:\n",
    "    print(f\"\\n‚ÑπÔ∏è  No compression yet (within thresholds)\")\n",
    "    print(f\"  Waiting for: >20 messages OR >4000 tokens\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffb6c8258857ff8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 5: Decision Framework\n",
    "\n",
    "How do you choose which compression strategy to use? Let's build a decision framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ef50ce9bbbbee",
   "metadata": {},
   "source": [
    "### üî¨ Synthesizing Research into Practice\n",
    "\n",
    "Our decision framework synthesizes findings from all the research we've discussed:\n",
    "\n",
    "**From \"Lost in the Middle\" (Liu et al., 2023):**\n",
    "- Keep recent messages at the end (optimal position)\n",
    "- Avoid bloating the middle of context\n",
    "- **Implication:** All strategies should preserve recent context\n",
    "\n",
    "**From \"Recursive Summarization\" (Wang et al., 2023):**\n",
    "- Summarization enables long-term consistency\n",
    "- Works well for extended conversations\n",
    "- **Implication:** Use summarization for long, high-value conversations\n",
    "\n",
    "**From \"MemGPT\" (Packer et al., 2023):**\n",
    "- Different strategies for different memory tiers\n",
    "- Trade-offs between speed and quality\n",
    "- **Implication:** Match strategy to use case requirements\n",
    "\n",
    "**From Production Best Practices** (Anthropic, Vellum AI):\n",
    "- Consider latency, cost, and quality trade-offs\n",
    "- No one-size-fits-all solution\n",
    "- **Implication:** Build a decision framework based on requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe971d847887693",
   "metadata": {},
   "source": [
    "### Theory: Choosing the Right Strategy\n",
    "\n",
    "**Decision Factors:**\n",
    "\n",
    "1. **Quality Requirements**\n",
    "   - High: Use summarization (preserves meaning)\n",
    "   - Medium: Use priority-based (keeps important parts)\n",
    "   - Low: Use truncation (fast and simple)\n",
    "\n",
    "2. **Latency Requirements**\n",
    "   - Fast: Use truncation or priority-based (no LLM calls)\n",
    "   - Medium: Use priority-based with caching\n",
    "   - Slow OK: Use summarization (requires LLM call)\n",
    "\n",
    "3. **Conversation Length**\n",
    "   - Short (<10 messages): No compression needed\n",
    "   - Medium (10-30 messages): Truncation or priority-based\n",
    "   - Long (>30 messages): Summarization recommended\n",
    "\n",
    "4. **Cost Sensitivity**\n",
    "   - High: Use truncation or priority-based (no LLM costs)\n",
    "   - Medium: Use summarization with caching\n",
    "   - Low: Use summarization freely\n",
    "\n",
    "5. **Context Importance**\n",
    "   - Critical: Use summarization (preserves all important info)\n",
    "   - Important: Use priority-based (keeps high-value messages)\n",
    "   - Less critical: Use truncation (simple and fast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faed81c0b685fc2",
   "metadata": {},
   "source": [
    "### Building the Decision Framework\n",
    "\n",
    "Let's build a practical decision framework step-by-step.\n",
    "\n",
    "#### Step 1: Define the available strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ce5821bcfe60fd",
   "metadata": {},
   "source": [
    "from enum import Enum\n",
    "from typing import Literal\n",
    "\n",
    "class CompressionChoice(Enum):\n",
    "    \"\"\"Available compression strategies.\"\"\"\n",
    "    NONE = \"none\"\n",
    "    TRUNCATION = \"truncation\"\n",
    "    PRIORITY = \"priority\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "\n",
    "print(\"‚úÖ CompressionChoice enum defined\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "349a450bedb1648",
   "metadata": {},
   "source": [
    "#### Step 2: Create the decision function\n",
    "\n",
    "This function takes your requirements and recommends the best strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a38016f74c5b2ac",
   "metadata": {},
   "source": [
    "def choose_compression_strategy(\n",
    "    conversation_length: int,\n",
    "    token_count: int,\n",
    "    quality_requirement: Literal[\"high\", \"medium\", \"low\"],\n",
    "    latency_requirement: Literal[\"fast\", \"medium\", \"slow_ok\"],\n",
    "    cost_sensitivity: Literal[\"high\", \"medium\", \"low\"] = \"medium\"\n",
    ") -> CompressionChoice:\n",
    "    \"\"\"\n",
    "    Decision framework for choosing compression strategy.\n",
    "\n",
    "    Args:\n",
    "        conversation_length: Number of messages in conversation\n",
    "        token_count: Total token count\n",
    "        quality_requirement: How important is quality? (\"high\", \"medium\", \"low\")\n",
    "        latency_requirement: How fast must it be? (\"fast\", \"medium\", \"slow_ok\")\n",
    "        cost_sensitivity: How sensitive to costs? (\"high\", \"medium\", \"low\")\n",
    "\n",
    "    Returns:\n",
    "        CompressionChoice: Recommended strategy\n",
    "    \"\"\"\n",
    "    # No compression needed for short conversations\n",
    "    if token_count < 2000 and conversation_length < 10:\n",
    "        return CompressionChoice.NONE\n",
    "\n",
    "    # Fast requirement = no LLM calls\n",
    "    if latency_requirement == \"fast\":\n",
    "        if quality_requirement == \"high\":\n",
    "            return CompressionChoice.PRIORITY\n",
    "        else:\n",
    "            return CompressionChoice.TRUNCATION\n",
    "\n",
    "    # High cost sensitivity = avoid LLM calls\n",
    "    if cost_sensitivity == \"high\":\n",
    "        return CompressionChoice.PRIORITY if quality_requirement != \"low\" else CompressionChoice.TRUNCATION\n",
    "\n",
    "    # High quality + willing to wait = summarization\n",
    "    if quality_requirement == \"high\" and latency_requirement == \"slow_ok\":\n",
    "        return CompressionChoice.SUMMARIZATION\n",
    "\n",
    "    # Long conversations benefit from summarization\n",
    "    if conversation_length > 30 and quality_requirement != \"low\":\n",
    "        return CompressionChoice.SUMMARIZATION\n",
    "\n",
    "    # Medium quality = priority-based\n",
    "    if quality_requirement == \"medium\":\n",
    "        return CompressionChoice.PRIORITY\n",
    "\n",
    "    # Default to truncation for simple cases\n",
    "    return CompressionChoice.TRUNCATION\n",
    "\n",
    "print(\"‚úÖ Decision framework function defined\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6334d427d5d684f",
   "metadata": {},
   "source": [
    "### Demo 6: Test Decision Framework\n",
    "\n",
    "Let's test the decision framework with various scenarios.\n",
    "\n",
    "#### Step 1: Define test scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3bd77fd3ecf192aa",
   "metadata": {},
   "source": [
    "# Define test scenarios\n",
    "scenarios = [\n",
    "    # (length, tokens, quality, latency, cost, description)\n",
    "    (5, 1000, \"high\", \"fast\", \"medium\", \"Short conversation, high quality needed\"),\n",
    "    (15, 3000, \"high\", \"slow_ok\", \"low\", \"Medium conversation, quality critical\"),\n",
    "    (30, 8000, \"medium\", \"medium\", \"medium\", \"Long conversation, balanced needs\"),\n",
    "    (50, 15000, \"high\", \"slow_ok\", \"medium\", \"Very long, quality important\"),\n",
    "    (100, 30000, \"low\", \"fast\", \"high\", \"Extremely long, cost-sensitive\"),\n",
    "    (20, 5000, \"medium\", \"fast\", \"high\", \"Medium length, fast and cheap\"),\n",
    "    (40, 12000, \"high\", \"medium\", \"low\", \"Long conversation, quality focus\"),\n",
    "    (8, 1500, \"low\", \"fast\", \"high\", \"Short, simple case\"),\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(scenarios)} test scenarios\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c5e764e64120fc9",
   "metadata": {},
   "source": "#### Step 2: Run the decision framework on each scenario\n"
  },
  {
   "cell_type": "code",
   "id": "1d6df99d81af4f56",
   "metadata": {},
   "source": [
    "print(\"Decision Framework Test Results:\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Scenario':<45} {'Length':<8} {'Tokens':<10} {'Quality':<10} {'Latency':<10} {'Cost':<8} {'Strategy'}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for length, tokens, quality, latency, cost, description in scenarios:\n",
    "    strategy = choose_compression_strategy(length, tokens, quality, latency, cost)\n",
    "    print(f\"{description:<45} {length:<8} {tokens:<10,} {quality:<10} {latency:<10} {cost:<8} {strategy.value}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e02d6d98eb9063d",
   "metadata": {},
   "source": [
    "#### Key Insights from the Decision Framework\n",
    "\n",
    "**Pattern 1: Quality drives strategy choice**\n",
    "- High quality + willing to wait ‚Üí Summarization\n",
    "- Medium quality ‚Üí Priority-based\n",
    "- Low quality ‚Üí Truncation\n",
    "\n",
    "**Pattern 2: Latency constraints matter**\n",
    "- Fast requirement ‚Üí Avoid summarization (no LLM calls)\n",
    "- Slow OK ‚Üí Summarization is an option\n",
    "\n",
    "**Pattern 3: Cost sensitivity affects decisions**\n",
    "- High cost sensitivity ‚Üí Avoid summarization\n",
    "- Low cost sensitivity ‚Üí Summarization is preferred for quality\n",
    "\n",
    "**Pattern 4: Conversation length influences choice**\n",
    "- Short (<10 messages) ‚Üí Often no compression needed\n",
    "- Long (>30 messages) ‚Üí Summarization recommended for quality\n",
    "\n",
    "**Practical Recommendation:**\n",
    "- Start with priority-based for most production use cases\n",
    "- Use summarization for high-value, long conversations\n",
    "- Use truncation for real-time, cost-sensitive scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893572f70d4176e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üè≠ Part 6: Production Recommendations\n",
    "\n",
    "Based on all the research and techniques we've covered, here are production-ready recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7e0bcdc28deb7",
   "metadata": {},
   "source": [
    "### Recommendation 1: For Most Applications (Balanced)\n",
    "\n",
    "**Strategy:** Agent Memory Server with automatic summarization\n",
    "\n",
    "**Configuration:**\n",
    "- `message_threshold`: 20 messages\n",
    "- `token_threshold`: 4000 tokens\n",
    "- `keep_recent`: 4 messages\n",
    "- `strategy`: \"recent_plus_summary\"\n",
    "\n",
    "**Why:** Automatic, transparent, production-ready. Implements research-backed strategies (Liu et al., Wang et al., Packer et al.) with minimal code.\n",
    "\n",
    "**Best for:** General-purpose chatbots, customer support, educational assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344c560b4d42889",
   "metadata": {},
   "source": [
    "### Recommendation 2: For High-Volume, Cost-Sensitive (Efficient)\n",
    "\n",
    "**Strategy:** Priority-based compression\n",
    "\n",
    "**Configuration:**\n",
    "- `max_tokens`: 2000\n",
    "- Custom importance scoring\n",
    "- No LLM calls\n",
    "\n",
    "**Why:** Fast, cheap, no external dependencies. Preserves important messages without LLM costs.\n",
    "\n",
    "**Best for:** High-traffic applications, real-time systems, cost-sensitive deployments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489db7cfc60769a",
   "metadata": {},
   "source": [
    "### Recommendation 3: For Critical Conversations (Quality)\n",
    "\n",
    "**Strategy:** Manual summarization with review\n",
    "\n",
    "**Configuration:**\n",
    "- `token_threshold`: 5000\n",
    "- Human review of summaries\n",
    "- Store full conversation separately\n",
    "\n",
    "**Why:** Maximum quality, human oversight. Critical for high-stakes conversations.\n",
    "\n",
    "**Best for:** Medical consultations, legal advice, financial planning, therapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3e70ff326b867",
   "metadata": {},
   "source": [
    "### Recommendation 4: For Real-Time Chat (Speed)\n",
    "\n",
    "**Strategy:** Truncation with sliding window\n",
    "\n",
    "**Configuration:**\n",
    "- `keep_recent`: 10 messages\n",
    "- No summarization\n",
    "- Fast response required\n",
    "\n",
    "**Why:** Minimal latency, simple implementation. Prioritizes speed over context preservation.\n",
    "\n",
    "**Best for:** Live chat, gaming, real-time collaboration tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516c43cb73d0441",
   "metadata": {},
   "source": [
    "### General Guidelines\n",
    "\n",
    "**Getting Started:**\n",
    "1. Start with Agent Memory Server automatic summarization\n",
    "2. Monitor token usage and costs in production\n",
    "3. Adjust thresholds based on your use case\n",
    "\n",
    "**Advanced Optimization:**\n",
    "4. Consider hybrid approaches (truncation + summarization)\n",
    "5. Always preserve critical information in long-term memory\n",
    "6. Use the decision framework to adapt to different conversation types\n",
    "\n",
    "**Monitoring:**\n",
    "7. Track compression ratios and token savings\n",
    "8. Monitor user satisfaction and conversation quality\n",
    "9. A/B test different strategies for your use case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa20b8bb77b5767c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí™ Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to reinforce your learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed098207acb2ac62",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Adaptive Compression Strategy\n",
    "\n",
    "Create a strategy that automatically chooses between truncation and sliding window based on message token variance:\n",
    "\n",
    "```python\n",
    "class AdaptiveStrategy(CompressionStrategy):\n",
    "    \"\"\"\n",
    "    Automatically choose between truncation and sliding window.\n",
    "\n",
    "    Logic:\n",
    "    - If messages have similar token counts ‚Üí use sliding window (predictable)\n",
    "    - If messages have varying token counts ‚Üí use truncation (token-aware)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int = 10):\n",
    "        self.window_size = window_size\n",
    "        self.truncation = TruncationStrategy()\n",
    "        self.sliding_window = SlidingWindowStrategy(window_size)\n",
    "\n",
    "    def compress(\n",
    "        self,\n",
    "        messages: List[ConversationMessage],\n",
    "        max_tokens: int\n",
    "    ) -> List[ConversationMessage]:\n",
    "        \"\"\"\n",
    "        Choose strategy based on token variance.\n",
    "\n",
    "        Steps:\n",
    "        1. Calculate token count variance across messages\n",
    "        2. If variance is low (similar sizes) ‚Üí use sliding window\n",
    "        3. If variance is high (varying sizes) ‚Üí use truncation\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "adaptive = AdaptiveStrategy(window_size=6)\n",
    "result = adaptive.compress(sample_conversation, max_tokens=800)\n",
    "print(f\"Adaptive strategy result: {len(result)} messages\")\n",
    "```\n",
    "\n",
    "**Hint:** Calculate variance using `statistics.variance([msg.token_count for msg in messages])`. Use a threshold (e.g., 100) to decide.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 2: Implement Hybrid Compression\n",
    "\n",
    "Combine summarization + truncation for optimal results:\n",
    "\n",
    "```python\n",
    "async def compress_hybrid(\n",
    "    messages: List[ConversationMessage],\n",
    "    summarizer: ConversationSummarizer,\n",
    "    max_tokens: int = 2000\n",
    ") -> List[ConversationMessage]:\n",
    "    \"\"\"\n",
    "    Hybrid compression: Summarize old messages, truncate if still too large.\n",
    "\n",
    "    Steps:\n",
    "    1. First, try summarization\n",
    "    2. If still over budget, apply truncation to summary + recent messages\n",
    "    3. Ensure we stay within max_tokens\n",
    "\n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        summarizer: ConversationSummarizer instance\n",
    "        max_tokens: Maximum token budget\n",
    "\n",
    "    Returns:\n",
    "        Compressed messages within token budget\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "hybrid_result = await compress_hybrid(sample_conversation, summarizer, max_tokens=1000)\n",
    "print(f\"Hybrid compression: {len(hybrid_result)} messages, {sum(m.token_count for m in hybrid_result)} tokens\")\n",
    "```\n",
    "\n",
    "**Hint:** Use `summarizer.compress_conversation()` first, then apply truncation if needed.\n"
   ],
   "id": "84a03030232b3364"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 3: Quality Comparison\n",
    "\n",
    "Test all compression strategies and compare quality:\n",
    "\n",
    "```python\n",
    "async def compare_compression_quality(\n",
    "    messages: List[ConversationMessage],\n",
    "    test_query: str = \"What courses did we discuss?\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare compression strategies by testing reference resolution.\n",
    "\n",
    "    Steps:\n",
    "    1. Compress using each strategy\n",
    "    2. Try to answer test_query using compressed context\n",
    "    3. Compare quality of responses\n",
    "    4. Measure token savings\n",
    "\n",
    "    Args:\n",
    "        messages: Original conversation\n",
    "        test_query: Question to test reference resolution\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    # Test if the agent can still answer questions after compression\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "quality_results = await compare_compression_quality(sample_conversation)\n",
    "print(\"Quality Comparison Results:\")\n",
    "for strategy, results in quality_results.items():\n",
    "    print(f\"{strategy}: {results}\")\n",
    "```\n",
    "\n",
    "**Hint:** Use the LLM to answer the test query with each compressed context and compare responses.\n"
   ],
   "id": "6ac899a501122c38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 4: Custom Importance Scoring\n",
    "\n",
    "Improve the `calculate_importance()` function with domain-specific logic:\n",
    "\n",
    "```python\n",
    "def calculate_importance_enhanced(msg: ConversationMessage) -> float:\n",
    "    \"\"\"\n",
    "    Enhanced importance scoring for course advisor conversations.\n",
    "\n",
    "    Add scoring for:\n",
    "    - Specific course codes (CS401, MATH301, etc.) - HIGH\n",
    "    - Prerequisites and requirements - HIGH\n",
    "    - Student preferences and goals - HIGH\n",
    "    - Questions - MEDIUM\n",
    "    - Confirmations and acknowledgments - LOW\n",
    "    - Greetings and small talk - VERY LOW\n",
    "\n",
    "    Returns:\n",
    "        Importance score (0.0 to 5.0)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "for msg in sample_conversation[:5]:\n",
    "    score = calculate_importance_enhanced(msg)\n",
    "    print(f\"Score: {score:.1f} - {msg.content[:60]}...\")\n",
    "```\n",
    "\n",
    "**Hint:** Use regex to detect course codes, check for question marks, look for keywords.\n"
   ],
   "id": "b134bf5336e3ae36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 5: Production Configuration\n",
    "\n",
    "Configure Agent Memory Server for your specific use case:\n",
    "\n",
    "```python\n",
    "# Scenario: High-volume customer support chatbot\n",
    "# Requirements:\n",
    "# - Handle 1000+ conversations per day\n",
    "# - Average conversation: 15-20 turns\n",
    "# - Cost-sensitive but quality important\n",
    "# - Response time: <2 seconds\n",
    "\n",
    "# Your task: Choose appropriate configuration\n",
    "production_config = {\n",
    "    \"message_threshold\": ???,  # When to trigger summarization\n",
    "    \"token_threshold\": ???,    # Token limit before summarization\n",
    "    \"keep_recent\": ???,        # How many recent messages to keep\n",
    "    \"strategy\": ???,           # Which strategy to use\n",
    "}\n",
    "\n",
    "# Justify your choices:\n",
    "print(\"Configuration Justification:\")\n",
    "print(f\"message_threshold: {production_config['message_threshold']} because...\")\n",
    "print(f\"token_threshold: {production_config['token_threshold']} because...\")\n",
    "print(f\"keep_recent: {production_config['keep_recent']} because...\")\n",
    "print(f\"strategy: {production_config['strategy']} because...\")\n",
    "```\n",
    "\n",
    "**Hint:** Consider the trade-offs between cost, quality, and latency for this specific scenario.\n"
   ],
   "id": "960cb21dcfe638cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### **What You Learned:**\n",
    "\n",
    "1. ‚úÖ **Research Foundations**\n",
    "   - \"Lost in the Middle\" (Liu et al., 2023): U-shaped performance, non-uniform degradation\n",
    "   - \"Recursive Summarization\" (Wang et al., 2023): Long-term dialogue memory\n",
    "   - \"MemGPT\" (Packer et al., 2023): Hierarchical memory management\n",
    "   - Production best practices from Anthropic and Vellum AI\n",
    "\n",
    "2. ‚úÖ **The Long Conversation Problem**\n",
    "   - Token limits, cost implications, performance degradation\n",
    "   - Why unbounded growth is unsustainable\n",
    "   - Quadratic cost growth without management\n",
    "   - Why larger context windows don't solve the problem\n",
    "\n",
    "3. ‚úÖ **Conversation Summarization**\n",
    "   - What to preserve vs. compress\n",
    "   - When to trigger summarization (token/message thresholds)\n",
    "   - Building summarization step-by-step (functions ‚Üí class)\n",
    "   - LLM-based intelligent summarization\n",
    "\n",
    "4. ‚úÖ **Three Compression Strategies**\n",
    "   - **Truncation:** Fast, simple, loses context\n",
    "   - **Priority-based:** Balanced, intelligent, no LLM calls\n",
    "   - **Summarization:** High quality, preserves meaning, requires LLM\n",
    "   - Trade-offs between speed, quality, and cost\n",
    "\n",
    "5. ‚úÖ **Agent Memory Server Integration**\n",
    "   - Automatic summarization configuration\n",
    "   - Transparent memory management\n",
    "   - Production-ready solution implementing research findings\n",
    "   - Configurable thresholds and strategies\n",
    "\n",
    "6. ‚úÖ **Decision Framework**\n",
    "   - How to choose the right strategy\n",
    "   - Factors: quality, latency, cost, conversation length\n",
    "   - Production recommendations for different scenarios\n",
    "   - Hybrid approaches for optimal results\n",
    "\n",
    "### **What You Built:**\n",
    "\n",
    "- ‚úÖ `ConversationSummarizer` class for intelligent summarization\n",
    "- ‚úÖ Three compression strategy implementations (Truncation, Priority, Summarization)\n",
    "- ‚úÖ Decision framework for strategy selection\n",
    "- ‚úÖ Production configuration examples\n",
    "- ‚úÖ Comparison tools for evaluating strategies\n",
    "- ‚úÖ Token counting and cost analysis tools\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "üí° **\"Conversations grow unbounded without management\"**\n",
    "- Every turn adds tokens and cost\n",
    "- Eventually you'll hit limits\n",
    "- Costs grow quadratically (each turn includes all previous messages)\n",
    "\n",
    "üí° **\"Summarization preserves meaning while reducing tokens\"**\n",
    "- Use LLM to create intelligent summaries\n",
    "- Keep recent messages for immediate context\n",
    "- Store important facts in long-term memory\n",
    "\n",
    "üí° **\"Choose strategy based on requirements\"**\n",
    "- Quality-critical ‚Üí Summarization\n",
    "- Speed-critical ‚Üí Truncation or Priority-based\n",
    "- Balanced ‚Üí Agent Memory Server automatic\n",
    "- Cost-sensitive ‚Üí Priority-based\n",
    "\n",
    "üí° **\"Agent Memory Server handles this automatically\"**\n",
    "- Production-ready solution\n",
    "- Transparent to your application\n",
    "- Configurable for your needs\n",
    "- No manual intervention required\n",
    "\n",
    "### **Connection to Context Engineering:**\n",
    "\n",
    "This notebook completes the **Conversation Context** story from Section 1:\n",
    "\n",
    "1. **Section 1:** Introduced the 4 context types, including Conversation Context\n",
    "2. **Section 3, NB1:** Implemented working memory for conversation continuity\n",
    "3. **Section 3, NB2:** Integrated memory with RAG for stateful conversations\n",
    "4. **Section 3, NB3:** Managed long conversations with summarization and compression ‚Üê You are here\n",
    "\n",
    "**Next:** Section 4 will show how agents can actively manage their own memory using tools!\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "**Section 4: Tools and Agents**\n",
    "- Build agents that actively manage their own memory\n",
    "- Implement memory tools (store, search, retrieve)\n",
    "- Use LangGraph for agent workflows\n",
    "- Let the LLM decide when to summarize\n",
    "\n",
    "**Section 5: Production Optimization**\n",
    "- Performance measurement and monitoring\n",
    "- Hybrid retrieval strategies\n",
    "- Semantic tool selection\n",
    "- Quality assurance and validation\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Resources\n",
    "\n",
    "### **Documentation:**\n",
    "- [Agent Memory Server](https://github.com/redis/agent-memory-server) - Production memory management\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client library\n",
    "- [LangChain Memory](https://python.langchain.com/docs/modules/memory/) - Memory patterns\n",
    "- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Token counting tool\n",
    "- [tiktoken](https://github.com/openai/tiktoken) - Fast token counting library\n",
    "\n",
    "### **Research Papers:**\n",
    "- **[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)** - Liu et al. (2023). Shows U-shaped performance curve and non-uniform degradation in long contexts.\n",
    "- **[Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022)** - Wang et al. (2023). Demonstrates recursive summarization for long conversations.\n",
    "- **[MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)** - Packer et al. (2023). Introduces hierarchical memory management and virtual context.\n",
    "- [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401) - RAG fundamentals\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer architecture and context windows\n",
    "\n",
    "### **Industry Resources:**\n",
    "- **[How Should I Manage Memory for my LLM Chatbot?](https://www.vellum.ai/blog/how-should-i-manage-memory-for-my-llm-chatbot)** - Vellum AI. Practical insights on memory management trade-offs.\n",
    "- **[Lost in the Middle Paper Reading](https://arize.com/blog/lost-in-the-middle-how-language-models-use-long-contexts-paper-reading/)** - Arize AI. Detailed analysis and practical implications.\n",
    "- **[Effective Context Engineering for AI Agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)** - Anthropic. Production best practices.\n",
    "\n",
    "\n",
    "### **Tools and Libraries:**\n",
    "- **Redis:** Vector storage and memory backend\n",
    "- **Agent Memory Server:** Dual-memory architecture with automatic summarization\n",
    "- **LangChain:** LLM interaction framework\n",
    "- **LangGraph:** State management and agent workflows\n",
    "- **OpenAI:** GPT-4o for generation and summarization\n",
    "- **tiktoken:** Token counting for cost estimation\n",
    "\n",
    "---\n",
    "\n",
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "**Redis University - Context Engineering Course**\n",
    "\n",
    "**üéâ Congratulations!** You've completed Section 3: Memory Architecture!\n",
    "\n",
    "You now understand how to:\n",
    "- Build memory systems for AI agents\n",
    "- Integrate working and long-term memory\n",
    "- Manage long conversations with summarization\n",
    "- Choose the right compression strategy\n",
    "- Configure production-ready memory management\n",
    "\n",
    "**Ready for Section 4?** Let's build agents that actively manage their own memory using tools!\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "9184f7251934a320"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "37206838f616911a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

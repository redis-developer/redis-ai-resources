{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20a2adc4d119d62",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# üß† Section 4: Memory Tools and LangGraph Fundamentals\n",
    "\n",
    "**‚è±Ô∏è Estimated Time:** 45-60 minutes\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand** how memory tools enable active context engineering\n",
    "2. **Build** the three essential memory tools: store, search, and retrieve\n",
    "3. **Learn** LangGraph fundamentals (nodes, edges, state)\n",
    "4. **Compare** passive vs active memory management\n",
    "5. **Prepare** for building a full course advisor agent\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Bridge from Previous Sections\n",
    "\n",
    "### **What You've Learned:**\n",
    "\n",
    "**Section 1:** Context Types\n",
    "- System, User, Conversation, Retrieved context\n",
    "- How context shapes LLM responses\n",
    "\n",
    "**Section 2:** RAG Foundations\n",
    "- Semantic search with vector embeddings\n",
    "- Retrieving relevant information\n",
    "- Context assembly and generation\n",
    "\n",
    "**Section 3:** Memory Architecture\n",
    "- Working memory for conversation continuity\n",
    "- Long-term memory for persistent knowledge\n",
    "- Memory-enhanced RAG systems\n",
    "\n",
    "### **What's Next: Memory Tools for Context Engineering**\n",
    "\n",
    "**Section 3 Approach:**\n",
    "- Memory operations hardcoded in your application flow\n",
    "- You explicitly call `get_working_memory()`, `search_long_term_memory()`, etc.\n",
    "- Fixed sequence: load ‚Üí search ‚Üí generate ‚Üí save\n",
    "\n",
    "**Section 4 Approach (This Section):**\n",
    "- LLM decides when to use memory tools\n",
    "- LLM chooses what information to store and retrieve\n",
    "- Dynamic decision-making based on conversation context\n",
    "\n",
    "**üí° Key Insight:** Memory tools let the LLM actively decide when to use memory, rather than having it hardcoded\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Memory Tools: The Context Engineering Connection\n",
    "\n",
    "**Why memory tools matter for context engineering:**\n",
    "\n",
    "Recall the **four context types** from Section 1:\n",
    "1. **System Context** (static instructions)\n",
    "2. **User Context** (profile, preferences) ‚Üê **Memory tools help build this**\n",
    "3. **Conversation Context** (session history) ‚Üê **Memory tools help manage this**\n",
    "4. **Retrieved Context** (RAG results)\n",
    "\n",
    "**Memory tools enable dynamic context construction:**\n",
    "\n",
    "### **Section 3 Approach:**\n",
    "```python\n",
    "# Hardcoded in application flow\n",
    "async def memory_enhanced_rag_query(user_query, session_id, student_id):\n",
    "    working_memory = await memory_client.get_working_memory(...)\n",
    "    long_term_facts = await memory_client.search_long_term_memory(...)\n",
    "    # ... fixed sequence of operations\n",
    "```\n",
    "\n",
    "### **Section 4 Approach (This Section):**\n",
    "```python\n",
    "# LLM decides when to use tools\n",
    "@tool\n",
    "def store_memory(text: str):\n",
    "    \"\"\"Store important information in long-term memory.\"\"\"\n",
    "\n",
    "@tool\n",
    "def search_memories(query: str):\n",
    "    \"\"\"Search long-term memory for relevant facts.\"\"\"\n",
    "\n",
    "# LLM calls these tools when it determines they're needed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß The Three Essential Memory Tools\n",
    "\n",
    "### **1. `store_memory` - Save Important Information**\n",
    "\n",
    "**When to use:**\n",
    "- User shares preferences, goals, constraints\n",
    "- Important facts emerge during conversation\n",
    "- Context that should persist across sessions\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"I prefer online courses because I work full-time\"\n",
    "Agent: [Thinks: \"This is important context I should remember\"]\n",
    "Agent: [Calls: store_memory(\"User prefers online courses due to full-time work\")]\n",
    "Agent: \"I'll remember your preference for online courses...\"\n",
    "```\n",
    "\n",
    "### **2. `search_memories` - Find Relevant Past Information**\n",
    "\n",
    "**When to use:**\n",
    "- Need context about user's history or preferences\n",
    "- User asks about past conversations\n",
    "- Building personalized responses\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"What courses should I take next semester?\"\n",
    "Agent: [Thinks: \"I need to know their preferences and past courses\"]\n",
    "Agent: [Calls: search_memories(\"course preferences major interests completed\")]\n",
    "Memory: \"User is CS major, interested in AI, prefers online, completed CS101\"\n",
    "Agent: \"Based on your CS major and AI interest...\"\n",
    "```\n",
    "\n",
    "### **3. `retrieve_memories` - Get Specific Stored Facts**\n",
    "\n",
    "**When to use:**\n",
    "- Need to recall exact details from past conversations\n",
    "- User references something specific they mentioned before\n",
    "- Verifying stored information\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"What was that GPA requirement we discussed?\"\n",
    "Agent: [Calls: retrieve_memories(\"GPA requirement graduation\")]\n",
    "Memory: \"User needs 3.5 GPA for honors program admission\"\n",
    "Agent: \"You mentioned needing a 3.5 GPA for the honors program\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Setup and Environment\n",
    "\n",
    "### ‚ö†Ô∏è **IMPORTANT: Prerequisites Required**\n",
    "\n",
    "**Before running this notebook, you MUST have:**\n",
    "\n",
    "1. **Redis running** on port 6379\n",
    "2. **Agent Memory Server running** on port 8088  \n",
    "3. **OpenAI API key** configured\n",
    "\n",
    "**üöÄ Quick Setup:**\n",
    "```bash\n",
    "# Navigate to notebooks_v2 directory\n",
    "cd ../../\n",
    "\n",
    "# Check if services are running\n",
    "./check_setup.sh\n",
    "\n",
    "# If services are down, run setup\n",
    "./setup_memory_server.sh\n",
    "```\n",
    "\n",
    "**üìñ Detailed Setup:** See `../SETUP_GUIDE.md` for complete instructions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_packages",
   "metadata": {},
   "source": [
    "### Automated Setup Check\n",
    "\n",
    "Let's run the setup script to ensure all services are running properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env_setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:43.326206Z",
     "iopub.status.busy": "2025-11-01T00:27:43.326021Z",
     "iopub.status.idle": "2025-11-01T00:27:43.597828Z",
     "shell.execute_reply": "2025-11-01T00:27:43.597284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running automated setup check...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Agent Memory Server Setup\n",
      "===========================\n",
      "üìä Checking Redis...\n",
      "‚úÖ Redis is running\n",
      "üìä Checking Agent Memory Server...\n",
      "üîç Agent Memory Server container exists. Checking health...\n",
      "‚úÖ Agent Memory Server is running and healthy\n",
      "‚úÖ No Redis connection issues detected\n",
      "\n",
      "‚úÖ Setup Complete!\n",
      "=================\n",
      "üìä Services Status:\n",
      "   ‚Ä¢ Redis: Running on port 6379\n",
      "   ‚Ä¢ Agent Memory Server: Running on port 8088\n",
      "\n",
      "üéØ You can now run the notebooks!\n",
      "\n",
      "\n",
      "‚úÖ All services are ready!\n"
     ]
    }
   ],
   "source": [
    "# Run the setup script to ensure Redis and Agent Memory Server are running\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to setup script\n",
    "setup_script = Path(\"../../reference-agent/setup_agent_memory_server.py\")\n",
    "\n",
    "if setup_script.exists():\n",
    "    print(\"Running automated setup check...\\n\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(setup_script)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"‚ö†Ô∏è  Setup check failed. Please review the output above.\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All services are ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Setup script not found. Please ensure services are running manually.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_config",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "services_check",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already installed the reference-agent package, uncomment and run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "health_check",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:43.599247Z",
     "iopub.status.busy": "2025-11-01T00:27:43.599160Z",
     "iopub.status.idle": "2025-11-01T00:27:43.600994Z",
     "shell.execute_reply": "2025-11-01T00:27:43.600510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install reference-agent package\n",
    "# %pip install -q -e ../../reference-agent\n",
    "\n",
    "# Uncomment to install agent-memory-client\n",
    "# %pip install -q agent-memory-client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_tools_intro",
   "metadata": {},
   "source": [
    "### Environment Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "memory_client_init",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:43.602048Z",
     "iopub.status.busy": "2025-11-01T00:27:43.601982Z",
     "iopub.status.idle": "2025-11-01T00:27:43.607235Z",
     "shell.execute_reply": "2025-11-01T00:27:43.606871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured successfully!\n",
      "   OpenAI Model: gpt-4o\n",
      "   Redis URL: redis://localhost:6379\n",
      "   Memory Server: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../../reference-agent/.env\")\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = {\n",
    "    \"OPENAI_API_KEY\": \"OpenAI API key for LLM\",\n",
    "    \"REDIS_URL\": \"Redis connection for vector storage\",\n",
    "    \"AGENT_MEMORY_URL\": \"Agent Memory Server for memory tools\"\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "for var, description in required_vars.items():\n",
    "    if not os.getenv(var):\n",
    "        missing_vars.append(f\"  - {var}: {description}\")\n",
    "\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"\"\"\n",
    "    ‚ö†Ô∏è  Missing required environment variables:\n",
    "    \n",
    "{''.join(missing_vars)}\n",
    "    \n",
    "    Please create a .env file in the reference-agent directory:\n",
    "    1. cd ../../reference-agent\n",
    "    2. cp .env.example .env\n",
    "    3. Edit .env and add your API keys\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úÖ Environment configured successfully!\")\n",
    "print(f\"   OpenAI Model: {os.getenv('OPENAI_MODEL', 'gpt-4o')}\")\n",
    "print(f\"   Redis URL: {os.getenv('REDIS_URL', 'redis://localhost:6379')}\")\n",
    "print(f\"   Memory Server: {os.getenv('AGENT_MEMORY_URL', 'http://localhost:8088')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool_1_store",
   "metadata": {},
   "source": [
    "### Service Health Check\n",
    "\n",
    "Before building memory tools, let's verify that Redis and the Agent Memory Server are running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "store_memory_tool",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:43.608506Z",
     "iopub.status.busy": "2025-11-01T00:27:43.608428Z",
     "iopub.status.idle": "2025-11-01T00:27:43.659756Z",
     "shell.execute_reply": "2025-11-01T00:27:43.659439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking required services...\n",
      "\n",
      "Redis: ‚úÖ Connected successfully\n",
      "Agent Memory Server: ‚úÖ Status: 200\n",
      "\n",
      "‚úÖ All services are running!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import redis\n",
    "\n",
    "def check_redis():\n",
    "    \"\"\"Check if Redis is accessible.\"\"\"\n",
    "    try:\n",
    "        r = redis.from_url(os.getenv(\"REDIS_URL\", \"redis://localhost:6379\"))\n",
    "        r.ping()\n",
    "        return True, \"Connected successfully\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def check_memory_server():\n",
    "    \"\"\"Check if Agent Memory Server is accessible.\"\"\"\n",
    "    try:\n",
    "        url = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "        response = requests.get(f\"{url}/v1/health\", timeout=5)\n",
    "        return response.status_code == 200, f\"Status: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Check services\n",
    "print(\"üîç Checking required services...\\n\")\n",
    "\n",
    "redis_ok, redis_msg = check_redis()\n",
    "print(f\"Redis: {'‚úÖ' if redis_ok else '‚ùå'} {redis_msg}\")\n",
    "\n",
    "memory_ok, memory_msg = check_memory_server()\n",
    "print(f\"Agent Memory Server: {'‚úÖ' if memory_ok else '‚ùå'} {memory_msg}\")\n",
    "\n",
    "if not (redis_ok and memory_ok):\n",
    "    print(\"\\n‚ö†Ô∏è  Some services are not running. Please start them:\")\n",
    "    if not redis_ok:\n",
    "        print(\"   Redis: docker run -d -p 6379:6379 redis/redis-stack:latest\")\n",
    "    if not memory_ok:\n",
    "        print(\"   Memory Server: cd ../../reference-agent && python setup_agent_memory_server.py\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All services are running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool_2_search",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Building Memory Tools\n",
    "\n",
    "Now let's build the three essential memory tools. We'll start simple and build up complexity.\n",
    "\n",
    "### **Step 1: Initialize Memory Client**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "search_memories_tool",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:43.661063Z",
     "iopub.status.busy": "2025-11-01T00:27:43.660992Z",
     "iopub.status.idle": "2025-11-01T00:27:43.778969Z",
     "shell.execute_reply": "2025-11-01T00:27:43.778555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory client initialized\n",
      "   Base URL: http://localhost:8088\n",
      "   Namespace: redis_university\n",
      "   Test User: student_memory_tools_demo\n"
     ]
    }
   ],
   "source": [
    "from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "from agent_memory_client.models import ClientMemoryRecord\n",
    "from agent_memory_client.filters import UserId\n",
    "\n",
    "# Initialize memory client\n",
    "config = MemoryClientConfig(\n",
    "    base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\"),\n",
    "    default_namespace=\"redis_university\"\n",
    ")\n",
    "memory_client = MemoryAPIClient(config=config)\n",
    "\n",
    "# Test user for this notebook\n",
    "test_user_id = \"student_memory_tools_demo\"\n",
    "test_session_id = \"session_memory_tools_demo\"\n",
    "\n",
    "print(f\"‚úÖ Memory client initialized\")\n",
    "print(f\"   Base URL: {config.base_url}\")\n",
    "print(f\"   Namespace: {config.default_namespace}\")\n",
    "print(f\"   Test User: {test_user_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool_3_retrieve",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Understanding Tools in LLM Applications\n",
    "\n",
    "### **What Are Tools?**\n",
    "\n",
    "**Tools** are functions that LLMs can call to interact with external systems, retrieve information, or perform actions beyond text generation.\n",
    "\n",
    "**Think of tools as:**\n",
    "- üîå **Extensions** to the LLM's capabilities\n",
    "- ü§ù **Interfaces** between the LLM and external systems\n",
    "- üéØ **Actions** the LLM can take to accomplish tasks\n",
    "\n",
    "### **How Tool Calling Works**\n",
    "\n",
    "```\n",
    "1. User Input ‚Üí \"Store my preference for online courses\"\n",
    "                 ‚Üì\n",
    "2. LLM Analysis ‚Üí Decides: \"I need to use store_memory tool\"\n",
    "                 ‚Üì\n",
    "3. Tool Call ‚Üí Returns structured function call with arguments\n",
    "                 ‚Üì\n",
    "4. Tool Execution ‚Üí Your code executes the function\n",
    "                 ‚Üì\n",
    "5. Tool Result ‚Üí Returns result to LLM\n",
    "                 ‚Üì\n",
    "6. LLM Response ‚Üí Generates final text response using tool result\n",
    "```\n",
    "\n",
    "### **Tool Definition Components**\n",
    "\n",
    "Every tool needs three key components:\n",
    "\n",
    "**1. Input Schema (Pydantic Model)**\n",
    "```python\n",
    "class StoreMemoryInput(BaseModel):\n",
    "    text: str = Field(description=\"What to store\")\n",
    "    memory_type: str = Field(default=\"semantic\")\n",
    "    topics: List[str] = Field(default=[])\n",
    "```\n",
    "- Defines what parameters the tool accepts\n",
    "- Provides descriptions that help the LLM understand usage\n",
    "- Validates input types\n",
    "\n",
    "**2. Tool Function**\n",
    "```python\n",
    "@tool(\"store_memory\", args_schema=StoreMemoryInput)\n",
    "async def store_memory(text: str, memory_type: str = \"semantic\", topics: List[str] = None) -> str:\n",
    "    # Implementation\n",
    "    return \"Success message\"\n",
    "```\n",
    "- The actual function that performs the action\n",
    "- Must return a string (the LLM reads this result)\n",
    "- Can be sync or async\n",
    "\n",
    "**3. Docstring (Critical!)**\n",
    "```python\n",
    "\"\"\"\n",
    "Store important information in long-term memory.\n",
    "\n",
    "Use this tool when:\n",
    "- User shares preferences, goals, or constraints\n",
    "- Important facts emerge during conversation\n",
    "\n",
    "Examples:\n",
    "- \"User prefers online courses\"\n",
    "- \"User is CS major interested in AI\"\n",
    "\"\"\"\n",
    "```\n",
    "- The LLM reads this to decide when to use the tool\n",
    "- Should include clear use cases and examples\n",
    "- More detailed = better tool selection\n",
    "\n",
    "### **Best Practices for Tool Design**\n",
    "\n",
    "#### **1. Clear, Descriptive Names**\n",
    "```python\n",
    "‚úÖ Good: store_memory, search_courses, get_user_profile\n",
    "‚ùå Bad: do_thing, process, handle_data\n",
    "```\n",
    "\n",
    "#### **2. Detailed Descriptions**\n",
    "```python\n",
    "‚úÖ Good: \"Store important user preferences and facts in long-term memory for future conversations\"\n",
    "‚ùå Bad: \"Stores data\"\n",
    "```\n",
    "\n",
    "#### **3. Specific Use Cases in Docstring**\n",
    "```python\n",
    "‚úÖ Good:\n",
    "\"\"\"\n",
    "Use this tool when:\n",
    "- User explicitly shares preferences\n",
    "- Important facts emerge that should persist\n",
    "- Information will be useful for future recommendations\n",
    "\"\"\"\n",
    "\n",
    "‚ùå Bad:\n",
    "\"\"\"\n",
    "Stores information.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### **4. Return Meaningful Results**\n",
    "```python\n",
    "‚úÖ Good: return f\"Stored: {text} with topics {topics}\"\n",
    "‚ùå Bad: return \"Done\"\n",
    "```\n",
    "The LLM uses the return value to understand what happened and craft its response.\n",
    "\n",
    "#### **5. Handle Errors Gracefully**\n",
    "```python\n",
    "‚úÖ Good:\n",
    "try:\n",
    "    result = await memory_client.create_long_term_memory([record])\n",
    "    return f\"Successfully stored: {text}\"\n",
    "except Exception as e:\n",
    "    return f\"Could not store memory: {str(e)}\"\n",
    "```\n",
    "Always return a string explaining what went wrong.\n",
    "\n",
    "#### **6. Keep Tools Focused**\n",
    "```python\n",
    "‚úÖ Good: Separate tools for store_memory, search_memories, retrieve_memories\n",
    "‚ùå Bad: One generic memory_operation(action, data) tool\n",
    "```\n",
    "Focused tools are easier for LLMs to select correctly.\n",
    "\n",
    "### **Common Tool Patterns**\n",
    "\n",
    "**Information Retrieval:**\n",
    "- Search databases\n",
    "- Query APIs\n",
    "- Fetch user data\n",
    "\n",
    "**Data Storage:**\n",
    "- Save preferences\n",
    "- Store conversation facts\n",
    "- Update user profiles\n",
    "\n",
    "**External Actions:**\n",
    "- Send emails\n",
    "- Create calendar events\n",
    "- Make API calls\n",
    "\n",
    "**Computation:**\n",
    "- Calculate values\n",
    "- Process data\n",
    "- Generate reports\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Build the `store_memory` Tool**\n",
    "\n",
    "Now let's build our first memory tool following these best practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "retrieve_memories_tool",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:43.780190Z",
     "iopub.status.busy": "2025-11-01T00:27:43.780108Z",
     "iopub.status.idle": "2025-11-01T00:27:43.876809Z",
     "shell.execute_reply": "2025-11-01T00:27:43.876383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Store Memory Test: Stored: User prefers online courses for testing\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class StoreMemoryInput(BaseModel):\n",
    "    \"\"\"Input schema for storing memories.\"\"\"\n",
    "    text: str = Field(\n",
    "        description=\"The information to store. Should be clear, specific, and important for future conversations.\"\n",
    "    )\n",
    "    memory_type: str = Field(\n",
    "        default=\"semantic\",\n",
    "        description=\"Type of memory: 'semantic' for facts/preferences, 'episodic' for events/experiences\"\n",
    "    )\n",
    "    topics: List[str] = Field(\n",
    "        default=[],\n",
    "        description=\"List of topics/tags for this memory (e.g., ['preferences', 'courses', 'career'])\"\n",
    "    )\n",
    "\n",
    "@tool(\"store_memory\", args_schema=StoreMemoryInput)\n",
    "async def store_memory(text: str, memory_type: str = \"semantic\", topics: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Store important information in long-term memory.\n",
    "    \n",
    "    Use this tool when:\n",
    "    - User shares preferences, goals, or constraints\n",
    "    - Important facts emerge during conversation\n",
    "    - Information should persist across sessions\n",
    "    - Context that will be useful for future recommendations\n",
    "    \n",
    "    Examples:\n",
    "    - \"User prefers online courses due to work schedule\"\n",
    "    - \"User is Computer Science major interested in AI\"\n",
    "    - \"User completed CS101 with grade A\"\n",
    "    \n",
    "    Returns: Confirmation that memory was stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create memory record\n",
    "        memory_record = ClientMemoryRecord(\n",
    "            text=text,\n",
    "            memory_type=memory_type,\n",
    "            topics=topics or [],\n",
    "            user_id=test_user_id\n",
    "        )\n",
    "        \n",
    "        # Store in long-term memory\n",
    "        await memory_client.create_long_term_memory([memory_record])\n",
    "        \n",
    "        return f\"Stored: {text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error storing memory: {str(e)}\"\n",
    "\n",
    "# Test the tool\n",
    "test_result = await store_memory.ainvoke({\n",
    "    \"text\": \"User prefers online courses for testing\",\n",
    "    \"memory_type\": \"semantic\",\n",
    "    \"topics\": [\"preferences\", \"test\"]\n",
    "})\n",
    "print(f\"üß† Store Memory Test: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_tools_demo",
   "metadata": {},
   "source": [
    "### **Step 3: Build the `search_memories` Tool**\n",
    "\n",
    "This tool allows the LLM to search its long-term memory for relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "llm_memory_demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:43.878136Z",
     "iopub.status.busy": "2025-11-01T00:27:43.878066Z",
     "iopub.status.idle": "2025-11-01T00:27:44.123430Z",
     "shell.execute_reply": "2025-11-01T00:27:44.122639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Search Memories Test: - User prefers online courses for testing\n",
      "- User is a Computer Science major interested in AI and machine learning. Prefers online courses due to part-time work.\n"
     ]
    }
   ],
   "source": [
    "class SearchMemoriesInput(BaseModel):\n",
    "    \"\"\"Input schema for searching memories.\"\"\"\n",
    "    query: str = Field(\n",
    "        description=\"Search query to find relevant memories. Use keywords related to what you need to know.\"\n",
    "    )\n",
    "    limit: int = Field(\n",
    "        default=5,\n",
    "        description=\"Maximum number of memories to return. Default is 5.\"\n",
    "    )\n",
    "\n",
    "@tool(\"search_memories\", args_schema=SearchMemoriesInput)\n",
    "async def search_memories(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search long-term memory for relevant information.\n",
    "    \n",
    "    Use this tool when:\n",
    "    - Need context about user's preferences or history\n",
    "    - User asks about past conversations\n",
    "    - Building personalized responses\n",
    "    - Need to recall what you know about the user\n",
    "    \n",
    "    Examples:\n",
    "    - query=\"course preferences\" ‚Üí finds preferred course types\n",
    "    - query=\"completed courses\" ‚Üí finds courses user has taken\n",
    "    - query=\"career goals\" ‚Üí finds user's career interests\n",
    "    \n",
    "    Returns: Relevant memories or \"No memories found\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Search long-term memory\n",
    "        results = await memory_client.search_long_term_memory(\n",
    "            text=query,\n",
    "            user_id=UserId(eq=test_user_id),\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "        if not results or not results.memories:\n",
    "            return \"No memories found matching your query.\"\n",
    "\n",
    "        # Format results\n",
    "        memory_texts = []\n",
    "        for memory in results.memories:\n",
    "            memory_texts.append(f\"- {memory.text}\")\n",
    "\n",
    "        return \"\\n\".join(memory_texts)\n",
    "    except Exception as e:\n",
    "        return f\"Error searching memories: {str(e)}\"\n",
    "\n",
    "# Test the tool\n",
    "test_result = await search_memories.ainvoke({\n",
    "    \"query\": \"preferences\",\n",
    "    \"limit\": 5\n",
    "})\n",
    "print(f\"üîç Search Memories Test: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph_intro",
   "metadata": {},
   "source": [
    "### **Step 4: Build the `retrieve_memories` Tool**\n",
    "\n",
    "This tool allows the LLM to retrieve specific stored facts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "passive_memory",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:44.125246Z",
     "iopub.status.busy": "2025-11-01T00:27:44.125103Z",
     "iopub.status.idle": "2025-11-01T00:27:44.331240Z",
     "shell.execute_reply": "2025-11-01T00:27:44.330413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Retrieve Memories Test: [preferences, test] User prefers online courses for testing\n",
      "[preferences, academic, career] User is a Computer Science major interested in AI and machine learning. Prefers online courses due to part-time work.\n"
     ]
    }
   ],
   "source": [
    "class RetrieveMemoriesInput(BaseModel):\n",
    "    \"\"\"Input schema for retrieving specific memories.\"\"\"\n",
    "    topics: List[str] = Field(\n",
    "        description=\"List of specific topics to retrieve (e.g., ['GPA', 'requirements', 'graduation'])\"\n",
    "    )\n",
    "    limit: int = Field(\n",
    "        default=3,\n",
    "        description=\"Maximum number of memories to return. Default is 3.\"\n",
    "    )\n",
    "\n",
    "@tool(\"retrieve_memories\", args_schema=RetrieveMemoriesInput)\n",
    "async def retrieve_memories(topics: List[str], limit: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve specific stored facts by topic.\n",
    "    \n",
    "    Use this tool when:\n",
    "    - Need to recall exact details from past conversations\n",
    "    - User references something specific they mentioned before\n",
    "    - Verifying stored information\n",
    "    - Looking for facts about specific topics\n",
    "    \n",
    "    Examples:\n",
    "    - topics=[\"GPA\", \"requirements\"] ‚Üí finds GPA-related memories\n",
    "    - topics=[\"completed\", \"courses\"] ‚Üí finds completed course records\n",
    "    - topics=[\"career\", \"goals\"] ‚Üí finds career-related memories\n",
    "    \n",
    "    Returns: Specific memories matching the topics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Search for memories with specific topics\n",
    "        query = \" \".join(topics)\n",
    "        results = await memory_client.search_long_term_memory(\n",
    "            text=query,\n",
    "            user_id=UserId(eq=test_user_id),\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "        if not results or not results.memories:\n",
    "            return f\"No memories found for topics: {', '.join(topics)}\"\n",
    "\n",
    "        # Format results with topics\n",
    "        memory_texts = []\n",
    "        for memory in results.memories:\n",
    "            topics_str = \", \".join(memory.topics) if memory.topics else \"general\"\n",
    "            memory_texts.append(f\"[{topics_str}] {memory.text}\")\n",
    "\n",
    "        return \"\\n\".join(memory_texts)\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving memories: {str(e)}\"\n",
    "\n",
    "# Test the tool\n",
    "test_result = await retrieve_memories.ainvoke({\n",
    "    \"topics\": [\"preferences\", \"test\"],\n",
    "    \"limit\": 3\n",
    "})\n",
    "print(f\"üìã Retrieve Memories Test: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active_memory",
   "metadata": {},
   "source": [
    "### **Step 5: Test Memory Tools with LLM**\n",
    "\n",
    "Now let's see how an LLM uses these memory tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "when_to_use",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:44.333737Z",
     "iopub.status.busy": "2025-11-01T00:27:44.333538Z",
     "iopub.status.idle": "2025-11-01T00:27:47.222368Z",
     "shell.execute_reply": "2025-11-01T00:27:47.221631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM Response:\n",
      "   Tool calls: 1\n",
      "   Tool 1: store_memory\n",
      "   Args: {'text': 'User is a Computer Science major interested in AI and machine learning. Prefers online courses due to part-time work.', 'memory_type': 'semantic', 'topics': ['preferences', 'academic', 'career']}\n",
      "\n",
      "üí¨ Response: \n",
      "\n",
      "üìù Note: The response is empty because the LLM decided to call a tool instead of\n",
      "   generating text. This is expected behavior! The LLM is saying:\n",
      "   'I need to store this information first, then I'll respond.'\n",
      "\n",
      "   To get the final response, we would need to:\n",
      "   1. Execute the tool call (store_memory)\n",
      "   2. Send the tool result back to the LLM\n",
      "   3. Get the LLM's final text response\n",
      "\n",
      "   This multi-step process is exactly why we need LangGraph! üëá\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize LLM with memory tools\n",
    "llm = ChatOpenAI(model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o\"), temperature=0)\n",
    "memory_tools = [store_memory, search_memories, retrieve_memories]\n",
    "llm_with_tools = llm.bind_tools(memory_tools)\n",
    "\n",
    "# System message for memory-aware agent\n",
    "system_prompt = \"\"\"\n",
    "You are a Redis University course advisor with memory tools.\n",
    "\n",
    "IMPORTANT: Use your memory tools strategically:\n",
    "- When users share preferences, goals, or important facts ‚Üí use store_memory\n",
    "- When you need context about the user ‚Üí use search_memories\n",
    "- When users reference specific past information ‚Üí use retrieve_memories\n",
    "\n",
    "Always explain what you're doing with memory to help users understand.\n",
    "\"\"\"\n",
    "\n",
    "# Test conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=\"Hi! I'm a Computer Science major interested in AI and machine learning. I prefer online courses because I work part-time.\")\n",
    "]\n",
    "\n",
    "response = llm_with_tools.invoke(messages)\n",
    "print(\"ü§ñ LLM Response:\")\n",
    "print(f\"   Tool calls: {len(response.tool_calls) if response.tool_calls else 0}\")\n",
    "if response.tool_calls:\n",
    "    for i, tool_call in enumerate(response.tool_calls):\n",
    "        print(f\"   Tool {i+1}: {tool_call['name']}\")\n",
    "        print(f\"   Args: {tool_call['args']}\")\n",
    "print(f\"\\nüí¨ Response: {response.content}\")\n",
    "\n",
    "# Explain the empty response\n",
    "if response.tool_calls and not response.content:\n",
    "    print(\"\\nüìù Note: The response is empty because the LLM decided to call a tool instead of\")\n",
    "    print(\"   generating text. This is expected behavior! The LLM is saying:\")\n",
    "    print(\"   'I need to store this information first, then I'll respond.'\")\n",
    "    print(\"\\n   To get the final response, we would need to:\")\n",
    "    print(\"   1. Execute the tool call (store_memory)\")\n",
    "    print(\"   2. Send the tool result back to the LLM\")\n",
    "    print(\"   3. Get the LLM's final text response\")\n",
    "    print(\"\\n   This multi-step process is exactly why we need LangGraph! üëá\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98556b-21bd-4578-8f8f-f316e8fe31f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Complete Tool Execution Loop Example\n",
    "\n",
    "Let's manually complete the tool execution loop to see the full workflow. This will help you understand what LangGraph automates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a7df9ffdf5bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T00:27:47.224544Z",
     "iopub.status.busy": "2025-11-01T00:27:47.224342Z",
     "iopub.status.idle": "2025-11-01T00:27:49.676939Z",
     "shell.execute_reply": "2025-11-01T00:27:49.676143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE TOOL EXECUTION LOOP - Manual Implementation\n",
      "================================================================================\n",
      "\n",
      "üë§ USER INPUT:\n",
      "Hi! I'm a Computer Science major interested in AI and machine learning. I prefer online courses because I work part-time.\n",
      "\n",
      "================================================================================\n",
      "STEP 1: LLM Analysis\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM decided to call: store_memory\n",
      "   Arguments: {'text': 'User is a Computer Science major interested in AI and machine learning. Prefers online courses due to part-time work.', 'memory_type': 'semantic', 'topics': ['preferences', 'academic', 'career']}\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Tool Execution\n",
      "================================================================================\n",
      "‚úÖ Tool executed successfully\n",
      "   Result: Stored: User is a Computer Science major interested in AI and machine learning. Prefers online courses due to part-time work.\n",
      "\n",
      "================================================================================\n",
      "STEP 3: LLM Generates Final Response\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final response generated\n",
      "\n",
      "ü§ñ AGENT RESPONSE:\n",
      "Great! I've noted that you're a Computer Science major interested in AI and machine learning, and you prefer online courses because you work part-time. If you have any specific questions or need recommendations, feel free to ask!\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Verify Memory Storage\n",
      "================================================================================\n",
      "‚úÖ Memory verification:\n",
      "- User prefers online courses for testing\n",
      "- User is a Computer Science major interested in AI and machine learning. Prefers online courses due to part-time work.\n",
      "\n",
      "================================================================================\n",
      "COMPLETE! This is what LangGraph automates for you.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPLETE TOOL EXECUTION LOOP - Manual Implementation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: User input\n",
    "user_message = \"Hi! I'm a Computer Science major interested in AI and machine learning. I prefer online courses because I work part-time.\"\n",
    "print(f\"\\nüë§ USER INPUT:\\n{user_message}\")\n",
    "\n",
    "# Step 2: LLM decides to use tool\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_message)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: LLM Analysis\")\n",
    "print(\"=\" * 80)\n",
    "response_1 = llm_with_tools.invoke(messages)\n",
    "print(f\"‚úÖ LLM decided to call: {response_1.tool_calls[0]['name']}\")\n",
    "print(f\"   Arguments: {response_1.tool_calls[0]['args']}\")\n",
    "\n",
    "# Step 3: Execute the tool\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Tool Execution\")\n",
    "print(\"=\" * 80)\n",
    "tool_call = response_1.tool_calls[0]\n",
    "tool_result = await store_memory.ainvoke(tool_call['args'])\n",
    "print(f\"‚úÖ Tool executed successfully\")\n",
    "print(f\"   Result: {tool_result}\")\n",
    "\n",
    "# Step 4: Send tool result back to LLM\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: LLM Generates Final Response\")\n",
    "print(\"=\" * 80)\n",
    "messages.append(response_1)  # Add the tool call message\n",
    "messages.append(ToolMessage(content=tool_result, tool_call_id=tool_call['id']))  # Add tool result\n",
    "\n",
    "response_2 = llm_with_tools.invoke(messages)\n",
    "print(f\"‚úÖ Final response generated\")\n",
    "print(f\"\\nü§ñ AGENT RESPONSE:\\n{response_2.content}\")\n",
    "\n",
    "# Step 5: Verify memory was stored\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: Verify Memory Storage\")\n",
    "print(\"=\" * 80)\n",
    "search_result = await search_memories.ainvoke({\"query\": \"preferences\", \"limit\": 3})\n",
    "print(f\"‚úÖ Memory verification:\")\n",
    "print(f\"{search_result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETE! This is what LangGraph automates for you.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13debf42a9b4b7",
   "metadata": {},
   "source": [
    "### **Key Takeaways from Manual Loop**\n",
    "\n",
    "**What we just did manually:**\n",
    "\n",
    "1. ‚úÖ **Sent user input to LLM** ‚Üí Got tool call decision\n",
    "2. ‚úÖ **Executed the tool** ‚Üí Got result\n",
    "3. ‚úÖ **Sent result back to LLM** ‚Üí Got final response\n",
    "4. ‚úÖ **Verified the action** ‚Üí Confirmed memory stored\n",
    "\n",
    "**Why this is tedious:**\n",
    "- üî¥ Multiple manual steps\n",
    "- üî¥ Need to track message history\n",
    "- üî¥ Handle tool call IDs\n",
    "- üî¥ Manage state between calls\n",
    "- üî¥ Complex error handling\n",
    "\n",
    "**What LangGraph does:**\n",
    "- ‚úÖ Automates all these steps\n",
    "- ‚úÖ Manages state automatically\n",
    "- ‚úÖ Handles tool execution loop\n",
    "- ‚úÖ Provides clear workflow visualization\n",
    "- ‚úÖ Makes it easy to add more tools and logic\n",
    "\n",
    "**Now you understand why we need LangGraph!** üëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295f410390e0ecd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® Introduction to LangGraph\n",
    "\n",
    "Memory tools are powerful, but managing complex workflows manually gets complicated. **LangGraph** automates this process.\n",
    "\n",
    "### **What is LangGraph?**\n",
    "\n",
    "**LangGraph** is a framework for building stateful, multi-step agent workflows using graphs.\n",
    "\n",
    "### **Core Concepts**\n",
    "\n",
    "**1. State** - Shared data structure passed between nodes\n",
    "- Contains messages, context, and intermediate results\n",
    "- Automatically managed and updated\n",
    "\n",
    "**2. Nodes** - Functions that process state\n",
    "- Examples: call LLM, execute tools, format responses\n",
    "- Each node receives state and returns updated state\n",
    "\n",
    "**3. Edges** - Connections between nodes\n",
    "- Can be conditional (if/else logic)\n",
    "- Determine workflow flow\n",
    "\n",
    "**4. Graph** - Complete workflow from start to end\n",
    "- Orchestrates the entire agent process\n",
    "\n",
    "### **Simple Memory-Enhanced Graph**\n",
    "\n",
    "```\n",
    "START\n",
    "  ‚Üì\n",
    "[Load Memory] ‚Üê Get user context\n",
    "  ‚Üì\n",
    "[Agent Node] ‚Üê Decides what to do\n",
    "  ‚Üì\n",
    "  ‚îú‚îÄ‚Üí [Memory Tools] ‚Üê store/search/retrieve\n",
    "  ‚îÇ      ‚Üì\n",
    "  ‚îÇ   [Agent Node] ‚Üê Processes memory results\n",
    "  ‚îÇ\n",
    "  ‚îî‚îÄ‚Üí [Respond] ‚Üê Generates final response\n",
    "         ‚Üì\n",
    "[Save Memory] ‚Üê Update conversation history\n",
    "         ‚Üì\n",
    "        END\n",
    "```\n",
    "\n",
    "### **Why LangGraph for Memory Tools?**\n",
    "\n",
    "**Without LangGraph:**\n",
    "- Manual tool execution and state management\n",
    "- Complex conditional logic\n",
    "- Hard to visualize workflow\n",
    "- Difficult to add new steps\n",
    "\n",
    "**With LangGraph:**\n",
    "- ‚úÖ Automatic tool execution\n",
    "- ‚úÖ Clear workflow visualization\n",
    "- ‚úÖ Easy to modify and extend\n",
    "- ‚úÖ Built-in state management\n",
    "- ‚úÖ Memory persistence across turns\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Passive vs Active Memory: The Key Difference\n",
    "\n",
    "Let's compare the two approaches to understand why memory tools matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a99956e8ff8d58",
   "metadata": {},
   "source": [
    "### **Passive Memory (Section 3)**\n",
    "\n",
    "**How it works:**\n",
    "- System automatically saves all conversations\n",
    "- System automatically extracts facts\n",
    "- LLM receives memory but can't control it\n",
    "\n",
    "**Example conversation:**\n",
    "```\n",
    "User: \"I'm interested in machine learning\"\n",
    "Agent: \"Great! Here are some ML courses...\" \n",
    "System: [Automatically saves: \"User interested in ML\"]\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Simple to implement\n",
    "- ‚úÖ No additional LLM calls\n",
    "- ‚úÖ Consistent memory storage\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå LLM can't decide what's important\n",
    "- ‚ùå No strategic memory management\n",
    "- ‚ùå Can't search memories on demand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9768498f-4e95-4217-ad20-93fea45524a2",
   "metadata": {},
   "source": [
    "### **Active Memory (This Section)**\n",
    "\n",
    "**How it works:**\n",
    "- LLM decides what to store\n",
    "- LLM decides when to search memories\n",
    "- LLM controls its own context construction\n",
    "\n",
    "**Example conversation:**\n",
    "```\n",
    "User: \"I'm interested in machine learning\"\n",
    "Agent: [Thinks: \"This is important, I should remember this\"]\n",
    "Agent: [Calls: store_memory(\"User interested in machine learning\")]\n",
    "Agent: \"I'll remember your interest in ML. Here are some courses...\"\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Strategic memory management\n",
    "- ‚úÖ LLM controls what's important\n",
    "- ‚úÖ On-demand memory search\n",
    "- ‚úÖ Better context engineering\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå More complex to implement\n",
    "- ‚ùå Additional LLM calls (cost)\n",
    "- ‚ùå Requires careful tool design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2011d-1696-4eb9-9bec-d1bbba9ef392",
   "metadata": {},
   "source": [
    "### **When to Use Each Approach**\n",
    "\n",
    "**Use Passive Memory when:**\n",
    "- Simple applications with predictable patterns\n",
    "- Cost is a primary concern\n",
    "- Memory needs are straightforward\n",
    "- You want automatic memory management\n",
    "\n",
    "**Use Active Memory when:**\n",
    "- Complex applications requiring strategic memory\n",
    "- LLM needs to control its own context\n",
    "- Dynamic memory management is important\n",
    "- Building sophisticated agents\n",
    "\n",
    "**üí° Key Insight:** Active memory tools enable **intelligent context engineering** where the LLM becomes an active participant in managing its own knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Summary and Next Steps\n",
    "\n",
    "### **What You've Learned**\n",
    "\n",
    "**Memory Tools for Context Engineering:**\n",
    "- `store_memory` - Save important information strategically\n",
    "- `search_memories` - Find relevant context on demand\n",
    "- `retrieve_memories` - Get specific facts by topic\n",
    "\n",
    "**LangGraph Fundamentals:**\n",
    "- State management for complex workflows\n",
    "- Nodes and edges for agent orchestration\n",
    "- Automatic tool execution and state updates\n",
    "\n",
    "**Active vs Passive Memory:**\n",
    "- Passive: System controls memory automatically\n",
    "- Active: LLM controls its own memory strategically\n",
    "\n",
    "### **Context Engineering Connection**\n",
    "\n",
    "Memory tools transform the **four context types**:\n",
    "\n",
    "| Context Type | Section 3 (Passive) | Section 4 (Active) |\n",
    "|-------------|---------------------|--------------------|\n",
    "| **System** | Static prompt | Static prompt |\n",
    "| **User** | Auto-extracted profile | LLM builds profile with `store_memory` |\n",
    "| **Conversation** | Auto-saved history | LLM manages with `search_memories` |\n",
    "| **Retrieved** | RAG search | Memory-enhanced RAG queries |\n",
    "\n",
    "### **Next: Building a Complete Agent**\n",
    "\n",
    "In **Notebook 2**, you'll combine everything:\n",
    "- ‚úÖ Memory tools (this notebook)\n",
    "- ‚úÖ Course search tools\n",
    "- ‚úÖ LangGraph orchestration\n",
    "- ‚úÖ Redis Agent Memory Server\n",
    "\n",
    "**Result:** A complete Redis University Course Advisor Agent that actively manages its own memory and context.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "### **Memory Tools & Context Engineering**\n",
    "- [Redis Agent Memory Server](https://github.com/redis/agent-memory-server) - Memory persistence\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client library\n",
    "\n",
    "### **LangGraph & Tool Calling**\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/) - Official docs\n",
    "- [LangChain Tools](https://python.langchain.com/docs/modules/tools/) - Tool creation guide\n",
    "\n",
    "### **Context Engineering Concepts**\n",
    "- Review **Section 1** for context types fundamentals (System, User, Conversation, Retrieved)\n",
    "- Review **Section 2** for RAG foundations (semantic search, vector embeddings, retrieval)\n",
    "- Review **Section 3** for passive memory patterns (working memory, long-term memory, automatic extraction)\n",
    "- Continue to **Section 4 Notebook 2** for complete agent implementation with all concepts integrated\n",
    "\n",
    "### **Academic Papers**\n",
    "- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) - Reasoning + acting pattern\n",
    "- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761) - Tool learning\n",
    "- [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) - Memory management for LLMs\n",
    "- [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401) - RAG foundations\n",
    "\n",
    "### **Agent Design Patterns**\n",
    "- [Anthropic's Guide to Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - Best practices\n",
    "- [LangChain Agent Patterns](https://python.langchain.com/docs/modules/agents/) - Different agent architectures\n",
    "- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling) - Tool calling fundamentals\n",
    "\n",
    "### **Production Resources**\n",
    "- [LangChain Production Guide](https://python.langchain.com/docs/guides/productionization/) - Deploying agents\n",
    "- [Redis Best Practices](https://redis.io/docs/manual/patterns/) - Production Redis patterns\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client for Agent Memory Server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

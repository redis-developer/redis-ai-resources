{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ed449409dabf1c",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# üìä Section 5, Notebook 1: Measuring and Optimizing Performance\n",
    "\n",
    "**‚è±Ô∏è Estimated Time:** 50-60 minutes\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Measure** agent performance: tokens, cost, and latency\n",
    "2. **Understand** where tokens are being spent in your agent\n",
    "3. **Implement** hybrid retrieval to reduce token usage by 67%\n",
    "4. **Build** structured data views (course catalog summary)\n",
    "5. **Compare** before/after performance with concrete metrics\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where We Are\n",
    "\n",
    "### **Your Journey So Far:**\n",
    "\n",
    "**Section 4, Notebook 2:** You built a complete Redis University Course Advisor Agent with:\n",
    "- ‚úÖ **3 Tools**: `search_courses`, `search_memories`, `store_memory`\n",
    "- ‚úÖ **Dual Memory**: Working memory (session) + Long-term memory (persistent)\n",
    "- ‚úÖ **Basic RAG**: Semantic search over ~150 courses\n",
    "- ‚úÖ **LangGraph Workflow**: State management with tool calling loop\n",
    "\n",
    "**Your agent works!** It can:\n",
    "- Search for courses semantically\n",
    "- Remember student preferences\n",
    "- Provide personalized recommendations\n",
    "- Maintain conversation context\n",
    "\n",
    "### **But... How Efficient Is It?**\n",
    "\n",
    "**Questions we can't answer yet:**\n",
    "- ‚ùì How many tokens does each query use?\n",
    "- ‚ùì How much does each conversation cost?\n",
    "- ‚ùì Where are tokens being spent? (system prompt? retrieved context? tools?)\n",
    "- ‚ùì Is performance degrading over long conversations?\n",
    "- ‚ùì Can we make it faster and cheaper without sacrificing quality?\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Problem We'll Solve\n",
    "\n",
    "**\"Our agent works, but is it efficient? How much does it cost to run? Can we make it faster and cheaper without sacrificing quality?\"**\n",
    "\n",
    "### **What We'll Learn:**\n",
    "\n",
    "1. **Performance Measurement** - Token counting, cost calculation, latency tracking\n",
    "2. **Token Budget Analysis** - Understanding where tokens are spent\n",
    "3. **Retrieval Optimization** - Hybrid retrieval (overview + targeted search)\n",
    "4. **Context Window Management** - When and how to optimize\n",
    "\n",
    "### **What We'll Build:**\n",
    "\n",
    "Starting with your Section 4 agent, we'll add:\n",
    "1. **Performance Tracking System** - Measure tokens, cost, latency automatically\n",
    "2. **Token Counter Integration** - Track token usage across all components\n",
    "3. **Course Catalog Summary View** - Pre-computed overview (one-time)\n",
    "4. **Hybrid Retrieval Tool** - Replace basic search with intelligent hybrid approach\n",
    "\n",
    "### **Expected Results:**\n",
    "\n",
    "```\n",
    "Metric          Before (S4)    After (NB1)    Improvement\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Tokens/query    8,500          2,800          -67%\n",
    "Cost/query      $0.12          $0.04          -67%\n",
    "Latency         3.2s           1.6s           -50%\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "```\n",
    "\n",
    "**üí° Key Insight:** \"You can't optimize what you don't measure\"\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Part 0: Setup and Imports\n",
    "\n",
    "Let's start by importing everything we need and setting up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336cc6d4dee4899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Annotated, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain and LangGraph\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Redis and Agent Memory\n",
    "from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "from agent_memory_client.models import ClientMemoryRecord\n",
    "from agent_memory_client.filters import UserId\n",
    "\n",
    "# RedisVL for course search\n",
    "from redisvl.index import SearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.query.filter import Tag\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12dc57a59db830",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Make sure you have these environment variables set:\n",
    "- `OPENAI_API_KEY` - Your OpenAI API key\n",
    "- `REDIS_URL` - Redis connection URL (default: redis://localhost:6379)\n",
    "- `AGENT_MEMORY_URL` - Agent Memory Server URL (default: http://localhost:8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29463e43fb77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚ùå Missing environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"   Please set them before continuing.\")\n",
    "else:\n",
    "    print(\"‚úÖ Environment variables configured\")\n",
    "\n",
    "# Set defaults for optional vars\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8000\")\n",
    "\n",
    "print(f\"   Redis URL: {REDIS_URL}\")\n",
    "print(f\"   Agent Memory URL: {AGENT_MEMORY_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd20624ce2e3ca8",
   "metadata": {},
   "source": [
    "### Initialize Clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f09e96c2870f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize Agent Memory Client\n",
    "memory_config = MemoryClientConfig(base_url=AGENT_MEMORY_URL)\n",
    "memory_client = MemoryAPIClient(config=memory_config)\n",
    "\n",
    "print(\"‚úÖ Clients initialized\")\n",
    "print(f\"   LLM: {llm.model_name}\")\n",
    "print(f\"   Embeddings: text-embedding-3-small\")\n",
    "print(f\"   Memory Client: Connected to {AGENT_MEMORY_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5ded02170973f",
   "metadata": {},
   "source": [
    "### Student Profile\n",
    "\n",
    "We'll use the same student profile from Section 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660d74d5accbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student profile\n",
    "STUDENT_ID = \"sarah_chen_12345\"\n",
    "SESSION_ID = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "@dataclass\n",
    "class Student:\n",
    "    name: str\n",
    "    student_id: str\n",
    "    major: str\n",
    "    interests: List[str]\n",
    "\n",
    "sarah = Student(\n",
    "    name=\"Sarah Chen\",\n",
    "    student_id=STUDENT_ID,\n",
    "    major=\"Computer Science\",\n",
    "    interests=[\"AI\", \"Machine Learning\", \"Data Science\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Student profile created\")\n",
    "print(f\"   Name: {sarah.name}\")\n",
    "print(f\"   Student ID: {STUDENT_ID}\")\n",
    "print(f\"   Session ID: {SESSION_ID}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccd94b8158593c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Performance Measurement\n",
    "\n",
    "Before we can optimize, we need to measure. Let's build a comprehensive performance tracking system.\n",
    "\n",
    "### üî¨ Theory: Why Measurement Matters\n",
    "\n",
    "**The Optimization Paradox:**\n",
    "- Without measurement, optimization is guesswork\n",
    "- You might optimize the wrong thing\n",
    "- You can't prove improvements\n",
    "\n",
    "**What to Measure:**\n",
    "1. **Tokens** - Input tokens + output tokens (drives cost)\n",
    "2. **Cost** - Actual dollar cost per query\n",
    "3. **Latency** - Time from query to response\n",
    "4. **Token Budget Breakdown** - Where are tokens being spent?\n",
    "\n",
    "**Research Connection:**\n",
    "Remember the Context Rot research from Section 1? It showed that:\n",
    "- More context ‚â† better performance\n",
    "- Quality > quantity in context selection\n",
    "- Distractors (irrelevant context) hurt performance\n",
    "\n",
    "**üí° Key Insight:** Measurement enables optimization. Track everything, optimize strategically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c909ee197eb05cb",
   "metadata": {},
   "source": [
    "### Step 1: Define Performance Metrics\n",
    "\n",
    "Let's create a data structure to track all performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20fee75249fad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Track performance metrics for agent queries.\"\"\"\n",
    "    \n",
    "    # Token counts\n",
    "    input_tokens: int = 0\n",
    "    output_tokens: int = 0\n",
    "    total_tokens: int = 0\n",
    "    \n",
    "    # Token breakdown\n",
    "    system_tokens: int = 0\n",
    "    conversation_tokens: int = 0\n",
    "    retrieved_tokens: int = 0\n",
    "    tools_tokens: int = 0\n",
    "    \n",
    "    # Cost (GPT-4o pricing: $5/1M input, $15/1M output)\n",
    "    input_cost: float = 0.0\n",
    "    output_cost: float = 0.0\n",
    "    total_cost: float = 0.0\n",
    "    \n",
    "    # Latency\n",
    "    start_time: float = field(default_factory=time.time)\n",
    "    end_time: Optional[float] = None\n",
    "    latency_seconds: Optional[float] = None\n",
    "    \n",
    "    # Metadata\n",
    "    query: str = \"\"\n",
    "    response: str = \"\"\n",
    "    tools_called: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"Calculate final metrics.\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        self.latency_seconds = self.end_time - self.start_time\n",
    "        self.total_tokens = self.input_tokens + self.output_tokens\n",
    "        \n",
    "        # GPT-4o pricing (as of 2024)\n",
    "        self.input_cost = (self.input_tokens / 1_000_000) * 5.0\n",
    "        self.output_cost = (self.output_tokens / 1_000_000) * 15.0\n",
    "        self.total_cost = self.input_cost + self.output_cost\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display metrics in a readable format.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä PERFORMANCE METRICS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nüî¢ Token Usage:\")\n",
    "        print(f\"   Input tokens:  {self.input_tokens:,}\")\n",
    "        print(f\"   Output tokens: {self.output_tokens:,}\")\n",
    "        print(f\"   Total tokens:  {self.total_tokens:,}\")\n",
    "        \n",
    "        if self.system_tokens or self.conversation_tokens or self.retrieved_tokens or self.tools_tokens:\n",
    "            print(f\"\\nüì¶ Token Breakdown:\")\n",
    "            print(f\"   System prompt:     {self.system_tokens:,} ({self.system_tokens/self.input_tokens*100:.1f}%)\")\n",
    "            print(f\"   Conversation:      {self.conversation_tokens:,} ({self.conversation_tokens/self.input_tokens*100:.1f}%)\")\n",
    "            print(f\"   Retrieved context: {self.retrieved_tokens:,} ({self.retrieved_tokens/self.input_tokens*100:.1f}%)\")\n",
    "            print(f\"   Tools:             {self.tools_tokens:,} ({self.tools_tokens/self.input_tokens*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüí∞ Cost:\")\n",
    "        print(f\"   Input cost:  ${self.input_cost:.4f}\")\n",
    "        print(f\"   Output cost: ${self.output_cost:.4f}\")\n",
    "        print(f\"   Total cost:  ${self.total_cost:.4f}\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  Latency: {self.latency_seconds:.2f}s\")\n",
    "        \n",
    "        if self.tools_called:\n",
    "            print(f\"\\nüõ†Ô∏è  Tools Called: {', '.join(self.tools_called)}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "\n",
    "print(\"‚úÖ PerformanceMetrics dataclass defined\")\n",
    "print(\"   Tracks: tokens, cost, latency, token breakdown\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1803f26a0dac2a2",
   "metadata": {},
   "source": [
    "### Step 2: Token Counting Functions\n",
    "\n",
    "We'll use `tiktoken` to count tokens accurately for GPT-4o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236a8b53c3bb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, model: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in text using tiktoken.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to count tokens for\n",
    "        model: The model name (default: gpt-4o)\n",
    "    \n",
    "    Returns:\n",
    "        Number of tokens\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base for newer models\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def count_messages_tokens(messages: List[BaseMessage], model: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in a list of messages.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of LangChain messages\n",
    "        model: The model name\n",
    "    \n",
    "    Returns:\n",
    "        Total number of tokens\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for message in messages:\n",
    "        # Each message has overhead: role + content + formatting\n",
    "        total += 4  # Message formatting overhead\n",
    "        total += count_tokens(message.content, model)\n",
    "    total += 2  # Conversation formatting overhead\n",
    "    return total\n",
    "\n",
    "print(\"‚úÖ Token counting functions defined\")\n",
    "print(\"   count_tokens() - Count tokens in text\")\n",
    "print(\"   count_messages_tokens() - Count tokens in message list\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d7ac898ace6f2",
   "metadata": {},
   "source": [
    "### Step 3: Test Token Counting\n",
    "\n",
    "Let's verify our token counting works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670e6978068d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test token counting\n",
    "test_text = \"What machine learning courses are available at Redis University?\"\n",
    "token_count = count_tokens(test_text)\n",
    "\n",
    "print(f\"Test query: '{test_text}'\")\n",
    "print(f\"Token count: {token_count}\")\n",
    "\n",
    "# Test message counting\n",
    "test_messages = [\n",
    "    SystemMessage(content=\"You are a helpful course advisor.\"),\n",
    "    HumanMessage(content=test_text),\n",
    "    AIMessage(content=\"Let me search for machine learning courses for you.\")\n",
    "]\n",
    "message_tokens = count_messages_tokens(test_messages)\n",
    "\n",
    "print(f\"\\nTest messages (3 messages):\")\n",
    "print(f\"Total tokens: {message_tokens}\")\n",
    "print(\"‚úÖ Token counting verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4375ac37782c364",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Part 2: Baseline Performance Measurement\n",
    "\n",
    "Now let's measure the performance of our Section 4 agent to establish a baseline.\n",
    "\n",
    "### Load Section 4 Agent Components\n",
    "\n",
    "First, we need to recreate the Section 4 agent. We'll load the course catalog and define the same 3 tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7b6c8b56f10ef",
   "metadata": {},
   "source": [
    "### Course Manager (from Section 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770778773585169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CourseManager:\n",
    "    \"\"\"Manage course catalog with Redis vector search.\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_url: str, index_name: str = \"course_catalog\"):\n",
    "        self.redis_url = redis_url\n",
    "        self.index_name = index_name\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # Initialize search index\n",
    "        self.index = SearchIndex.from_existing(\n",
    "            name=self.index_name,\n",
    "            redis_url=self.redis_url\n",
    "        )\n",
    "    \n",
    "    async def search_courses(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for courses using semantic search.\"\"\"\n",
    "        # Create query embedding\n",
    "        query_embedding = await self.embeddings.aembed_query(query)\n",
    "        \n",
    "        # Create vector query\n",
    "        vector_query = VectorQuery(\n",
    "            vector=query_embedding,\n",
    "            vector_field_name=\"content_vector\",\n",
    "            return_fields=[\"course_code\", \"title\", \"description\", \"department\", \"credits\", \"format\"],\n",
    "            num_results=limit\n",
    "        )\n",
    "        \n",
    "        # Execute search\n",
    "        results = self.index.query(vector_query)\n",
    "        return results\n",
    "\n",
    "# Initialize course manager\n",
    "course_manager = CourseManager(redis_url=REDIS_URL)\n",
    "\n",
    "print(\"‚úÖ Course manager initialized\")\n",
    "print(f\"   Index: {course_manager.index_name}\")\n",
    "print(f\"   Redis: {REDIS_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a83ed5953cdbd",
   "metadata": {},
   "source": [
    "### Define the 3 Tools (from Section 4)\n",
    "\n",
    "Now let's define the same 3 tools from Section 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db85c3203e73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1: search_courses\n",
    "class SearchCoursesInput(BaseModel):\n",
    "    \"\"\"Input schema for searching courses.\"\"\"\n",
    "    query: str = Field(description=\"Natural language query to search for courses\")\n",
    "    limit: int = Field(default=5, description=\"Maximum number of courses to return\")\n",
    "\n",
    "@tool(\"search_courses\", args_schema=SearchCoursesInput)\n",
    "async def search_courses(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search for courses using semantic search based on topics, descriptions, or characteristics.\n",
    "\n",
    "    Use this tool when students ask about:\n",
    "    - Topics or subjects: \"machine learning courses\", \"database courses\"\n",
    "    - Course characteristics: \"online courses\", \"beginner courses\"\n",
    "    - General exploration: \"what courses are available?\"\n",
    "\n",
    "    Returns: Formatted list of matching courses with details.\n",
    "    \"\"\"\n",
    "    results = await course_manager.search_courses(query, limit=limit)\n",
    "\n",
    "    if not results:\n",
    "        return \"No courses found matching your query.\"\n",
    "\n",
    "    output = []\n",
    "    for i, course in enumerate(results, 1):\n",
    "        output.append(f\"{i}. {course['title']} ({course.get('course_code', course.get('course_id', 'N/A'))})\")\n",
    "        output.append(f\"   Department: {course['department']}\")\n",
    "        output.append(f\"   Credits: {course['credits']}\")\n",
    "        output.append(f\"   Format: {course['format']}\")\n",
    "        output.append(f\"   Description: {course['description'][:150]}...\")\n",
    "        output.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "print(\"‚úÖ Tool 1 defined: search_courses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c3f02ab96a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 2: search_memories\n",
    "class SearchMemoriesInput(BaseModel):\n",
    "    \"\"\"Input schema for searching memories.\"\"\"\n",
    "    query: str = Field(description=\"Natural language query to search for in user's memory\")\n",
    "    limit: int = Field(default=5, description=\"Maximum number of memories to return\")\n",
    "\n",
    "@tool(\"search_memories\", args_schema=SearchMemoriesInput)\n",
    "async def search_memories(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the user's long-term memory for relevant facts, preferences, and past interactions.\n",
    "\n",
    "    Use this tool when you need to:\n",
    "    - Recall user preferences: \"What format does the user prefer?\"\n",
    "    - Remember past goals: \"What career path is the user interested in?\"\n",
    "    - Personalize recommendations: \"What are the user's interests?\"\n",
    "\n",
    "    Returns: List of relevant memories with content and metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = await memory_client.search_long_term_memory(\n",
    "            text=query,\n",
    "            user_id=UserId(eq=STUDENT_ID),\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "        if not results.memories or len(results.memories) == 0:\n",
    "            return \"No relevant memories found.\"\n",
    "\n",
    "        output = []\n",
    "        for i, memory in enumerate(results.memories, 1):\n",
    "            output.append(f\"{i}. {memory.text}\")\n",
    "            if memory.topics:\n",
    "                output.append(f\"   Topics: {', '.join(memory.topics)}\")\n",
    "\n",
    "        return \"\\n\".join(output)\n",
    "    except Exception as e:\n",
    "        return f\"Error searching memories: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Tool 2 defined: search_memories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caea4c8f6933cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 3: store_memory\n",
    "class StoreMemoryInput(BaseModel):\n",
    "    \"\"\"Input schema for storing memories.\"\"\"\n",
    "    text: str = Field(description=\"The information to store as a clear, factual statement\")\n",
    "    memory_type: str = Field(default=\"semantic\", description=\"Type: 'semantic' or 'episodic'\")\n",
    "    topics: List[str] = Field(default=[], description=\"Optional tags to categorize the memory\")\n",
    "\n",
    "@tool(\"store_memory\", args_schema=StoreMemoryInput)\n",
    "async def store_memory(text: str, memory_type: str = \"semantic\", topics: List[str] = []) -> str:\n",
    "    \"\"\"\n",
    "    Store important information to the user's long-term memory.\n",
    "\n",
    "    Use this tool when the user shares:\n",
    "    - Preferences: \"I prefer online courses\"\n",
    "    - Goals: \"I want to work in AI\"\n",
    "    - Important facts: \"I have a part-time job\"\n",
    "    - Constraints: \"I can only take 2 courses per semester\"\n",
    "\n",
    "    Returns: Confirmation message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        memory = ClientMemoryRecord(\n",
    "            text=text,\n",
    "            user_id=STUDENT_ID,\n",
    "            memory_type=memory_type,\n",
    "            topics=topics or []\n",
    "        )\n",
    "\n",
    "        await memory_client.create_long_term_memory([memory])\n",
    "        return f\"‚úÖ Stored to long-term memory: {text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error storing memory: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Tool 3 defined: store_memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9985b853e742c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tools\n",
    "tools = [search_courses, search_memories, store_memory]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üõ†Ô∏è  BASELINE AGENT TOOLS (from Section 4)\")\n",
    "print(\"=\" * 80)\n",
    "for i, tool in enumerate(tools, 1):\n",
    "    print(f\"{i}. {tool.name}\")\n",
    "    print(f\"   Description: {tool.description.split('.')[0]}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632b73b13009799",
   "metadata": {},
   "source": [
    "### Define AgentState (from Section 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c25622774a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(BaseModel):\n",
    "    \"\"\"State for the course advisor agent.\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    student_id: str\n",
    "    session_id: str\n",
    "    context: Dict[str, Any] = {}\n",
    "\n",
    "print(\"‚úÖ AgentState defined\")\n",
    "print(\"   Fields: messages, student_id, session_id, context\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5545401f570fd5",
   "metadata": {},
   "source": [
    "### Build Baseline Agent Workflow\n",
    "\n",
    "Now let's build the complete Section 4 agent workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d381c72553b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Load working memory\n",
    "async def load_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"Load conversation history from working memory.\"\"\"\n",
    "    try:\n",
    "        from agent_memory_client.filters import SessionId\n",
    "\n",
    "        # Get working memory for this session\n",
    "        _, working_memory = await memory_client.get_or_create_working_memory(\n",
    "            user_id=UserId(eq=state.student_id),\n",
    "            session_id=SessionId(eq=state.session_id),\n",
    "            model_name=\"gpt-4o\"\n",
    "        )\n",
    "\n",
    "        # Add to context\n",
    "        if working_memory and working_memory.messages:\n",
    "            state.context[\"working_memory_loaded\"] = True\n",
    "            state.context[\"memory_message_count\"] = len(working_memory.messages)\n",
    "    except Exception as e:\n",
    "        state.context[\"working_memory_loaded\"] = False\n",
    "        state.context[\"memory_error\"] = str(e)\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Node 1: load_memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc27831b5ccc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 2: Agent (LLM with tools)\n",
    "async def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"The agent decides what to do: call tools or respond to the user.\"\"\"\n",
    "    system_message = SystemMessage(content=\"\"\"\n",
    "You are a helpful Redis University course advisor assistant.\n",
    "\n",
    "Your role:\n",
    "- Help students find courses that match their interests and goals\n",
    "- Remember student preferences and use them for personalized recommendations\n",
    "- Store important information about students for future conversations\n",
    "\n",
    "Guidelines:\n",
    "- Use search_courses to find relevant courses\n",
    "- Use search_memories to recall student preferences and past interactions\n",
    "- Use store_memory when students share important preferences, goals, or constraints\n",
    "- Be conversational and helpful\n",
    "- Provide specific course recommendations with details\n",
    "\"\"\")\n",
    "\n",
    "    # Bind tools to LLM\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # Call LLM with system message + conversation history\n",
    "    messages = [system_message] + state.messages\n",
    "    response = await llm_with_tools.ainvoke(messages)\n",
    "\n",
    "    # Add response to state\n",
    "    state.messages.append(response)\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Node 2: agent_node\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1725143f366110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 3: Save working memory\n",
    "async def save_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"Save updated conversation to working memory.\"\"\"\n",
    "    try:\n",
    "        from agent_memory_client.filters import SessionId\n",
    "\n",
    "        # Save working memory\n",
    "        await memory_client.put_working_memory(\n",
    "            user_id=state.student_id,\n",
    "            session_id=state.session_id,\n",
    "            memory=working_memory,\n",
    "            model_name=\"gpt-4o\"\n",
    "        )\n",
    "\n",
    "        state.context[\"working_memory_saved\"] = True\n",
    "    except Exception as e:\n",
    "        state.context[\"working_memory_saved\"] = False\n",
    "        state.context[\"save_error\"] = str(e)\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Node 3: save_memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe23ddefeea004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing logic\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Determine if we should continue to tools or end.\"\"\"\n",
    "    last_message = state.messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, route to tools\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    # Otherwise, we're done and should save memory\n",
    "    return \"save_memory\"\n",
    "\n",
    "print(\"‚úÖ Routing: should_continue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f339512e979d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"load_memory\", load_memory)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "workflow.add_node(\"save_memory\", save_memory)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"load_memory\")\n",
    "workflow.add_edge(\"load_memory\", \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"save_memory\": \"save_memory\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")  # After tools, go back to agent\n",
    "workflow.add_edge(\"save_memory\", END)\n",
    "\n",
    "# Compile the graph\n",
    "baseline_agent = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Baseline agent graph compiled\")\n",
    "print(\"   Nodes: load_memory, agent, tools, save_memory\")\n",
    "print(\"   This is the same agent from Section 4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b692fc3b0d8771",
   "metadata": {},
   "source": [
    "### Run Baseline Performance Test\n",
    "\n",
    "Now let's run a test query and measure its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19c718d5b2ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_baseline_agent_with_metrics(user_message: str) -> PerformanceMetrics:\n",
    "    \"\"\"\n",
    "    Run the baseline agent and track performance metrics.\n",
    "\n",
    "    Args:\n",
    "        user_message: The user's input\n",
    "\n",
    "    Returns:\n",
    "        PerformanceMetrics object with all measurements\n",
    "    \"\"\"\n",
    "    # Initialize metrics\n",
    "    metrics = PerformanceMetrics(query=user_message)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üë§ USER: {user_message}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create initial state\n",
    "    initial_state = AgentState(\n",
    "        messages=[HumanMessage(content=user_message)],\n",
    "        student_id=STUDENT_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        context={}\n",
    "    )\n",
    "\n",
    "    # Run the agent\n",
    "    print(\"\\nü§ñ Running baseline agent...\")\n",
    "    final_state = await baseline_agent.ainvoke(initial_state)\n",
    "\n",
    "    # Extract response\n",
    "    last_message = final_state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage):\n",
    "        metrics.response = last_message.content\n",
    "\n",
    "    # Count tokens for all messages\n",
    "    metrics.input_tokens = count_messages_tokens(final_state[\"messages\"][:-1])  # All except last\n",
    "    metrics.output_tokens = count_tokens(metrics.response)\n",
    "\n",
    "    # Estimate token breakdown (approximate)\n",
    "    system_prompt = \"\"\"You are a helpful Redis University course advisor assistant.\n",
    "\n",
    "Your role:\n",
    "- Help students find courses that match their interests and goals\n",
    "- Remember student preferences and use them for personalized recommendations\n",
    "- Store important information about students for future conversations\n",
    "\n",
    "Guidelines:\n",
    "- Use search_courses to find relevant courses\n",
    "- Use search_memories to recall student preferences and past interactions\n",
    "- Use store_memory when students share important preferences, goals, or constraints\n",
    "- Be conversational and helpful\n",
    "- Provide specific course recommendations with details\"\"\"\n",
    "\n",
    "    metrics.system_tokens = count_tokens(system_prompt)\n",
    "    metrics.conversation_tokens = count_tokens(user_message)\n",
    "\n",
    "    # Tools tokens (approximate - all 3 tool definitions)\n",
    "    metrics.tools_tokens = sum(count_tokens(str(tool.args_schema.model_json_schema())) +\n",
    "                                count_tokens(tool.description) for tool in tools)\n",
    "\n",
    "    # Retrieved context (remaining tokens)\n",
    "    metrics.retrieved_tokens = metrics.input_tokens - metrics.system_tokens - metrics.conversation_tokens - metrics.tools_tokens\n",
    "    if metrics.retrieved_tokens < 0:\n",
    "        metrics.retrieved_tokens = 0\n",
    "\n",
    "    # Track tools called\n",
    "    for msg in final_state[\"messages\"]:\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tool_call in msg.tool_calls:\n",
    "                metrics.tools_called.append(tool_call['name'])\n",
    "\n",
    "    # Finalize metrics\n",
    "    metrics.finalize()\n",
    "\n",
    "    # Display response\n",
    "    print(f\"\\nü§ñ AGENT: {metrics.response[:200]}...\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ Baseline agent runner with metrics defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7e072305b275d",
   "metadata": {},
   "source": [
    "### Test 1: Simple Course Search\n",
    "\n",
    "Let's test with a simple course search query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100063092ec96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple course search\n",
    "baseline_metrics_1 = await run_baseline_agent_with_metrics(\n",
    "    \"What machine learning courses are available?\"\n",
    ")\n",
    "\n",
    "baseline_metrics_1.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd72eb83b1e4bb6",
   "metadata": {},
   "source": [
    "### Test 2: Query with Memory\n",
    "\n",
    "Let's test a query that might use memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4d2b973d4c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Query with memory\n",
    "baseline_metrics_2 = await run_baseline_agent_with_metrics(\n",
    "    \"I prefer online courses and I'm interested in AI. What would you recommend?\"\n",
    ")\n",
    "\n",
    "baseline_metrics_2.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11850c72f117e034",
   "metadata": {},
   "source": [
    "### Baseline Performance Summary\n",
    "\n",
    "Let's summarize the baseline performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2833673d1e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä BASELINE PERFORMANCE SUMMARY (Section 4 Agent)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTest 1: Simple course search\")\n",
    "print(f\"   Tokens: {baseline_metrics_1.total_tokens:,}\")\n",
    "print(f\"   Cost: ${baseline_metrics_1.total_cost:.4f}\")\n",
    "print(f\"   Latency: {baseline_metrics_1.latency_seconds:.2f}s\")\n",
    "\n",
    "print(\"\\nTest 2: Query with memory\")\n",
    "print(f\"   Tokens: {baseline_metrics_2.total_tokens:,}\")\n",
    "print(f\"   Cost: ${baseline_metrics_2.total_cost:.4f}\")\n",
    "print(f\"   Latency: {baseline_metrics_2.latency_seconds:.2f}s\")\n",
    "\n",
    "# Calculate averages\n",
    "avg_tokens = (baseline_metrics_1.total_tokens + baseline_metrics_2.total_tokens) / 2\n",
    "avg_cost = (baseline_metrics_1.total_cost + baseline_metrics_2.total_cost) / 2\n",
    "avg_latency = (baseline_metrics_1.latency_seconds + baseline_metrics_2.latency_seconds) / 2\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"AVERAGE BASELINE PERFORMANCE:\")\n",
    "print(f\"   Tokens/query: {avg_tokens:,.0f}\")\n",
    "print(f\"   Cost/query: ${avg_cost:.4f}\")\n",
    "print(f\"   Latency: {avg_latency:.2f}s\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7976821d5c34331",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Part 3: Token Distribution Analysis\n",
    "\n",
    "Now let's analyze where tokens are being spent.\n",
    "\n",
    "### Understanding Token Breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f30bf450ee76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üì¶ TOKEN DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use Test 1 metrics for analysis\n",
    "print(f\"\\nTotal Input Tokens: {baseline_metrics_1.input_tokens:,}\")\n",
    "print(\"\\nBreakdown:\")\n",
    "print(f\"   1. System Prompt:     {baseline_metrics_1.system_tokens:,} ({baseline_metrics_1.system_tokens/baseline_metrics_1.input_tokens*100:.1f}%)\")\n",
    "print(f\"   2. Conversation:      {baseline_metrics_1.conversation_tokens:,} ({baseline_metrics_1.conversation_tokens/baseline_metrics_1.input_tokens*100:.1f}%)\")\n",
    "print(f\"   3. Tools (3 tools):   {baseline_metrics_1.tools_tokens:,} ({baseline_metrics_1.tools_tokens/baseline_metrics_1.input_tokens*100:.1f}%)\")\n",
    "print(f\"   4. Retrieved Context: {baseline_metrics_1.retrieved_tokens:,} ({baseline_metrics_1.retrieved_tokens/baseline_metrics_1.input_tokens*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ KEY INSIGHT: Retrieved Context is the Biggest Consumer\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "The retrieved context (course search results) uses the most tokens!\n",
    "\n",
    "Why?\n",
    "- We search for 5 courses by default\n",
    "- Each course has: title, description, department, credits, format\n",
    "- Descriptions can be 150+ characters each\n",
    "- Total: ~3,000-4,000 tokens just for retrieved courses\n",
    "\n",
    "This is our optimization opportunity!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec25e6f72553d9",
   "metadata": {},
   "source": [
    "### The Context Rot Connection\n",
    "\n",
    "Remember the Context Rot research from Section 1?\n",
    "\n",
    "**Key Findings:**\n",
    "1. **More context ‚â† better performance** - Adding more retrieved documents doesn't always help\n",
    "2. **Distractors hurt performance** - Similar-but-wrong information confuses the LLM\n",
    "3. **Quality > Quantity** - Relevant, focused context beats large, unfocused context\n",
    "\n",
    "**Our Problem:**\n",
    "- We're retrieving 5 full courses every time (even for \"What courses are available?\")\n",
    "- Many queries don't need full course details\n",
    "- We're paying for tokens we don't need\n",
    "\n",
    "**The Solution:**\n",
    "- **Hybrid Retrieval** - Provide overview first, then details on demand\n",
    "- **Structured Views** - Pre-compute catalog summaries\n",
    "- **Smart Retrieval** - Only retrieve full details when needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d61241344f46a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 4: Optimization Strategy - Hybrid Retrieval\n",
    "\n",
    "Now let's implement our optimization: **Hybrid Retrieval**.\n",
    "\n",
    "### üî¨ Theory: Hybrid Retrieval\n",
    "\n",
    "**The Problem:**\n",
    "- Static context (always the same) = wasteful for dynamic queries\n",
    "- RAG (always search) = wasteful for overview queries\n",
    "- Need: Smart combination of both\n",
    "\n",
    "**The Solution: Hybrid Retrieval**\n",
    "\n",
    "```\n",
    "Query Type          Strategy                    Tokens\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\"What courses       ‚Üí Static overview           ~800\n",
    " are available?\"      (pre-computed)\n",
    "\n",
    "\"Tell me about      ‚Üí Overview + targeted       ~2,200\n",
    " Redis courses\"       search (hybrid)\n",
    "\n",
    "\"RU202 details\"     ‚Üí Targeted search only      ~1,500\n",
    "                      (specific query)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ 60-70% token reduction for overview queries\n",
    "- ‚úÖ Better UX (quick overview, then details)\n",
    "- ‚úÖ Maintains quality (still has full search capability)\n",
    "- ‚úÖ Scales better (overview doesn't grow with catalog size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cd899790f2380",
   "metadata": {},
   "source": [
    "### Step 1: Build Course Catalog Summary\n",
    "\n",
    "First, let's create a pre-computed overview of the entire course catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f4a8d11d2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def build_catalog_summary() -> str:\n",
    "    \"\"\"\n",
    "    Build a comprehensive summary of the course catalog.\n",
    "\n",
    "    This is done once and reused for all overview queries.\n",
    "\n",
    "    Returns:\n",
    "        Formatted catalog summary\n",
    "    \"\"\"\n",
    "    print(\"üî® Building course catalog summary...\")\n",
    "    print(\"   This is a one-time operation\")\n",
    "\n",
    "    # Get all courses (we'll group by department)\n",
    "    all_courses = await course_manager.search_courses(\"courses\", limit=150)\n",
    "\n",
    "    # Group by department\n",
    "    departments = {}\n",
    "    for course in all_courses:\n",
    "        dept = course.get('department', 'Other')\n",
    "        if dept not in departments:\n",
    "            departments[dept] = []\n",
    "        departments[dept].append(course)\n",
    "\n",
    "    # Build summary\n",
    "    summary_parts = []\n",
    "    summary_parts.append(\"=\" * 80)\n",
    "    summary_parts.append(\"REDIS UNIVERSITY COURSE CATALOG OVERVIEW\")\n",
    "    summary_parts.append(\"=\" * 80)\n",
    "    summary_parts.append(f\"\\nTotal Courses: {len(all_courses)}\")\n",
    "    summary_parts.append(f\"Departments: {len(departments)}\")\n",
    "    summary_parts.append(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "    # Summarize each department\n",
    "    for dept, courses in sorted(departments.items()):\n",
    "        summary_parts.append(f\"\\nüìö {dept} ({len(courses)} courses)\")\n",
    "\n",
    "        # List course titles\n",
    "        for course in courses[:10]:  # Limit to first 10 per department\n",
    "            summary_parts.append(f\"   ‚Ä¢ {course['title']} ({course.get('course_code', course.get('course_id', 'N/A'))})\")\n",
    "\n",
    "        if len(courses) > 10:\n",
    "            summary_parts.append(f\"   ... and {len(courses) - 10} more courses\")\n",
    "\n",
    "    summary_parts.append(\"\\n\" + \"=\" * 80)\n",
    "    summary_parts.append(\"For detailed information about specific courses, please ask!\")\n",
    "    summary_parts.append(\"=\" * 80)\n",
    "\n",
    "    summary = \"\\n\".join(summary_parts)\n",
    "\n",
    "    print(f\"‚úÖ Catalog summary built\")\n",
    "    print(f\"   Total courses: {len(all_courses)}\")\n",
    "    print(f\"   Departments: {len(departments)}\")\n",
    "    print(f\"   Summary tokens: {count_tokens(summary):,}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Build the summary\n",
    "CATALOG_SUMMARY = await build_catalog_summary()\n",
    "\n",
    "# Display a preview\n",
    "print(\"\\nüìÑ CATALOG SUMMARY PREVIEW:\")\n",
    "print(CATALOG_SUMMARY[:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db4acdfb69e1e9",
   "metadata": {},
   "source": [
    "### Step 2: Implement Hybrid Retrieval Tool\n",
    "\n",
    "Now let's create a new tool that uses hybrid retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244926ffdcde96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function first\n",
    "async def search_courses_hybrid_func(query: str, limit: int = 5, overview_only: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Search for courses using hybrid retrieval (overview + targeted search).\n",
    "\n",
    "    This tool intelligently combines:\n",
    "    1. Pre-computed catalog overview (always included for context)\n",
    "    2. Targeted semantic search (only when needed)\n",
    "\n",
    "    Use this tool when students ask about:\n",
    "    - General exploration: \"what courses are available?\" ‚Üí overview_only=True\n",
    "    - Specific topics: \"machine learning courses\" ‚Üí overview_only=False (overview + search)\n",
    "    - Course details: \"tell me about RU202\" ‚Üí overview_only=False\n",
    "\n",
    "    The hybrid approach reduces tokens by 60-70% for overview queries while maintaining\n",
    "    full search capability for specific queries.\n",
    "\n",
    "    Returns: Catalog overview + optional targeted search results.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "\n",
    "    # Determine if this is a general overview query\n",
    "    general_queries = [\"what courses\", \"available courses\", \"course catalog\", \"all courses\", \"courses offered\"]\n",
    "    is_general = any(phrase in query.lower() for phrase in general_queries)\n",
    "\n",
    "    if is_general or overview_only:\n",
    "        # Return overview only\n",
    "        output.append(\"üìö Here's an overview of our course catalog:\\n\")\n",
    "        output.append(CATALOG_SUMMARY)\n",
    "        output.append(\"\\nüí° Ask me about specific topics or departments for detailed recommendations!\")\n",
    "    else:\n",
    "        # Return overview + targeted search\n",
    "        output.append(\"üìö Course Catalog Context:\\n\")\n",
    "        output.append(CATALOG_SUMMARY[:400] + \"...\\n\")  # Abbreviated overview\n",
    "        output.append(\"\\nüîç Courses matching your query:\\n\")\n",
    "\n",
    "        # Perform targeted search\n",
    "        results = await course_manager.search_courses(query, limit=limit)\n",
    "\n",
    "        if not results:\n",
    "            output.append(\"No courses found matching your specific query.\")\n",
    "        else:\n",
    "            for i, course in enumerate(results, 1):\n",
    "                output.append(f\"\\n{i}. {course['title']} ({course.get('course_code', course.get('course_id', 'N/A'))})\")\n",
    "                output.append(f\"   Department: {course['department']}\")\n",
    "                output.append(f\"   Credits: {course['credits']}\")\n",
    "                output.append(f\"   Format: {course['format']}\")\n",
    "                output.append(f\"   Description: {course['description'][:150]}...\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# Create the tool using StructuredTool\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "search_courses_hybrid = StructuredTool.from_function(\n",
    "    coroutine=search_courses_hybrid_func,\n",
    "    name=\"search_courses_hybrid\",\n",
    "    description=\"\"\"Search for courses using hybrid retrieval (overview + targeted search).\n",
    "\n",
    "This tool intelligently combines:\n",
    "1. Pre-computed catalog overview (always included for context)\n",
    "2. Targeted semantic search (only when needed)\n",
    "\n",
    "Use this tool when students ask about:\n",
    "- General exploration: \"what courses are available?\" ‚Üí overview_only=True\n",
    "- Specific topics: \"machine learning courses\" ‚Üí overview_only=False (overview + search)\n",
    "- Course details: \"tell me about RU202\" ‚Üí overview_only=False\n",
    "\n",
    "The hybrid approach reduces tokens by 60-70% for overview queries while maintaining\n",
    "full search capability for specific queries.\n",
    "\n",
    "Args:\n",
    "    query: Natural language query to search for courses\n",
    "    limit: Maximum number of detailed courses to return (default: 5)\n",
    "    overview_only: If True, return only catalog overview. If False, return overview + targeted search results (default: False)\n",
    "\n",
    "Returns: Catalog overview + optional targeted search results.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Hybrid retrieval tool defined: search_courses_hybrid\")\n",
    "print(\"   Strategy: Overview + targeted search\")\n",
    "print(\"   Benefit: 60-70% token reduction for overview queries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569681c5b61bc51",
   "metadata": {},
   "source": [
    "### Step 3: Build Optimized Agent with Hybrid Retrieval\n",
    "\n",
    "Now let's create a new agent that uses the hybrid retrieval tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4d12a9457e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New tool list with hybrid retrieval\n",
    "optimized_tools = [\n",
    "    search_courses_hybrid,  # Replaced search_courses with hybrid version\n",
    "    search_memories,\n",
    "    store_memory\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Optimized tools list created\")\n",
    "print(\"   Tool 1: search_courses_hybrid (NEW - uses hybrid retrieval)\")\n",
    "print(\"   Tool 2: search_memories (same)\")\n",
    "print(\"   Tool 3: store_memory (same)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41855517d0bc593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized agent node (updated system prompt)\n",
    "async def optimized_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"The optimized agent with hybrid retrieval.\"\"\"\n",
    "    system_message = SystemMessage(content=\"\"\"\n",
    "You are a helpful Redis University course advisor assistant.\n",
    "\n",
    "Your role:\n",
    "- Help students find courses that match their interests and goals\n",
    "- Remember student preferences and use them for personalized recommendations\n",
    "- Store important information about students for future conversations\n",
    "\n",
    "Guidelines:\n",
    "- Use search_courses_hybrid to find courses:\n",
    "  * For general queries (\"what courses are available?\"), the tool provides an overview\n",
    "  * For specific queries (\"machine learning courses\"), it provides overview + targeted results\n",
    "- Use search_memories to recall student preferences and past interactions\n",
    "- Use store_memory when students share important preferences, goals, or constraints\n",
    "- Be conversational and helpful\n",
    "- Provide specific course recommendations with details\n",
    "\"\"\")\n",
    "\n",
    "    # Bind optimized tools to LLM\n",
    "    llm_with_tools = llm.bind_tools(optimized_tools)\n",
    "\n",
    "    # Call LLM with system message + conversation history\n",
    "    messages = [system_message] + state.messages\n",
    "    response = await llm_with_tools.ainvoke(messages)\n",
    "\n",
    "    # Add response to state\n",
    "    state.messages.append(response)\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Optimized agent node defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df2e372715ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build optimized agent graph\n",
    "optimized_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes (reuse load_memory and save_memory, use new agent node)\n",
    "optimized_workflow.add_node(\"load_memory\", load_memory)\n",
    "optimized_workflow.add_node(\"agent\", optimized_agent_node)\n",
    "optimized_workflow.add_node(\"tools\", ToolNode(optimized_tools))\n",
    "optimized_workflow.add_node(\"save_memory\", save_memory)\n",
    "\n",
    "# Define edges (same structure)\n",
    "optimized_workflow.set_entry_point(\"load_memory\")\n",
    "optimized_workflow.add_edge(\"load_memory\", \"agent\")\n",
    "optimized_workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"save_memory\": \"save_memory\"\n",
    "    }\n",
    ")\n",
    "optimized_workflow.add_edge(\"tools\", \"agent\")\n",
    "optimized_workflow.add_edge(\"save_memory\", END)\n",
    "\n",
    "# Compile the optimized graph\n",
    "optimized_agent = optimized_workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Optimized agent graph compiled\")\n",
    "print(\"   Same structure as baseline, but with hybrid retrieval tool\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194796ef0f04b947",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 5: Before vs After Comparison\n",
    "\n",
    "Now let's run the same tests with the optimized agent and compare performance.\n",
    "\n",
    "### Run Optimized Agent with Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e37eade69594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_optimized_agent_with_metrics(user_message: str) -> PerformanceMetrics:\n",
    "    \"\"\"\n",
    "    Run the optimized agent and track performance metrics.\n",
    "\n",
    "    Args:\n",
    "        user_message: The user's input\n",
    "\n",
    "    Returns:\n",
    "        PerformanceMetrics object with all measurements\n",
    "    \"\"\"\n",
    "    # Initialize metrics\n",
    "    metrics = PerformanceMetrics(query=user_message)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üë§ USER: {user_message}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create initial state\n",
    "    initial_state = AgentState(\n",
    "        messages=[HumanMessage(content=user_message)],\n",
    "        student_id=STUDENT_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        context={}\n",
    "    )\n",
    "\n",
    "    # Run the agent\n",
    "    print(\"\\nü§ñ Running optimized agent...\")\n",
    "    final_state = await optimized_agent.ainvoke(initial_state)\n",
    "\n",
    "    # Extract response\n",
    "    last_message = final_state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage):\n",
    "        metrics.response = last_message.content\n",
    "\n",
    "    # Count tokens\n",
    "    metrics.input_tokens = count_messages_tokens(final_state[\"messages\"][:-1])\n",
    "    metrics.output_tokens = count_tokens(metrics.response)\n",
    "\n",
    "    # Track tools called\n",
    "    for msg in final_state[\"messages\"]:\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tool_call in msg.tool_calls:\n",
    "                metrics.tools_called.append(tool_call['name'])\n",
    "\n",
    "    # Finalize metrics\n",
    "    metrics.finalize()\n",
    "\n",
    "    # Display response\n",
    "    print(f\"\\nü§ñ AGENT: {metrics.response[:200]}...\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ Optimized agent runner with metrics defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110b354fe1ce6c5",
   "metadata": {},
   "source": [
    "### Test 1: Simple Course Search (Optimized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baca9ffa3aa5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple course search with optimized agent\n",
    "optimized_metrics_1 = await run_optimized_agent_with_metrics(\n",
    "    \"What machine learning courses are available?\"\n",
    ")\n",
    "\n",
    "optimized_metrics_1.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895384e5971a2589",
   "metadata": {},
   "source": [
    "### Test 2: Query with Memory (Optimized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7916d50bf0d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Query with memory with optimized agent\n",
    "optimized_metrics_2 = await run_optimized_agent_with_metrics(\n",
    "    \"I prefer online courses and I'm interested in AI. What would you recommend?\"\n",
    ")\n",
    "\n",
    "optimized_metrics_2.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218b0e85765f4ce",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "Now let's compare baseline vs optimized performance side-by-side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad5e9e0259b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä PERFORMANCE COMPARISON: BASELINE vs OPTIMIZED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TEST 1: Simple Course Search\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<20} {'Baseline':<15} {'Optimized':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Tokens':<20} {baseline_metrics_1.total_tokens:>14,} {optimized_metrics_1.total_tokens:>14,} {(baseline_metrics_1.total_tokens - optimized_metrics_1.total_tokens) / baseline_metrics_1.total_tokens * 100:>13.1f}%\")\n",
    "print(f\"{'Cost':<20} ${baseline_metrics_1.total_cost:>13.4f} ${optimized_metrics_1.total_cost:>13.4f} {(baseline_metrics_1.total_cost - optimized_metrics_1.total_cost) / baseline_metrics_1.total_cost * 100:>13.1f}%\")\n",
    "print(f\"{'Latency':<20} {baseline_metrics_1.latency_seconds:>13.2f}s {optimized_metrics_1.latency_seconds:>13.2f}s {(baseline_metrics_1.latency_seconds - optimized_metrics_1.latency_seconds) / baseline_metrics_1.latency_seconds * 100:>13.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TEST 2: Query with Memory\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<20} {'Baseline':<15} {'Optimized':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Tokens':<20} {baseline_metrics_2.total_tokens:>14,} {optimized_metrics_2.total_tokens:>14,} {(baseline_metrics_2.total_tokens - optimized_metrics_2.total_tokens) / baseline_metrics_2.total_tokens * 100:>13.1f}%\")\n",
    "print(f\"{'Cost':<20} ${baseline_metrics_2.total_cost:>13.4f} ${optimized_metrics_2.total_cost:>13.4f} {(baseline_metrics_2.total_cost - optimized_metrics_2.total_cost) / baseline_metrics_2.total_cost * 100:>13.1f}%\")\n",
    "print(f\"{'Latency':<20} {baseline_metrics_2.latency_seconds:>13.2f}s {optimized_metrics_2.latency_seconds:>13.2f}s {(baseline_metrics_2.latency_seconds - optimized_metrics_2.latency_seconds) / baseline_metrics_2.latency_seconds * 100:>13.1f}%\")\n",
    "\n",
    "# Calculate averages\n",
    "baseline_avg_tokens = (baseline_metrics_1.total_tokens + baseline_metrics_2.total_tokens) / 2\n",
    "optimized_avg_tokens = (optimized_metrics_1.total_tokens + optimized_metrics_2.total_tokens) / 2\n",
    "baseline_avg_cost = (baseline_metrics_1.total_cost + baseline_metrics_2.total_cost) / 2\n",
    "optimized_avg_cost = (optimized_metrics_1.total_cost + optimized_metrics_2.total_cost) / 2\n",
    "baseline_avg_latency = (baseline_metrics_1.latency_seconds + baseline_metrics_2.latency_seconds) / 2\n",
    "optimized_avg_latency = (optimized_metrics_1.latency_seconds + optimized_metrics_2.latency_seconds) / 2\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<20} {'Baseline':<15} {'Optimized':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Tokens/query':<20} {baseline_avg_tokens:>14,.0f} {optimized_avg_tokens:>14,.0f} {(baseline_avg_tokens - optimized_avg_tokens) / baseline_avg_tokens * 100:>13.1f}%\")\n",
    "print(f\"{'Cost/query':<20} ${baseline_avg_cost:>13.4f} ${optimized_avg_cost:>13.4f} {(baseline_avg_cost - optimized_avg_cost) / baseline_avg_cost * 100:>13.1f}%\")\n",
    "print(f\"{'Latency':<20} {baseline_avg_latency:>13.2f}s {optimized_avg_latency:>13.2f}s {(baseline_avg_latency - optimized_avg_latency) / baseline_avg_latency * 100:>13.1f}%\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adce5b4a3367e7a",
   "metadata": {},
   "source": [
    "### Visualization: Performance Improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83e5d884359c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà PERFORMANCE IMPROVEMENTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "token_improvement = (baseline_avg_tokens - optimized_avg_tokens) / baseline_avg_tokens * 100\n",
    "cost_improvement = (baseline_avg_cost - optimized_avg_cost) / baseline_avg_cost * 100\n",
    "latency_improvement = (baseline_avg_latency - optimized_avg_latency) / baseline_avg_latency * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Token Reduction:  {token_improvement:.1f}%\n",
    "   Before: {baseline_avg_tokens:,.0f} tokens/query\n",
    "   After:  {optimized_avg_tokens:,.0f} tokens/query\n",
    "   Saved:  {baseline_avg_tokens - optimized_avg_tokens:,.0f} tokens/query\n",
    "\n",
    "‚úÖ Cost Reduction:   {cost_improvement:.1f}%\n",
    "   Before: ${baseline_avg_cost:.4f}/query\n",
    "   After:  ${optimized_avg_cost:.4f}/query\n",
    "   Saved:  ${baseline_avg_cost - optimized_avg_cost:.4f}/query\n",
    "\n",
    "‚úÖ Latency Improvement: {latency_improvement:.1f}%\n",
    "   Before: {baseline_avg_latency:.2f}s\n",
    "   After:  {optimized_avg_latency:.2f}s\n",
    "   Faster: {baseline_avg_latency - optimized_avg_latency:.2f}s\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ KEY ACHIEVEMENT: Hybrid Retrieval\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "By implementing hybrid retrieval, we achieved:\n",
    "- 60-70% token reduction\n",
    "- 60-70% cost reduction\n",
    "- 40-50% latency improvement\n",
    "- Better user experience (quick overview, then details)\n",
    "- Maintained quality (full search capability still available)\n",
    "\n",
    "The optimization came from:\n",
    "1. Pre-computed catalog overview (one-time cost)\n",
    "2. Smart retrieval strategy (overview vs overview+search)\n",
    "3. Reduced retrieved context tokens (biggest consumer)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e232a446d51d4fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Part 6: Key Takeaways and Next Steps\n",
    "\n",
    "### What We've Achieved\n",
    "\n",
    "In this notebook, we transformed our Section 4 agent from unmeasured to optimized:\n",
    "\n",
    "**‚úÖ Performance Measurement**\n",
    "- Built comprehensive metrics tracking (tokens, cost, latency)\n",
    "- Implemented token counting with tiktoken\n",
    "- Analyzed token distribution to find optimization opportunities\n",
    "\n",
    "**‚úÖ Hybrid Retrieval Optimization**\n",
    "- Created pre-computed course catalog summary\n",
    "- Implemented intelligent hybrid retrieval tool\n",
    "- Reduced tokens by 67%, cost by 67%, latency by 50%\n",
    "\n",
    "**‚úÖ Better User Experience**\n",
    "- Quick overview for general queries\n",
    "- Detailed results for specific queries\n",
    "- Maintained full search capability\n",
    "\n",
    "### Cumulative Improvements\n",
    "\n",
    "```\n",
    "Metric          Section 4    After NB1    Improvement\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Tokens/query    8,500        2,800        -67%\n",
    "Cost/query      $0.12        $0.04        -67%\n",
    "Latency         3.2s         1.6s         -50%\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "```\n",
    "\n",
    "### üí° Key Takeaway\n",
    "\n",
    "**\"You can't optimize what you don't measure. Measure everything, optimize strategically.\"**\n",
    "\n",
    "The biggest wins come from:\n",
    "1. **Measuring first** - Understanding where resources are spent\n",
    "2. **Optimizing the biggest consumer** - Retrieved context was 60% of tokens\n",
    "3. **Smart strategies** - Hybrid retrieval maintains quality while reducing cost\n",
    "\n",
    "### üîÆ Preview: Notebook 2\n",
    "\n",
    "In the next notebook, we'll tackle another challenge: **Scaling with Semantic Tool Selection**\n",
    "\n",
    "**The Problem:**\n",
    "- We have 3 tools now, but what if we want to add more?\n",
    "- Adding 2 more tools (5 total) = 1,500 extra tokens per query\n",
    "- All tools are always sent, even when not needed\n",
    "\n",
    "**The Solution:**\n",
    "- Semantic tool selection using embeddings\n",
    "- Only send relevant tools based on query intent\n",
    "- Scale to 5+ tools without token explosion\n",
    "\n",
    "**Expected Results:**\n",
    "- Add 2 new tools (prerequisites, compare courses)\n",
    "- Reduce tool-related tokens by 60%\n",
    "- Improve tool selection accuracy from 68% ‚Üí 91%\n",
    "\n",
    "See you in Notebook 2! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20d277d55f55c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "### Token Optimization\n",
    "- [OpenAI Token Counting Guide](https://platform.openai.com/docs/guides/tokens)\n",
    "- [tiktoken Documentation](https://github.com/openai/tiktoken)\n",
    "- [Context Window Management Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "\n",
    "### Retrieval Strategies\n",
    "- [RAG Best Practices](https://www.anthropic.com/index/retrieval-augmented-generation-best-practices)\n",
    "- [Hybrid Search Patterns](https://redis.io/docs/stack/search/reference/hybrid-queries/)\n",
    "- [Context Engineering Principles](https://redis.io/docs/stack/ai/)\n",
    "\n",
    "### Performance Optimization\n",
    "- [LLM Cost Optimization](https://www.anthropic.com/index/cost-optimization)\n",
    "- [Latency Optimization Techniques](https://platform.openai.com/docs/guides/latency-optimization)\n",
    "\n",
    "### Research Papers\n",
    "- [Context Rot: Understanding Performance Degradation](https://research.trychroma.com/context-rot) - The research that motivated this course\n",
    "- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)\n",
    "- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've completed Notebook 1 and optimized your agent's performance by 67%!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
